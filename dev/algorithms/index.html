<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Algorithms · LinearAlgebraMPI.jl</title><meta name="title" content="Algorithms · LinearAlgebraMPI.jl"/><meta property="og:title" content="Algorithms · LinearAlgebraMPI.jl"/><meta property="twitter:title" content="Algorithms · LinearAlgebraMPI.jl"/><meta name="description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="og:description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="twitter:description" content="Documentation for LinearAlgebraMPI.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">LinearAlgebraMPI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting-started/">Getting Started</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>Algorithms</a><ul class="internal"><li><a class="tocitem" href="#Overview"><span>Overview</span></a></li><li><a class="tocitem" href="#The-Multifrontal-Method"><span>The Multifrontal Method</span></a></li><li><a class="tocitem" href="#Fill-Reducing-Ordering"><span>Fill-Reducing Ordering</span></a></li><li><a class="tocitem" href="#Elimination-Tree"><span>Elimination Tree</span></a></li><li><a class="tocitem" href="#Supernodes"><span>Supernodes</span></a></li><li><a class="tocitem" href="#Distributed-Execution"><span>Distributed Execution</span></a></li><li><a class="tocitem" href="#Numerical-Pivoting"><span>Numerical Pivoting</span></a></li><li><a class="tocitem" href="#Solve-Phase"><span>Solve Phase</span></a></li><li><a class="tocitem" href="#Plan-Caching"><span>Plan Caching</span></a></li><li><a class="tocitem" href="#Complexity"><span>Complexity</span></a></li><li><a class="tocitem" href="#References"><span>References</span></a></li></ul></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Algorithms</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Algorithms</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Algorithms"><a class="docs-heading-anchor" href="#Algorithms">Algorithms</a><a id="Algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Algorithms" title="Permalink"></a></h1><p>This document describes the algorithms used in LinearAlgebraMPI&#39;s distributed sparse direct solvers. It is intended for experts familiar with sparse matrix computations who want to understand our implementation choices.</p><h2 id="Overview"><a class="docs-heading-anchor" href="#Overview">Overview</a><a id="Overview-1"></a><a class="docs-heading-anchor-permalink" href="#Overview" title="Permalink"></a></h2><p>LinearAlgebraMPI implements distributed sparse LU and LDLT factorizations using the <strong>multifrontal method</strong>. The implementation follows a three-phase approach:</p><ol><li><strong>Symbolic Phase</strong>: Compute fill-reducing ordering, elimination tree, supernodes, and rank assignments</li><li><strong>Numerical Phase</strong>: Distributed multifrontal factorization with communication</li><li><strong>Solve Phase</strong>: Distributed triangular solves</li></ol><h2 id="The-Multifrontal-Method"><a class="docs-heading-anchor" href="#The-Multifrontal-Method">The Multifrontal Method</a><a id="The-Multifrontal-Method-1"></a><a class="docs-heading-anchor-permalink" href="#The-Multifrontal-Method" title="Permalink"></a></h2><p>The multifrontal method was introduced by Duff and Reid [1] for symmetric indefinite systems and later extended to unsymmetric systems [2]. Liu provides an excellent survey of the method [3].</p><p>The key insight is that sparse Gaussian elimination can be organized as a sequence of partial factorizations of dense submatrices called <strong>frontal matrices</strong>. Each frontal matrix corresponds to eliminating a set of pivot columns and produces:</p><ul><li>Completed rows/columns of the factors L and U (or L and D for LDLT)</li><li>A dense <strong>update matrix</strong> (Schur complement) that is passed to a parent frontal</li></ul><p>This organization enables:</p><ul><li>High performance through dense BLAS operations on frontal matrices</li><li>Natural parallelism from the tree structure of dependencies</li><li>Reduced memory traffic compared to left-looking or right-looking methods</li></ul><h3 id="Frontal-Matrix-Structure"><a class="docs-heading-anchor" href="#Frontal-Matrix-Structure">Frontal Matrix Structure</a><a id="Frontal-Matrix-Structure-1"></a><a class="docs-heading-anchor-permalink" href="#Frontal-Matrix-Structure" title="Permalink"></a></h3><p>For a supernode with <code>nfs</code> fully-summed (pivot) columns and <code>nrs</code> rows in total, the frontal matrix F has the block structure:</p><pre><code class="nohighlight hljs">F = [ F11  F12 ]    (nfs × nfs)  (nfs × (nrs-nfs))
    [ F21  F22 ]    ((nrs-nfs) × nfs)  ((nrs-nfs) × (nrs-nfs))</code></pre><p>After partial factorization:</p><ul><li><code>F11</code> contains the diagonal block of the factors</li><li><code>F21</code> contains the subdiagonal of L (LU) or is symmetric with F12 (LDLT)</li><li><code>F12</code> contains the superdiagonal of U (LU only)</li><li><code>F22</code> becomes the update matrix (Schur complement) passed to the parent</li></ul><h3 id="Assembly-(Extend-Add)"><a class="docs-heading-anchor" href="#Assembly-(Extend-Add)">Assembly (Extend-Add)</a><a id="Assembly-(Extend-Add)-1"></a><a class="docs-heading-anchor-permalink" href="#Assembly-(Extend-Add)" title="Permalink"></a></h3><p>When a frontal matrix receives update matrices from its children in the elimination tree, these are assembled using the <strong>extend-add</strong> operation. The child&#39;s update matrix is scattered into the parent&#39;s frontal matrix at positions determined by the index mapping, and values are summed.</p><h2 id="Fill-Reducing-Ordering"><a class="docs-heading-anchor" href="#Fill-Reducing-Ordering">Fill-Reducing Ordering</a><a id="Fill-Reducing-Ordering-1"></a><a class="docs-heading-anchor-permalink" href="#Fill-Reducing-Ordering" title="Permalink"></a></h2><p>We use the <strong>Approximate Minimum Degree (AMD)</strong> ordering algorithm [4, 5] to reduce fill-in during factorization. AMD computes a permutation P such that factoring P&#39;AP produces fewer nonzeros in L and U than factoring A directly.</p><p>The minimum degree algorithm is a greedy heuristic that, at each elimination step, selects the variable whose elimination creates the least fill-in. AMD uses quotient graph techniques to efficiently approximate the true minimum degree, achieving O(n²) worst-case complexity while producing orderings comparable to exact minimum degree.</p><p>Our implementation uses Julia&#39;s <code>AMD.jl</code> package, which provides a native Julia implementation of the AMD algorithm.</p><h2 id="Elimination-Tree"><a class="docs-heading-anchor" href="#Elimination-Tree">Elimination Tree</a><a id="Elimination-Tree-1"></a><a class="docs-heading-anchor-permalink" href="#Elimination-Tree" title="Permalink"></a></h2><p>The <strong>elimination tree</strong> captures the dependencies between columns during factorization [6]. For a matrix A with Cholesky factor L, the elimination tree T has:</p><ul><li>n nodes (one per column)</li><li>An edge from j to parent(j) where parent(j) is the row index of the first subdiagonal nonzero in column j of L</li></ul><p>Key properties:</p><ul><li>Column j of L depends only on columns in the subtree rooted at j</li><li>Disjoint subtrees can be factored independently (parallelism)</li><li>The tree structure determines the flow of update matrices in the multifrontal method</li></ul><p>For unsymmetric LU factorization, we use the elimination tree of the symmetrized structure A + A&#39;.</p><h2 id="Supernodes"><a class="docs-heading-anchor" href="#Supernodes">Supernodes</a><a id="Supernodes-1"></a><a class="docs-heading-anchor-permalink" href="#Supernodes" title="Permalink"></a></h2><p>A <strong>supernode</strong> is a set of contiguous columns with nearly identical sparsity structure [7, 8]. Grouping columns into supernodes enables:</p><ul><li>Dense matrix operations (BLAS-3) instead of sparse column operations</li><li>Reduced indexing overhead</li><li>Better cache utilization</li></ul><p>We detect <strong>fundamental supernodes</strong>: maximal sets of contiguous columns where each column&#39;s structure is contained in the next column&#39;s structure (plus the new diagonal). This can be characterized in terms of the elimination tree: columns j and j+1 are in the same fundamental supernode if and only if parent(j) = j+1 in the elimination tree [7].</p><p>After detecting fundamental supernodes, we construct a <strong>supernodal elimination tree</strong> where each node represents a supernode rather than a single column.</p><h2 id="Distributed-Execution"><a class="docs-heading-anchor" href="#Distributed-Execution">Distributed Execution</a><a id="Distributed-Execution-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Execution" title="Permalink"></a></h2><h3 id="Supernode-to-Rank-Assignment"><a class="docs-heading-anchor" href="#Supernode-to-Rank-Assignment">Supernode-to-Rank Assignment</a><a id="Supernode-to-Rank-Assignment-1"></a><a class="docs-heading-anchor-permalink" href="#Supernode-to-Rank-Assignment" title="Permalink"></a></h3><p>We use a <strong>subtree-to-rank mapping</strong> strategy inspired by MUMPS [9, 10]. The key idea is to assign complete subtrees of the supernodal elimination tree to single MPI ranks, minimizing communication.</p><p>The assignment algorithm:</p><ol><li>Compute the &quot;work&quot; for each supernode: <code>work[s] = nfs * nrows² + Σ work[children]</code></li><li>Identify subtree roots (supernodes whose parent is assigned to a different rank or is the root)</li><li>Use bin-packing to assign subtrees to ranks, balancing the total work per rank</li></ol><p>This approach ensures that:</p><ul><li>Communication only occurs at subtree boundaries</li><li>Load is approximately balanced across ranks</li><li>Small problems use fewer ranks efficiently</li></ul><p>For very large frontal matrices near the root of the tree, more sophisticated 2D distribution schemes (as in MUMPS [9]) could be employed, but our current implementation assigns each supernode to a single rank.</p><h3 id="Communication-Pattern"><a class="docs-heading-anchor" href="#Communication-Pattern">Communication Pattern</a><a id="Communication-Pattern-1"></a><a class="docs-heading-anchor-permalink" href="#Communication-Pattern" title="Permalink"></a></h3><p>During numerical factorization, communication occurs when:</p><ol><li>A child supernode on rank r₁ sends its update matrix to a parent supernode on rank r₂ ≠ r₁</li><li>The update matrix and its row indices are sent via MPI point-to-point communication</li></ol><p>We use non-blocking sends (<code>MPI.Isend</code>) to overlap communication with computation where possible. Synchronization barriers between supernodes ensure correct ordering.</p><h2 id="Numerical-Pivoting"><a class="docs-heading-anchor" href="#Numerical-Pivoting">Numerical Pivoting</a><a id="Numerical-Pivoting-1"></a><a class="docs-heading-anchor-permalink" href="#Numerical-Pivoting" title="Permalink"></a></h2><h3 id="LU-Factorization:-Partial-Pivoting"><a class="docs-heading-anchor" href="#LU-Factorization:-Partial-Pivoting">LU Factorization: Partial Pivoting</a><a id="LU-Factorization:-Partial-Pivoting-1"></a><a class="docs-heading-anchor-permalink" href="#LU-Factorization:-Partial-Pivoting" title="Permalink"></a></h3><p>For unsymmetric matrices, we use <strong>partial pivoting</strong> within each frontal matrix. At each elimination step k within the frontal:</p><ol><li>Find the largest magnitude element in column k below the diagonal</li><li>Swap rows if necessary</li><li>Proceed with elimination</li></ol><p>The row permutation is tracked and applied during the solve phase. Partial pivoting ensures numerical stability with element growth bounded by 2^(n-1) in the worst case, though growth is typically much smaller in practice.</p><h3 id="LDLT-Factorization:-Bunch-Kaufman-Pivoting"><a class="docs-heading-anchor" href="#LDLT-Factorization:-Bunch-Kaufman-Pivoting">LDLT Factorization: Bunch-Kaufman Pivoting</a><a id="LDLT-Factorization:-Bunch-Kaufman-Pivoting-1"></a><a class="docs-heading-anchor-permalink" href="#LDLT-Factorization:-Bunch-Kaufman-Pivoting" title="Permalink"></a></h3><p>For symmetric matrices (definite or indefinite), we use the <strong>Bunch-Kaufman pivoting</strong> strategy [11]. This produces a factorization:</p><pre><code class="nohighlight hljs">P&#39; * A * P = L * D * L&#39;</code></pre><p>where:</p><ul><li>P is a permutation matrix</li><li>L is unit lower triangular</li><li>D is block diagonal with 1×1 and 2×2 blocks</li></ul><p>The algorithm maintains symmetry while providing numerical stability for indefinite matrices. At each step, it chooses between:</p><ul><li>A 1×1 pivot if the diagonal element is sufficiently large</li><li>A 2×2 pivot using an off-diagonal element if better conditioning is achieved</li></ul><p>The threshold parameter α = (1 + √17)/8 ≈ 0.6404 balances stability and sparsity [11].</p><p>For 2×2 pivots at positions (k, k+1), the block:</p><pre><code class="nohighlight hljs">D_block = [ d_kk    d_k,k+1  ]
          [ d_k+1,k  d_k+1,k+1 ]</code></pre><p>is stored, and the solve phase handles these blocks specially by solving 2×2 systems.</p><p><strong>Important</strong>: Our LDLT uses transpose (L&#39;), not conjugate transpose (L*). This is correct for real symmetric and complex symmetric matrices, but NOT for complex Hermitian matrices.</p><h2 id="Solve-Phase"><a class="docs-heading-anchor" href="#Solve-Phase">Solve Phase</a><a id="Solve-Phase-1"></a><a class="docs-heading-anchor-permalink" href="#Solve-Phase" title="Permalink"></a></h2><h3 id="LU-Solve"><a class="docs-heading-anchor" href="#LU-Solve">LU Solve</a><a id="LU-Solve-1"></a><a class="docs-heading-anchor-permalink" href="#LU-Solve" title="Permalink"></a></h3><p>Given the factorization P<em>r&#39; * L * U * P</em>c = A (with row permutation P<em>r from pivoting and column permutation P</em>c from AMD ordering), we solve Ax = b as:</p><ol><li>Apply AMD permutation: <code>y = P_c&#39; * b</code></li><li>Apply row pivot permutation: <code>z = P_r * y</code></li><li>Forward solve: <code>L * w = z</code> (in elimination order)</li><li>Backward solve: <code>U * v = w</code> (in reverse elimination order)</li><li>Apply inverse row permutation: <code>u = P_r&#39; * v</code></li><li>Apply inverse AMD permutation: <code>x = P_c * u</code></li></ol><h3 id="LDLT-Solve"><a class="docs-heading-anchor" href="#LDLT-Solve">LDLT Solve</a><a id="LDLT-Solve-1"></a><a class="docs-heading-anchor-permalink" href="#LDLT-Solve" title="Permalink"></a></h3><p>Given P&#39; * A * P = L * D * L&#39; with symmetric pivot permutation P_s, we solve Ax = b as:</p><ol><li>Apply AMD permutation: <code>y = P_perm * b</code></li><li>Apply symmetric pivot permutation: <code>z = P_s * y</code></li><li>Forward solve: <code>L * w = z</code></li><li>Diagonal solve: <code>D * v = w</code> (handling 2×2 blocks)</li><li>Backward solve: <code>L&#39; * u = v</code></li><li>Apply inverse symmetric permutation: <code>t = P_s&#39; * u</code></li><li>Apply inverse AMD permutation: <code>x = P_perm&#39; * t</code></li></ol><p>The diagonal solve for 2×2 blocks requires solving:</p><pre><code class="nohighlight hljs">[ d11  d12 ] [ v_k   ]   [ w_k   ]
[ d21  d22 ] [ v_k+1 ] = [ w_k+1 ]</code></pre><p>which is done by direct 2×2 matrix inversion.</p><h2 id="Plan-Caching"><a class="docs-heading-anchor" href="#Plan-Caching">Plan Caching</a><a id="Plan-Caching-1"></a><a class="docs-heading-anchor-permalink" href="#Plan-Caching" title="Permalink"></a></h2><p>Following the pattern used throughout LinearAlgebraMPI, we cache computed structures for reuse:</p><ul><li><strong>Symbolic factorization cache</strong>: Keyed by the structural hash of the matrix. Stores the AMD ordering, elimination tree, supernodes, and rank assignments.</li><li><strong>Factorization plan cache</strong>: Keyed by (structural hash, element type). Stores pre-allocated communication buffers.</li></ul><p>When factoring a sequence of matrices with the same sparsity pattern, only the first factorization incurs the cost of symbolic analysis. Subsequent factorizations reuse the cached symbolic structure and communication plans, performing only the numerical computation.</p><h2 id="Complexity"><a class="docs-heading-anchor" href="#Complexity">Complexity</a><a id="Complexity-1"></a><a class="docs-heading-anchor-permalink" href="#Complexity" title="Permalink"></a></h2><p>For a sparse matrix of dimension n with nnz nonzeros:</p><ul><li><strong>AMD ordering</strong>: O(nnz) average case, O(n²) worst case</li><li><strong>Symbolic factorization</strong>: O(nnz<em>L) where nnz</em>L is nonzeros in L</li><li><strong>Numerical factorization</strong>: Dominated by dense operations on frontal matrices. For 2D problems from finite elements, typically O(n^1.5); for 3D problems, O(n²)</li><li><strong>Solve</strong>: O(nnz<em>L + nnz</em>U)</li></ul><p>The distributed implementation adds communication overhead proportional to the number of cross-rank edges in the supernodal elimination tree.</p><h2 id="References"><a class="docs-heading-anchor" href="#References">References</a><a id="References-1"></a><a class="docs-heading-anchor-permalink" href="#References" title="Permalink"></a></h2><p>[1] I. S. Duff and J. K. Reid, &quot;The multifrontal solution of indefinite sparse symmetric linear equations,&quot; <em>ACM Transactions on Mathematical Software</em>, vol. 9, no. 3, pp. 302–325, 1983. <a href="https://doi.org/10.1145/356044.356047">https://doi.org/10.1145/356044.356047</a></p><p>[2] I. S. Duff and J. K. Reid, &quot;The multifrontal solution of unsymmetric sets of linear equations,&quot; <em>SIAM Journal on Scientific and Statistical Computing</em>, vol. 5, no. 3, pp. 633–641, 1984.</p><p>[3] J. W. H. Liu, &quot;The multifrontal method for sparse matrix solution: Theory and practice,&quot; <em>SIAM Review</em>, vol. 34, no. 1, pp. 82–109, 1992. <a href="https://doi.org/10.1137/1034004">https://doi.org/10.1137/1034004</a></p><p>[4] P. R. Amestoy, T. A. Davis, and I. S. Duff, &quot;An approximate minimum degree ordering algorithm,&quot; <em>SIAM Journal on Matrix Analysis and Applications</em>, vol. 17, no. 4, pp. 886–905, 1996. <a href="https://doi.org/10.1137/S0895479894278952">https://doi.org/10.1137/S0895479894278952</a></p><p>[5] P. R. Amestoy, T. A. Davis, and I. S. Duff, &quot;Algorithm 837: AMD, an approximate minimum degree ordering algorithm,&quot; <em>ACM Transactions on Mathematical Software</em>, vol. 30, no. 3, pp. 381–388, 2004. <a href="https://doi.org/10.1145/1024074.1024081">https://doi.org/10.1145/1024074.1024081</a></p><p>[6] J. W. H. Liu, &quot;The role of elimination trees in sparse factorization,&quot; <em>SIAM Journal on Matrix Analysis and Applications</em>, vol. 11, no. 1, pp. 134–172, 1990. <a href="https://doi.org/10.1137/0611010">https://doi.org/10.1137/0611010</a></p><p>[7] J. W. H. Liu, E. G. Ng, and B. W. Peyton, &quot;On finding supernodes for sparse matrix computations,&quot; <em>SIAM Journal on Matrix Analysis and Applications</em>, vol. 14, no. 1, pp. 242–252, 1993. <a href="https://doi.org/10.1137/0614019">https://doi.org/10.1137/0614019</a></p><p>[8] C. Ashcraft and R. Grimes, &quot;The influence of relaxed supernode partitions on the multifrontal method,&quot; <em>ACM Transactions on Mathematical Software</em>, vol. 15, no. 4, pp. 291–309, 1989. <a href="https://doi.org/10.1145/76909.76912">https://doi.org/10.1145/76909.76912</a></p><p>[9] P. R. Amestoy, I. S. Duff, J. Koster, and J.-Y. L&#39;Excellent, &quot;A fully asynchronous multifrontal solver using distributed dynamic scheduling,&quot; <em>SIAM Journal on Matrix Analysis and Applications</em>, vol. 23, no. 1, pp. 15–41, 2001. <a href="https://doi.org/10.1137/S0895479899358194">https://doi.org/10.1137/S0895479899358194</a></p><p>[10] P. R. Amestoy, I. S. Duff, and J.-Y. L&#39;Excellent, &quot;Multifrontal parallel distributed symmetric and unsymmetric solvers,&quot; <em>Computer Methods in Applied Mechanics and Engineering</em>, vol. 184, no. 2–4, pp. 501–520, 2000.</p><p>[11] J. R. Bunch and L. Kaufman, &quot;Some stable methods for calculating inertia and solving symmetric linear systems,&quot; <em>Mathematics of Computation</em>, vol. 31, no. 137, pp. 163–179, 1977. <a href="https://doi.org/10.1090/S0025-5718-1977-0428694-0">https://doi.org/10.1090/S0025-5718-1977-0428694-0</a></p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../api/">API Reference »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Saturday 13 December 2025 12:58">Saturday 13 December 2025</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
