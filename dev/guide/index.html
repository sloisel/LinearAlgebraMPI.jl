<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>User Guide · LinearAlgebraMPI.jl</title><meta name="title" content="User Guide · LinearAlgebraMPI.jl"/><meta property="og:title" content="User Guide · LinearAlgebraMPI.jl"/><meta property="twitter:title" content="User Guide · LinearAlgebraMPI.jl"/><meta name="description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="og:description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="twitter:description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="og:url" content="https://sloisel.github.io/LinearAlgebraMPI.jl/guide/"/><meta property="twitter:url" content="https://sloisel.github.io/LinearAlgebraMPI.jl/guide/"/><link rel="canonical" href="https://sloisel.github.io/LinearAlgebraMPI.jl/guide/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">LinearAlgebraMPI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li class="is-active"><a class="tocitem" href>User Guide</a><ul class="internal"><li><a class="tocitem" href="#Core-Types"><span>Core Types</span></a></li><li><a class="tocitem" href="#Creating-Distributed-Types"><span>Creating Distributed Types</span></a></li><li><a class="tocitem" href="#Basic-Operations"><span>Basic Operations</span></a></li><li><a class="tocitem" href="#Solving-Linear-Systems"><span>Solving Linear Systems</span></a></li><li><a class="tocitem" href="#Threading"><span>Threading</span></a></li><li><a class="tocitem" href="#Row-wise-Operations-with-map_rows"><span>Row-wise Operations with map_rows</span></a></li><li><a class="tocitem" href="#Type-Conversions"><span>Type Conversions</span></a></li><li><a class="tocitem" href="#IO-and-Output"><span>IO and Output</span></a></li><li><a class="tocitem" href="#Repartitioning"><span>Repartitioning</span></a></li><li><a class="tocitem" href="#GPU-Support-(Metal)"><span>GPU Support (Metal)</span></a></li><li><a class="tocitem" href="#Cache-Management"><span>Cache Management</span></a></li><li><a class="tocitem" href="#MPI-Collective-Operations"><span>MPI Collective Operations</span></a></li><li><a class="tocitem" href="#Next-Steps"><span>Next Steps</span></a></li></ul></li><li><a class="tocitem" href="../examples/">Examples</a></li><li><a class="tocitem" href="../api/">API Reference</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>User Guide</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>User Guide</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sloisel/LinearAlgebraMPI.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/main/docs/src/guide.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="User-Guide"><a class="docs-heading-anchor" href="#User-Guide">User Guide</a><a id="User-Guide-1"></a><a class="docs-heading-anchor-permalink" href="#User-Guide" title="Permalink"></a></h1><p>This guide covers the essential workflows for using LinearAlgebraMPI.jl.</p><h2 id="Core-Types"><a class="docs-heading-anchor" href="#Core-Types">Core Types</a><a id="Core-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Core-Types" title="Permalink"></a></h2><p>LinearAlgebraMPI provides three distributed types:</p><table><tr><th style="text-align: right">Type</th><th style="text-align: right">Description</th><th style="text-align: right">Storage</th></tr><tr><td style="text-align: right"><code>VectorMPI{T,AV}</code></td><td style="text-align: right">Distributed vector</td><td style="text-align: right">Row-partitioned</td></tr><tr><td style="text-align: right"><code>MatrixMPI{T,AM}</code></td><td style="text-align: right">Distributed dense matrix</td><td style="text-align: right">Row-partitioned</td></tr><tr><td style="text-align: right"><code>SparseMatrixMPI{T,Ti,AV}</code></td><td style="text-align: right">Distributed sparse matrix</td><td style="text-align: right">Row-partitioned CSR</td></tr></table><p>The type parameters are:</p><ul><li><code>T</code>: Element type (<code>Float64</code>, <code>Float32</code>, <code>ComplexF64</code>, etc.)</li><li><code>AV&lt;:AbstractVector{T}</code>: Underlying storage for vectors and sparse matrix values (<code>Vector{T}</code> for CPU, <code>MtlVector{T}</code> for Metal GPU)</li><li><code>AM&lt;:AbstractMatrix{T}</code>: Underlying storage for dense matrices (<code>Matrix{T}</code> for CPU, <code>MtlMatrix{T}</code> for Metal GPU)</li><li><code>Ti</code>: Index type for sparse matrices (typically <code>Int</code>)</li></ul><p>All types are row-partitioned across MPI ranks, meaning each rank owns a contiguous range of rows.</p><h3 id="Internal-Storage:-CSR-Format"><a class="docs-heading-anchor" href="#Internal-Storage:-CSR-Format">Internal Storage: CSR Format</a><a id="Internal-Storage:-CSR-Format-1"></a><a class="docs-heading-anchor-permalink" href="#Internal-Storage:-CSR-Format" title="Permalink"></a></h3><p>Internally, <code>SparseMatrixMPI</code> stores local rows in CSR (Compressed Sparse Row) format using the <code>SparseMatrixCSR</code> type. This enables efficient row-wise iteration for a row-partitioned distributed matrix.</p><p>In Julia, <code>SparseMatrixCSR{T,Ti}</code> is a type alias for <code>Transpose{T, SparseMatrixCSC{T,Ti}}</code>. You don&#39;t need to worry about this for normal usage - it&#39;s handled automatically.</p><h2 id="Creating-Distributed-Types"><a class="docs-heading-anchor" href="#Creating-Distributed-Types">Creating Distributed Types</a><a id="Creating-Distributed-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Creating-Distributed-Types" title="Permalink"></a></h2><h3 id="From-Native-Julia-Types"><a class="docs-heading-anchor" href="#From-Native-Julia-Types">From Native Julia Types</a><a id="From-Native-Julia-Types-1"></a><a class="docs-heading-anchor-permalink" href="#From-Native-Julia-Types" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI
MPI.Init()

using LinearAlgebraMPI
using SparseArrays

# Create from native types (data is distributed automatically)
v = VectorMPI(randn(100))
A = MatrixMPI(randn(50, 30))
S = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))</code></pre><h3 id="Local-Constructors"><a class="docs-heading-anchor" href="#Local-Constructors">Local Constructors</a><a id="Local-Constructors-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Constructors" title="Permalink"></a></h3><p>For performance-critical code, use local constructors that avoid global communication:</p><pre><code class="language-julia hljs"># Create from local data (each rank provides its own rows)
v_local = VectorMPI_local(my_local_vector)
A_local = MatrixMPI_local(my_local_matrix)
S_local = SparseMatrixMPI_local(my_local_sparse)</code></pre><h3 id="Efficient-Local-Only-Construction"><a class="docs-heading-anchor" href="#Efficient-Local-Only-Construction">Efficient Local-Only Construction</a><a id="Efficient-Local-Only-Construction-1"></a><a class="docs-heading-anchor-permalink" href="#Efficient-Local-Only-Construction" title="Permalink"></a></h3><p>For large matrices, avoid replicating data across all ranks by only populating each rank&#39;s local portion:</p><pre><code class="language-julia hljs">comm = MPI.COMM_WORLD
rank = MPI.Comm_rank(comm)
nranks = MPI.Comm_size(comm)

# Global dimensions
m, n = 1000, 1000

# Compute which rows this rank owns
rows_per_rank = div(m, nranks)
remainder = mod(m, nranks)
my_row_start = 1 + rank * rows_per_rank + min(rank, remainder)
my_row_end = my_row_start + rows_per_rank - 1 + (rank &lt; remainder ? 1 : 0)

# Create sparse matrix with correct size, but only populate local rows
I, J, V = Int[], Int[], Float64[]
for i in my_row_start:my_row_end
    # Example: tridiagonal matrix
    if i &gt; 1
        push!(I, i); push!(J, i-1); push!(V, -1.0)
    end
    push!(I, i); push!(J, i); push!(V, 2.0)
    if i &lt; m
        push!(I, i); push!(J, i+1); push!(V, -1.0)
    end
end
A = sparse(I, J, V, m, n)

# The constructor extracts only local rows - other rows are ignored
Adist = SparseMatrixMPI{Float64}(A)</code></pre><h2 id="Basic-Operations"><a class="docs-heading-anchor" href="#Basic-Operations">Basic Operations</a><a id="Basic-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Basic-Operations" title="Permalink"></a></h2><h3 id="Vector-Operations"><a class="docs-heading-anchor" href="#Vector-Operations">Vector Operations</a><a id="Vector-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-Operations" title="Permalink"></a></h3><pre><code class="language-julia hljs">v = VectorMPI(randn(100))
w = VectorMPI(randn(100))

# Arithmetic
u = v + w
u = v - w
u = 2.0 * v
u = v * 2.0

# Linear algebra
n = norm(v)
d = dot(v, w)
c = conj(v)</code></pre><h3 id="Matrix-Vector-Products"><a class="docs-heading-anchor" href="#Matrix-Vector-Products">Matrix-Vector Products</a><a id="Matrix-Vector-Products-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-Vector-Products" title="Permalink"></a></h3><pre><code class="language-julia hljs">A = MatrixMPI(randn(50, 100))
v = VectorMPI(randn(100))

# Matrix-vector multiply
y = A * v</code></pre><h3 id="Sparse-Operations"><a class="docs-heading-anchor" href="#Sparse-Operations">Sparse Operations</a><a id="Sparse-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-Operations" title="Permalink"></a></h3><pre><code class="language-julia hljs">using SparseArrays

A = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))
v = VectorMPI(randn(100))

# Matrix-vector multiply
y = A * v

# Matrix-matrix multiply
B = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))
C = A * B</code></pre><h2 id="Solving-Linear-Systems"><a class="docs-heading-anchor" href="#Solving-Linear-Systems">Solving Linear Systems</a><a id="Solving-Linear-Systems-1"></a><a class="docs-heading-anchor-permalink" href="#Solving-Linear-Systems" title="Permalink"></a></h2><h3 id="Direct-Solve-with-Backslash"><a class="docs-heading-anchor" href="#Direct-Solve-with-Backslash">Direct Solve with Backslash</a><a id="Direct-Solve-with-Backslash-1"></a><a class="docs-heading-anchor-permalink" href="#Direct-Solve-with-Backslash" title="Permalink"></a></h3><pre><code class="language-julia hljs">using SparseArrays

# Create a well-conditioned sparse matrix
A = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1) + 10I)
b = VectorMPI(randn(100))

# Solve A * x = b
x = A \ b</code></pre><h3 id="Symmetric-Systems-(Faster)"><a class="docs-heading-anchor" href="#Symmetric-Systems-(Faster)">Symmetric Systems (Faster)</a><a id="Symmetric-Systems-(Faster)-1"></a><a class="docs-heading-anchor-permalink" href="#Symmetric-Systems-(Faster)" title="Permalink"></a></h3><p>For symmetric matrices, wrap with <code>Symmetric</code> to use faster LDLT factorization:</p><pre><code class="language-julia hljs">using LinearAlgebra

# Create symmetric positive definite matrix
A_base = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))
A_spd = A_base + SparseMatrixMPI(transpose(A_base)) + 
        SparseMatrixMPI{Float64}(sparse(10.0I, 100, 100))

b = VectorMPI(randn(100))

# Use Symmetric wrapper for faster solve
x = Symmetric(A_spd) \ b</code></pre><h3 id="Reusing-Factorizations"><a class="docs-heading-anchor" href="#Reusing-Factorizations">Reusing Factorizations</a><a id="Reusing-Factorizations-1"></a><a class="docs-heading-anchor-permalink" href="#Reusing-Factorizations" title="Permalink"></a></h3><p>For repeated solves with the same matrix, compute the factorization once:</p><pre><code class="language-julia hljs">using LinearAlgebra

# LU factorization
F = lu(A)
x1 = F \ b1
x2 = F \ b2
finalize!(F)  # Clean up MUMPS resources

# LDLT factorization (symmetric matrices)
F = ldlt(A_spd)
x = F \ b
finalize!(F)</code></pre><h2 id="Threading"><a class="docs-heading-anchor" href="#Threading">Threading</a><a id="Threading-1"></a><a class="docs-heading-anchor-permalink" href="#Threading" title="Permalink"></a></h2><p>LinearAlgebraMPI uses two threading mechanisms for the MUMPS sparse direct solver:</p><ol><li><strong>OpenMP threads</strong> (<code>OMP_NUM_THREADS</code>) - Affects MUMPS algorithm-level parallelism</li><li><strong>BLAS threads</strong> (<code>OPENBLAS_NUM_THREADS</code>) - Affects dense matrix operations in both Julia and MUMPS</li></ol><h3 id="MUMPS-Solver-Threading"><a class="docs-heading-anchor" href="#MUMPS-Solver-Threading">MUMPS Solver Threading</a><a id="MUMPS-Solver-Threading-1"></a><a class="docs-heading-anchor-permalink" href="#MUMPS-Solver-Threading" title="Permalink"></a></h3><p>LinearAlgebraMPI uses the MUMPS (MUltifrontal Massively Parallel Solver) library for sparse direct solves via <code>lu()</code> and <code>ldlt()</code>. MUMPS has two independent threading mechanisms that can be tuned for performance.</p><p><strong>OpenMP threads (<code>OMP_NUM_THREADS</code>)</strong></p><ul><li>Controls MUMPS&#39;s algorithm-level parallelism</li><li>The multifrontal method builds an elimination tree of &quot;frontal matrices&quot;</li><li>OpenMP threads process independent subtrees in parallel</li><li>This is coarse-grained: different threads work on different parts of the matrix</li></ul><p><strong>BLAS threads (<code>OPENBLAS_NUM_THREADS</code>)</strong></p><ul><li>Controls parallelism inside dense matrix operations</li><li>When MUMPS factors a frontal matrix, it calls BLAS routines (DGEMM, etc.)</li><li>OpenBLAS can parallelize these dense operations</li><li>This is fine-grained: threads cooperate on the same dense block</li></ul><p><strong>Note on BLAS libraries</strong>: Julia and MUMPS use separate OpenBLAS libraries (<code>libopenblas64_.dylib</code> for Julia&#39;s ILP64 interface, <code>libopenblas.dylib</code> for MUMPS&#39;s LP64 interface). Both libraries read <code>OPENBLAS_NUM_THREADS</code> at initialization, so this environment variable affects both.</p><h3 id="Recommended-Configuration"><a class="docs-heading-anchor" href="#Recommended-Configuration">Recommended Configuration</a><a id="Recommended-Configuration-1"></a><a class="docs-heading-anchor-permalink" href="#Recommended-Configuration" title="Permalink"></a></h3><p>For behavior that closely matches Julia&#39;s built-in sparse solver (UMFPACK):</p><pre><code class="language-bash hljs">export OMP_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=&lt;number_of_cores&gt;</code></pre><p>This configuration uses only BLAS-level threading, which is the same strategy Julia&#39;s built-in solver uses.</p><h3 id="Performance-Comparison-(Single-Rank)"><a class="docs-heading-anchor" href="#Performance-Comparison-(Single-Rank)">Performance Comparison (Single-Rank)</a><a id="Performance-Comparison-(Single-Rank)-1"></a><a class="docs-heading-anchor-permalink" href="#Performance-Comparison-(Single-Rank)" title="Permalink"></a></h3><p>The following table compares MUMPS (<code>OMP_NUM_THREADS=1</code>, <code>OPENBLAS_NUM_THREADS=10</code>) against Julia&#39;s built-in sparse solver (also using the same settings) on a 2D Laplacian problem. <strong>This is a single-rank comparison</strong> to establish baseline overhead; multi-rank MPI parallelism provides additional speedup. Benchmarks were run on a 2025 M4 MacBook Pro with 10 CPU cores:</p><table><tr><th style="text-align: right">n</th><th style="text-align: right">Julia (ms)</th><th style="text-align: right">MUMPS (ms)</th><th style="text-align: right">Ratio</th></tr><tr><td style="text-align: right">9</td><td style="text-align: right">0.004</td><td style="text-align: right">0.024</td><td style="text-align: right">6.1x</td></tr><tr><td style="text-align: right">100</td><td style="text-align: right">0.020</td><td style="text-align: right">0.044</td><td style="text-align: right">2.3x</td></tr><tr><td style="text-align: right">961</td><td style="text-align: right">0.226</td><td style="text-align: right">0.276</td><td style="text-align: right">1.2x</td></tr><tr><td style="text-align: right">10,000</td><td style="text-align: right">3.99</td><td style="text-align: right">3.76</td><td style="text-align: right">0.9x</td></tr><tr><td style="text-align: right">99,856</td><td style="text-align: right">48.6</td><td style="text-align: right">44.8</td><td style="text-align: right">0.9x</td></tr><tr><td style="text-align: right">1,000,000</td><td style="text-align: right">597</td><td style="text-align: right">550</td><td style="text-align: right">0.9x</td></tr></table><p>Key observations:</p><ul><li>At small problem sizes, MUMPS has initialization overhead (~0.02ms)</li><li>At large problem sizes (n ≥ 10,000), MUMPS is <strong>faster</strong> than Julia&#39;s built-in solver</li><li>Cached symbolic analysis and vectorized value copying minimize repeated factorization overhead</li></ul><h3 id="Default-Behavior"><a class="docs-heading-anchor" href="#Default-Behavior">Default Behavior</a><a id="Default-Behavior-1"></a><a class="docs-heading-anchor-permalink" href="#Default-Behavior" title="Permalink"></a></h3><p>For optimal performance, set threading environment variables <strong>before starting Julia</strong>:</p><pre><code class="language-bash hljs">export OMP_NUM_THREADS=1
export OPENBLAS_NUM_THREADS=10  # or your number of CPU cores
julia your_script.jl</code></pre><p>Environment variables must be set before starting Julia because OpenBLAS creates its thread pool during library initialization. LinearAlgebraMPI attempts to set sensible defaults programmatically, but this may not always take effect if the thread pool is already initialized.</p><p>You can also add these to your shell profile (<code>.bashrc</code>, <code>.zshrc</code>, etc.) or Julia&#39;s <code>startup.jl</code>:</p><pre><code class="language-julia hljs"># In ~/.julia/config/startup.jl
ENV[&quot;OMP_NUM_THREADS&quot;] = &quot;1&quot;
ENV[&quot;OPENBLAS_NUM_THREADS&quot;] = string(Sys.CPU_THREADS)</code></pre><h3 id="Advanced:-Combined-OMP-and-BLAS-Threading"><a class="docs-heading-anchor" href="#Advanced:-Combined-OMP-and-BLAS-Threading">Advanced: Combined OMP and BLAS Threading</a><a id="Advanced:-Combined-OMP-and-BLAS-Threading-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced:-Combined-OMP-and-BLAS-Threading" title="Permalink"></a></h3><p>For some problems, combining OMP and BLAS threading can be faster:</p><pre><code class="language-bash hljs">export OMP_NUM_THREADS=4
export OPENBLAS_NUM_THREADS=4
julia your_script.jl</code></pre><p>This MUMPS configuration (OMP=4, BLAS=4) achieved 14% faster performance than Julia&#39;s built-in solver on a 1M DOF 2D Laplacian in testing. However, the optimal configuration depends on your specific problem structure and hardware.</p><p><strong>Important caveat</strong>: <code>OPENBLAS_NUM_THREADS</code> is a process-wide setting that affects both MUMPS and Julia&#39;s built-in sparse solver (UMFPACK). If you set <code>OPENBLAS_NUM_THREADS=4</code> to optimize MUMPS, Julia&#39;s built-in solver will also be limited to 4 BLAS threads.</p><h2 id="Row-wise-Operations-with-map_rows"><a class="docs-heading-anchor" href="#Row-wise-Operations-with-map_rows">Row-wise Operations with map_rows</a><a id="Row-wise-Operations-with-map_rows-1"></a><a class="docs-heading-anchor-permalink" href="#Row-wise-Operations-with-map_rows" title="Permalink"></a></h2><p>The <code>map_rows</code> function applies a function to corresponding rows across distributed arrays:</p><pre><code class="language-julia hljs">A = MatrixMPI(randn(50, 10))

# Compute row norms
norms = map_rows(row -&gt; norm(row), A)  # Returns VectorMPI

# Compute row sums and products
stats = map_rows(row -&gt; [sum(row), prod(row)]&#39;, A)  # Returns MatrixMPI

# Combine multiple inputs
v = VectorMPI(randn(50))
weighted = map_rows((row, w) -&gt; sum(row) * w[1], A, v)</code></pre><h3 id="Result-Types"><a class="docs-heading-anchor" href="#Result-Types">Result Types</a><a id="Result-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Result-Types" title="Permalink"></a></h3><table><tr><th style="text-align: right"><code>f</code> returns</th><th style="text-align: right">Result type</th></tr><tr><td style="text-align: right">Scalar</td><td style="text-align: right"><code>VectorMPI</code></td></tr><tr><td style="text-align: right">Column vector</td><td style="text-align: right"><code>VectorMPI</code> (concatenated)</td></tr><tr><td style="text-align: right">Row vector (<code>v&#39;</code>)</td><td style="text-align: right"><code>MatrixMPI</code></td></tr><tr><td style="text-align: right">Matrix</td><td style="text-align: right"><code>MatrixMPI</code></td></tr></table><h2 id="Type-Conversions"><a class="docs-heading-anchor" href="#Type-Conversions">Type Conversions</a><a id="Type-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#Type-Conversions" title="Permalink"></a></h2><h3 id="Gathering-to-Native-Types"><a class="docs-heading-anchor" href="#Gathering-to-Native-Types">Gathering to Native Types</a><a id="Gathering-to-Native-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Gathering-to-Native-Types" title="Permalink"></a></h3><p>Convert distributed types back to native Julia arrays (gathers data to all ranks):</p><pre><code class="language-julia hljs">v_mpi = VectorMPI(randn(100))
v_native = Vector(v_mpi)  # Full vector on all ranks

A_mpi = MatrixMPI(randn(50, 30))
A_native = Matrix(A_mpi)  # Full matrix on all ranks

S_mpi = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))
S_native = SparseMatrixCSC(S_mpi)  # Full sparse matrix</code></pre><h2 id="IO-and-Output"><a class="docs-heading-anchor" href="#IO-and-Output">IO and Output</a><a id="IO-and-Output-1"></a><a class="docs-heading-anchor-permalink" href="#IO-and-Output" title="Permalink"></a></h2><h3 id="Printing-from-Rank-0"><a class="docs-heading-anchor" href="#Printing-from-Rank-0">Printing from Rank 0</a><a id="Printing-from-Rank-0-1"></a><a class="docs-heading-anchor-permalink" href="#Printing-from-Rank-0" title="Permalink"></a></h3><p>Use <code>io0()</code> to print from rank 0 only:</p><pre><code class="language-julia hljs">println(io0(), &quot;This prints once from rank 0!&quot;)

# Custom rank selection
println(io0(r=Set([0, 1])), &quot;Hello from ranks 0 and 1!&quot;)</code></pre><h3 id="MPI-Rank-Information"><a class="docs-heading-anchor" href="#MPI-Rank-Information">MPI Rank Information</a><a id="MPI-Rank-Information-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-Rank-Information" title="Permalink"></a></h3><pre><code class="language-julia hljs">using MPI

rank = MPI.Comm_rank(MPI.COMM_WORLD)   # Current rank (0 to nranks-1)
nranks = MPI.Comm_size(MPI.COMM_WORLD) # Total number of ranks</code></pre><h2 id="Repartitioning"><a class="docs-heading-anchor" href="#Repartitioning">Repartitioning</a><a id="Repartitioning-1"></a><a class="docs-heading-anchor-permalink" href="#Repartitioning" title="Permalink"></a></h2><p>Redistribute data to match a different partition:</p><pre><code class="language-julia hljs">v = VectorMPI(randn(100))

# Get current partition
old_partition = v.partition

# Create new partition
new_partition = uniform_partition(100, MPI.Comm_size(MPI.COMM_WORLD))

# Repartition
v_new = repartition(v, new_partition)</code></pre><h2 id="GPU-Support-(Metal)"><a class="docs-heading-anchor" href="#GPU-Support-(Metal)">GPU Support (Metal)</a><a id="GPU-Support-(Metal)-1"></a><a class="docs-heading-anchor-permalink" href="#GPU-Support-(Metal)" title="Permalink"></a></h2><p>LinearAlgebraMPI supports GPU acceleration on macOS via Metal.jl. GPU support is optional and loaded as a package extension.</p><h3 id="Setup"><a class="docs-heading-anchor" href="#Setup">Setup</a><a id="Setup-1"></a><a class="docs-heading-anchor-permalink" href="#Setup" title="Permalink"></a></h3><p>Load Metal <strong>before</strong> MPI for proper GPU detection:</p><pre><code class="language-julia hljs">using Metal  # Load first!
using MPI
MPI.Init()
using LinearAlgebraMPI</code></pre><h3 id="Converting-Between-CPU-and-GPU"><a class="docs-heading-anchor" href="#Converting-Between-CPU-and-GPU">Converting Between CPU and GPU</a><a id="Converting-Between-CPU-and-GPU-1"></a><a class="docs-heading-anchor-permalink" href="#Converting-Between-CPU-and-GPU" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Create CPU vector
x_cpu = VectorMPI(Float32.(rand(1000)))

# Convert to GPU
x_gpu = mtl(x_cpu)  # Returns VectorMPI{Float32, MtlVector{Float32}}

# GPU operations work transparently
y_gpu = x_gpu + x_gpu  # Native GPU addition
z_gpu = 2.0f0 * x_gpu  # Native GPU scalar multiply

# Convert back to CPU
y_cpu = cpu(y_gpu)</code></pre><h3 id="How-It-Works"><a class="docs-heading-anchor" href="#How-It-Works">How It Works</a><a id="How-It-Works-1"></a><a class="docs-heading-anchor-permalink" href="#How-It-Works" title="Permalink"></a></h3><p>The type parameter <code>AV</code> (or <code>AM</code> for matrices) determines where data lives:</p><table><tr><th style="text-align: right">Type</th><th style="text-align: right">Storage</th><th style="text-align: right">Operations</th></tr><tr><td style="text-align: right"><code>VectorMPI{T, Vector{T}}</code></td><td style="text-align: right">CPU</td><td style="text-align: right">Native CPU</td></tr><tr><td style="text-align: right"><code>VectorMPI{T, MtlVector{T}}</code></td><td style="text-align: right">GPU</td><td style="text-align: right">Native GPU for vector ops</td></tr><tr><td style="text-align: right"><code>MatrixMPI{T, Matrix{T}}</code></td><td style="text-align: right">CPU</td><td style="text-align: right">Native CPU</td></tr><tr><td style="text-align: right"><code>MatrixMPI{T, MtlMatrix{T}}</code></td><td style="text-align: right">GPU</td><td style="text-align: right">Native GPU</td></tr></table><h3 id="MPI-Communication"><a class="docs-heading-anchor" href="#MPI-Communication">MPI Communication</a><a id="MPI-Communication-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-Communication" title="Permalink"></a></h3><p>MPI always uses CPU buffers (no Metal-aware MPI exists). GPU data is automatically staged through CPU:</p><ol><li>GPU vector data copied to CPU staging buffer</li><li>MPI communication on CPU buffers</li><li>Results copied back to GPU</li></ol><p>This is handled transparently - you just use the same operations.</p><h3 id="Sparse-Matrix-Operations"><a class="docs-heading-anchor" href="#Sparse-Matrix-Operations">Sparse Matrix Operations</a><a id="Sparse-Matrix-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-Matrix-Operations" title="Permalink"></a></h3><p>Sparse matrices (<code>SparseMatrixMPI</code>) remain on CPU. When multiplying with GPU vectors:</p><pre><code class="language-julia hljs">A = SparseMatrixMPI{Float32}(sprand(Float32, 100, 100, 0.1))
x_gpu = mtl(VectorMPI(Float32.(rand(100))))

# Sparse multiply: x gathered via CPU, multiply on CPU, result copied to GPU
y_gpu = A * x_gpu  # Returns VectorMPI{Float32, MtlVector{Float32}}</code></pre><h3 id="Supported-GPU-Operations"><a class="docs-heading-anchor" href="#Supported-GPU-Operations">Supported GPU Operations</a><a id="Supported-GPU-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Supported-GPU-Operations" title="Permalink"></a></h3><table><tr><th style="text-align: right">Operation</th><th style="text-align: right">GPU Support</th></tr><tr><td style="text-align: right"><code>v + w</code>, <code>v - w</code></td><td style="text-align: right">Native GPU</td></tr><tr><td style="text-align: right"><code>α * v</code> (scalar)</td><td style="text-align: right">Native GPU</td></tr><tr><td style="text-align: right"><code>A * x</code> (sparse)</td><td style="text-align: right">CPU staging</td></tr><tr><td style="text-align: right"><code>A * x</code> (dense)</td><td style="text-align: right">CPU staging</td></tr><tr><td style="text-align: right"><code>transpose(A) * x</code></td><td style="text-align: right">CPU staging</td></tr><tr><td style="text-align: right">Broadcasting (<code>abs.(v)</code>)</td><td style="text-align: right">Native GPU</td></tr></table><h3 id="Element-Types"><a class="docs-heading-anchor" href="#Element-Types">Element Types</a><a id="Element-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Element-Types" title="Permalink"></a></h3><p>Metal requires <code>Float32</code> - it does not support <code>Float64</code>.</p><h2 id="Cache-Management"><a class="docs-heading-anchor" href="#Cache-Management">Cache Management</a><a id="Cache-Management-1"></a><a class="docs-heading-anchor-permalink" href="#Cache-Management" title="Permalink"></a></h2><p>LinearAlgebraMPI caches communication plans for efficiency. Clear caches when needed:</p><pre><code class="language-julia hljs">clear_plan_cache!()  # Clears all plan caches including MUMPS analysis cache</code></pre><h2 id="MPI-Collective-Operations"><a class="docs-heading-anchor" href="#MPI-Collective-Operations">MPI Collective Operations</a><a id="MPI-Collective-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-Collective-Operations" title="Permalink"></a></h2><div class="admonition is-warning" id="All-Operations-Are-Collective-1bb7003cbb73574d"><header class="admonition-header">All Operations Are Collective<a class="admonition-anchor" href="#All-Operations-Are-Collective-1bb7003cbb73574d" title="Permalink"></a></header><div class="admonition-body"><p>Most LinearAlgebraMPI functions are MPI collective operations. All ranks must:</p><ul><li>Call the function together</li><li>Use the same parameters</li><li>Avoid conditional execution based on rank</li></ul></div></div><p><strong>Correct:</strong></p><pre><code class="language-julia hljs"># All ranks execute this together
x = A \ b</code></pre><p><strong>Incorrect (causes deadlock):</strong></p><pre><code class="language-julia hljs">if rank == 0
    x = A \ b  # Only rank 0 calls - DEADLOCK!
end</code></pre><h2 id="Next-Steps"><a class="docs-heading-anchor" href="#Next-Steps">Next Steps</a><a id="Next-Steps-1"></a><a class="docs-heading-anchor-permalink" href="#Next-Steps" title="Permalink"></a></h2><ul><li>See <a href="../examples/#Examples">Examples</a> for detailed code examples</li><li>See the <a href="../api/#API-Reference">API Reference</a> for detailed function documentation</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../installation/">« Installation</a><a class="docs-footer-nextpage" href="../examples/">Examples »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 26 December 2025 13:27">Friday 26 December 2025</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
