var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"This page documents the public API of LinearAlgebraMPI.jl.","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"","category":"section"},{"location":"api/#SparseMatrixMPI","page":"API Reference","title":"SparseMatrixMPI","text":"","category":"section"},{"location":"api/#MatrixMPI","page":"API Reference","title":"MatrixMPI","text":"","category":"section"},{"location":"api/#VectorMPI","page":"API Reference","title":"VectorMPI","text":"","category":"section"},{"location":"api/#Sparse-Matrix-Operations","page":"API Reference","title":"Sparse Matrix Operations","text":"","category":"section"},{"location":"api/#Arithmetic","page":"API Reference","title":"Arithmetic","text":"A * B          # Matrix multiplication\nA + B          # Addition\nA - B          # Subtraction\na * A          # Scalar multiplication\nA * a          # Scalar multiplication","category":"section"},{"location":"api/#Threaded-Sparse-Multiplication","page":"API Reference","title":"Threaded Sparse Multiplication","text":"The ⊛ operator (typed as \\circledast<tab>) provides multithreaded sparse matrix multiplication for SparseMatrixCSC. It is used internally by SparseMatrixMPI multiplication and can also be used directly on local sparse matrices:\n\nusing SparseArrays\nA = sprand(10000, 10000, 0.001)\nB = sprand(10000, 5000, 0.001)\nC = A ⊛ B   # Parallel multiplication using available threads","category":"section"},{"location":"api/#Transpose-and-Adjoint","page":"API Reference","title":"Transpose and Adjoint","text":"transpose(A)   # Lazy transpose\nconj(A)        # Conjugate (new matrix)\nA'             # Adjoint (conjugate transpose, lazy)","category":"section"},{"location":"api/#Matrix-Vector-Multiplication","page":"API Reference","title":"Matrix-Vector Multiplication","text":"y = A * x      # Returns VectorMPI\nmul!(y, A, x)  # In-place version","category":"section"},{"location":"api/#Vector-Matrix-Multiplication","page":"API Reference","title":"Vector-Matrix Multiplication","text":"transpose(v) * A   # Row vector times matrix\nv' * A             # Conjugate row vector times matrix","category":"section"},{"location":"api/#Norms","page":"API Reference","title":"Norms","text":"norm(A)        # Frobenius norm (default)\nnorm(A, 1)     # Sum of absolute values\nnorm(A, Inf)   # Maximum absolute value\nnorm(A, p)     # General p-norm\n\nopnorm(A, 1)   # Maximum absolute column sum\nopnorm(A, Inf) # Maximum absolute row sum","category":"section"},{"location":"api/#Properties","page":"API Reference","title":"Properties","text":"size(A)        # Global dimensions (m, n)\nsize(A, d)     # Size along dimension d\neltype(A)      # Element type\nnnz(A)         # Number of nonzeros\nissparse(A)    # Returns true","category":"section"},{"location":"api/#Reductions","page":"API Reference","title":"Reductions","text":"sum(A)         # Sum of all stored elements\nsum(A; dims=1) # Column sums (returns VectorMPI) - SparseMatrixMPI only\nsum(A; dims=2) # Row sums (returns VectorMPI) - SparseMatrixMPI only\nmaximum(A)     # Maximum of stored values\nminimum(A)     # Minimum of stored values\ntr(A)          # Trace (sum of diagonal) - SparseMatrixMPI only","category":"section"},{"location":"api/#Element-wise-Operations","page":"API Reference","title":"Element-wise Operations","text":"abs(A)         # Absolute value\nabs2(A)        # Squared absolute value\nreal(A)        # Real part\nimag(A)        # Imaginary part\nfloor(A)       # Floor\nceil(A)        # Ceiling\nround(A)       # Round","category":"section"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"copy(A)        # Deep copy\ndropzeros(A)   # Remove stored zeros\ndiag(A)        # Main diagonal (returns VectorMPI)\ndiag(A, k)     # k-th diagonal\ntriu(A)        # Upper triangular\ntriu(A, k)     # Upper triangular from k-th diagonal\ntril(A)        # Lower triangular\ntril(A, k)     # Lower triangular from k-th diagonal","category":"section"},{"location":"api/#Block-Operations","page":"API Reference","title":"Block Operations","text":"cat(A, B, C; dims=1)       # Vertical concatenation\ncat(A, B, C; dims=2)       # Horizontal concatenation\ncat(A, B, C, D; dims=(2,2)) # 2x2 block matrix [A B; C D]\nvcat(A, B, C)              # Vertical concatenation\nhcat(A, B, C)              # Horizontal concatenation\nblockdiag(A, B, C)         # Block diagonal matrix","category":"section"},{"location":"api/#Diagonal-Matrix-Construction","page":"API Reference","title":"Diagonal Matrix Construction","text":"spdiagm(v)                 # Diagonal matrix from VectorMPI\nspdiagm(m, n, v)           # m x n diagonal matrix\nspdiagm(k => v)            # k-th diagonal\nspdiagm(0 => v, 1 => w)    # Multiple diagonals","category":"section"},{"location":"api/#Dense-Matrix-Operations","page":"API Reference","title":"Dense Matrix Operations","text":"","category":"section"},{"location":"api/#Arithmetic-2","page":"API Reference","title":"Arithmetic","text":"A * x          # Matrix-vector multiplication (returns VectorMPI)\ntranspose(A)   # Lazy transpose\nconj(A)        # Conjugate\nA'             # Adjoint\na * A          # Scalar multiplication","category":"section"},{"location":"api/#mapslices","page":"API Reference","title":"mapslices","text":"Apply a function to rows or columns of a distributed dense matrix.\n\nmapslices(f, A; dims=2)   # Apply f to each row (local, no MPI)\nmapslices(f, A; dims=1)   # Apply f to each column (requires MPI)\n\nExample:\n\nusing LinearAlgebra\n\n# Create deterministic test matrix (same on all ranks)\nA_global = Float64.([i + 0.1*j for i in 1:100, j in 1:10])\nA = MatrixMPI(A_global)\n\n# Compute row statistics: norm, max, sum for each row\n# Transforms 100x10 to 100x3\nB = mapslices(x -> [norm(x), maximum(x), sum(x)], A; dims=2)","category":"section"},{"location":"api/#Block-Operations-2","page":"API Reference","title":"Block Operations","text":"cat(A, B; dims=1)          # Vertical concatenation\ncat(A, B; dims=2)          # Horizontal concatenation\nvcat(A, B)                 # Vertical concatenation\nhcat(A, B)                 # Horizontal concatenation","category":"section"},{"location":"api/#Norms-2","page":"API Reference","title":"Norms","text":"norm(A)        # Frobenius norm\nnorm(A, p)     # General p-norm\nopnorm(A, 1)   # Maximum absolute column sum\nopnorm(A, Inf) # Maximum absolute row sum","category":"section"},{"location":"api/#Vector-Operations","page":"API Reference","title":"Vector Operations","text":"","category":"section"},{"location":"api/#Arithmetic-3","page":"API Reference","title":"Arithmetic","text":"u + v          # Addition (auto-aligns partitions)\nu - v          # Subtraction\n-v             # Negation\na * v          # Scalar multiplication\nv * a          # Scalar multiplication\nv / a          # Scalar division","category":"section"},{"location":"api/#Transpose-and-Adjoint-2","page":"API Reference","title":"Transpose and Adjoint","text":"transpose(v)   # Lazy transpose (row vector)\nconj(v)        # Conjugate\nv'             # Adjoint","category":"section"},{"location":"api/#Norms-3","page":"API Reference","title":"Norms","text":"norm(v)        # 2-norm (default)\nnorm(v, 1)     # 1-norm\nnorm(v, Inf)   # Infinity norm\nnorm(v, p)     # General p-norm","category":"section"},{"location":"api/#Reductions-2","page":"API Reference","title":"Reductions","text":"sum(v)         # Sum of elements\nprod(v)        # Product of elements\nmaximum(v)     # Maximum element\nminimum(v)     # Minimum element\nmean(v)        # Mean of elements","category":"section"},{"location":"api/#Element-wise-Operations-2","page":"API Reference","title":"Element-wise Operations","text":"abs(v)         # Absolute value\nabs2(v)        # Squared absolute value\nreal(v)        # Real part\nimag(v)        # Imaginary part\ncopy(v)        # Deep copy","category":"section"},{"location":"api/#Broadcasting","page":"API Reference","title":"Broadcasting","text":"VectorMPI supports broadcasting for element-wise operations:\n\nv .+ w         # Element-wise addition\nv .* w         # Element-wise multiplication\nsin.(v)        # Apply function element-wise\nv .* 2.0 .+ w  # Compound expressions","category":"section"},{"location":"api/#Block-Operations-3","page":"API Reference","title":"Block Operations","text":"vcat(u, v, w)  # Concatenate vectors (returns VectorMPI)\nhcat(u, v, w)  # Stack as columns (returns MatrixMPI)","category":"section"},{"location":"api/#Properties-2","page":"API Reference","title":"Properties","text":"length(v)      # Global length\nsize(v)        # Returns (length,)\neltype(v)      # Element type","category":"section"},{"location":"api/#Indexing","page":"API Reference","title":"Indexing","text":"All distributed types support element access and assignment. These are collective operations - all MPI ranks must call them with the same arguments.","category":"section"},{"location":"api/#VectorMPI-Indexing","page":"API Reference","title":"VectorMPI Indexing","text":"v[i]           # Get element (collective)\nv[i] = x       # Set element (collective)\nv[1:10]        # Range indexing (returns VectorMPI)\nv[1:10] = x    # Range assignment (scalar or vector)\nv[idx]         # VectorMPI{Int} indexing (returns VectorMPI)\nv[idx] = src   # VectorMPI{Int} assignment (src::VectorMPI)","category":"section"},{"location":"api/#MatrixMPI-Indexing","page":"API Reference","title":"MatrixMPI Indexing","text":"# Single element\nA[i, j]        # Get element\nA[i, j] = x    # Set element\n\n# Range indexing (returns MatrixMPI)\nA[1:3, 2:5]    # Submatrix by ranges\nA[1:3, :]      # Row range, all columns\nA[:, 2:5]      # All rows, column range\n\n# VectorMPI indices (returns MatrixMPI)\nA[row_idx, col_idx]  # Both indices are VectorMPI{Int}\n\n# Mixed indexing (returns MatrixMPI or VectorMPI)\nA[row_idx, 1:5]      # VectorMPI rows, range columns\nA[row_idx, :]        # VectorMPI rows, all columns\nA[1:5, col_idx]      # Range rows, VectorMPI columns\nA[:, col_idx]        # All rows, VectorMPI columns\nA[row_idx, j]        # VectorMPI rows, single column (returns VectorMPI)\nA[i, col_idx]        # Single row, VectorMPI columns (returns VectorMPI)","category":"section"},{"location":"api/#SparseMatrixMPI-Indexing","page":"API Reference","title":"SparseMatrixMPI Indexing","text":"# Single element\nA[i, j]        # Get element (returns 0 for structural zeros)\nA[i, j] = x    # Set element (modifies structure if needed)\n\n# Range indexing (returns SparseMatrixMPI)\nA[1:3, 2:5]    # Submatrix by ranges\nA[1:3, :]      # Row range, all columns\nA[:, 2:5]      # All rows, column range\n\n# VectorMPI indices (returns SparseMatrixMPI)\nA[row_idx, col_idx]  # Both indices are VectorMPI{Int}\n\n# Mixed indexing (returns SparseMatrixMPI or VectorMPI)\nA[row_idx, 1:5]      # VectorMPI rows, range columns\nA[row_idx, :]        # VectorMPI rows, all columns\nA[1:5, col_idx]      # Range rows, VectorMPI columns\nA[:, col_idx]        # All rows, VectorMPI columns\nA[row_idx, j]        # VectorMPI rows, single column (returns VectorMPI)\nA[i, col_idx]        # Single row, VectorMPI columns (returns VectorMPI)","category":"section"},{"location":"api/#setindex!-Source-Types","page":"API Reference","title":"setindex! Source Types","text":"For setindex! operations, the source type depends on the indexing pattern:\n\nIndex Pattern Source Type\nA[i, j] = x Scalar\nA[range, range] = x Scalar, Matrix, or distributed matrix\nA[VectorMPI, VectorMPI] = src MatrixMPI (matching partitions)\nA[VectorMPI, range] = src MatrixMPI\nA[range, VectorMPI] = src MatrixMPI\nA[VectorMPI, j] = src VectorMPI\nA[i, VectorMPI] = src VectorMPI","category":"section"},{"location":"api/#Utility-Functions","page":"API Reference","title":"Utility Functions","text":"","category":"section"},{"location":"api/#Partition-Computation","page":"API Reference","title":"Partition Computation","text":"","category":"section"},{"location":"api/#Rank-Selective-Output","page":"API Reference","title":"Rank-Selective Output","text":"","category":"section"},{"location":"api/#Gathering-Distributed-Data","page":"API Reference","title":"Gathering Distributed Data","text":"Convert distributed MPI types to standard Julia types (gathers data to all ranks):\n\nVector(v::VectorMPI)              # Gather to Vector\nMatrix(A::MatrixMPI)              # Gather to Matrix\nSparseMatrixCSC(A::SparseMatrixMPI) # Gather to SparseMatrixCSC\n\nThese conversions enable show and string interpolation:\n\nprintln(io0(), \"Result: $v\")      # Works with VectorMPI\nprintln(io0(), \"Matrix: $A\")      # Works with MatrixMPI/SparseMatrixMPI","category":"section"},{"location":"api/#Local-Constructors","page":"API Reference","title":"Local Constructors","text":"Create distributed types from local data (each rank provides only its portion):","category":"section"},{"location":"api/#Factorization","page":"API Reference","title":"Factorization","text":"LinearAlgebraMPI provides distributed sparse direct solvers using MUMPS (MUltifrontal Massively Parallel Solver).","category":"section"},{"location":"api/#LU-Factorization","page":"API Reference","title":"LU Factorization","text":"F = lu(A::SparseMatrixMPI{T})\n\nCompute LU factorization of a distributed sparse matrix using MUMPS. Suitable for general (non-symmetric) matrices.","category":"section"},{"location":"api/#LDLT-Factorization","page":"API Reference","title":"LDLT Factorization","text":"F = ldlt(A::SparseMatrixMPI{T})\n\nCompute LDLT factorization of a distributed symmetric sparse matrix using MUMPS. More efficient than LU for symmetric matrices.\n\nNote: Uses transpose (L^T), not adjoint (L*). Correct for real symmetric and complex symmetric matrices, but NOT for complex Hermitian matrices.","category":"section"},{"location":"api/#Solving-Linear-Systems","page":"API Reference","title":"Solving Linear Systems","text":"","category":"section"},{"location":"api/#Releasing-Factorization-Resources","page":"API Reference","title":"Releasing Factorization Resources","text":"","category":"section"},{"location":"api/#Usage-Examples","page":"API Reference","title":"Usage Examples","text":"using LinearAlgebraMPI\nusing LinearAlgebra\nusing SparseArrays\n\n# Create a distributed sparse matrix\nA_local = sprand(1000, 1000, 0.01) + 10I\nA_local = A_local + A_local'  # Make symmetric\nA = SparseMatrixMPI{Float64}(A_local)\n\n# LDLT factorization (for symmetric matrices)\nF = ldlt(A)\n\n# Solve Ax = b\nb = VectorMPI(ones(1000))\nx = solve(F, b)\n\n# Or use backslash\nx = F \\ b\n\n# Release factorization resources when done\nfinalize!(F)\n\n# For non-symmetric matrices, use LU\nA_nonsym = SparseMatrixMPI{Float64}(sprand(1000, 1000, 0.01) + 10I)\nF_lu = lu(A_nonsym)\nx = F_lu \\ b\nfinalize!(F_lu)","category":"section"},{"location":"api/#Direct-Solve-Syntax","page":"API Reference","title":"Direct Solve Syntax","text":"Both left division (\\) and right division (/) are supported:\n\n# Left division: solve A*x = b\nx = A \\ b\nx = transpose(A) \\ b    # solve transpose(A)*x = b\nx = A' \\ b              # solve A'*x = b\n\n# Right division: solve x*A = b (for row vectors)\nx = transpose(b) / A           # solve x*A = transpose(b)\nx = transpose(b) / transpose(A)  # solve x*transpose(A) = transpose(b)\n\nNote: One-shot solves like A \\ b automatically clean up the factorization. For repeated solves with the same matrix, compute the factorization once with lu() or ldlt(), reuse it, then call finalize!() when done.","category":"section"},{"location":"api/#Cache-Management","page":"API Reference","title":"Cache Management","text":"","category":"section"},{"location":"api/#Full-API-Index","page":"API Reference","title":"Full API Index","text":"","category":"section"},{"location":"api/#LinearAlgebraMPI.SparseMatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.SparseMatrixMPI","text":"SparseMatrixMPI{T}\n\nA distributed sparse matrix partitioned by rows across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (placeholder for transpose)\ncol_indices::Vector{Int}: Global column indices that appear in the local part (local→global mapping)\nA::Transpose{T,SparseMatrixCSC{T,Int}}: Local rows as a lazy transpose wrapper around compressed CSC storage\n\nInvariants\n\ncol_indices, row_partition, and col_partition are sorted\nrow_partition[nranks+1] = total number of rows\ncol_partition[nranks+1] = total number of columns\nsize(A, 1) == row_partition[rank+1] - row_partition[rank] (number of local rows)\nsize(A.parent, 1) == length(col_indices) (compressed column dimension)\nA.parent.rowval contains local indices in 1:length(col_indices)\n\nStorage Details\n\nThe local rows are stored in compressed form as A = transpose(AT) where AT::SparseMatrixCSC has:\n\nAT.m = length(col_indices) (compressed, not global ncols)\nAT.n = number of local rows\nAT.rowval contains LOCAL column indices (1:length(col_indices))\ncol_indices[local_idx] maps local→global column indices\n\nThis compression avoids \"hypersparse\" storage where AT.m >> length(unique(AT.rowval)), which would cause excessive allocations in matrix operations.\n\nAccess the underlying CSC via A.parent when needed for low-level operations.\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.MatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.MatrixMPI","text":"MatrixMPI{T}\n\nA distributed dense matrix partitioned by rows across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (for transpose)\nA::Matrix{T}: Local rows (NOT transposed), size = (local_nrows, ncols)\n\nInvariants\n\nrow_partition and col_partition are sorted\nrow_partition[nranks+1] = total number of rows + 1\ncol_partition[nranks+1] = total number of columns + 1\nsize(A, 1) == row_partition[rank+2] - row_partition[rank+1]\nsize(A, 2) == col_partition[end] - 1\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.VectorMPI","page":"API Reference","title":"LinearAlgebraMPI.VectorMPI","text":"VectorMPI{T}\n\nA distributed dense vector partitioned across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the partition\npartition::Vector{Int}: Partition boundaries, length = nranks + 1\nv::Vector{T}: Local vector elements owned by this rank\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.:⊛","page":"API Reference","title":"LinearAlgebraMPI.:⊛","text":"⊛(A::SparseMatrixCSC{Tv,Ti}, B::SparseMatrixCSC{Tv,Ti}; max_threads=Threads.nthreads()) where {Tv,Ti}\n\nMultithreaded sparse matrix multiplication. Splits B into column blocks and computes A * B_block in parallel using Julia's optimized builtin *.\n\nThreading behavior\n\nUses at most n ÷ 100 threads, where n = size(B, 2), ensuring at least 100 columns per thread\nFalls back to single-threaded A * B when n < 100 or when threading overhead would dominate\nThe max_threads keyword limits the maximum number of threads used\n\nExamples\n\nusing SparseArrays\nA = sprand(1000, 1000, 0.01)\nB = sprand(1000, 500, 0.01)\nC = A ⊛ B                    # Use all available threads (up to n÷100)\nC = ⊛(A, B; max_threads=2)   # Limit to 2 threads\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.mean","page":"API Reference","title":"LinearAlgebraMPI.mean","text":"mean(v::VectorMPI{T}) where T\n\nCompute the mean of all elements in the distributed vector.\n\n\n\n\n\nmean(A::SparseMatrixMPI{T}) where T\n\nCompute the mean of all elements (including implicit zeros) in the distributed sparse matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.uniform_partition","page":"API Reference","title":"LinearAlgebraMPI.uniform_partition","text":"uniform_partition(n::Int, nranks::Int) -> Vector{Int}\n\nCompute a balanced partition of n elements across nranks ranks. Returns a vector of length nranks + 1 with 1-indexed partition boundaries.\n\nThe first mod(n, nranks) ranks get div(n, nranks) + 1 elements, the remaining ranks get div(n, nranks) elements.\n\nExample\n\npartition = uniform_partition(10, 4)  # [1, 4, 7, 9, 11]\n# Rank 0: 1:3 (3 elements)\n# Rank 1: 4:6 (3 elements)\n# Rank 2: 7:8 (2 elements)\n# Rank 3: 9:10 (2 elements)\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.io0","page":"API Reference","title":"LinearAlgebraMPI.io0","text":"io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)\n\nReturn io if the current MPI rank is in set r, otherwise return dn (default: devnull).\n\nThis is useful for printing only from specific ranks:\n\nprintln(io0(), \"Hello from rank 0!\")\nprintln(io0(r=Set([0,1])), \"Hello from ranks 0 and 1!\")\n\nWith string interpolation:\n\nprintln(io0(), \"Matrix A = $A\")\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.VectorMPI_local","page":"API Reference","title":"LinearAlgebraMPI.VectorMPI_local","text":"VectorMPI_local(v_local::Vector{T}, comm::MPI.Comm=MPI.COMM_WORLD) where T\n\nCreate a VectorMPI from a local vector on each rank.\n\nUnlike VectorMPI(v_global) which takes a global vector and partitions it, this constructor takes only the local portion of the vector that each rank owns. The partition is computed by gathering the local sizes from all ranks.\n\nExample\n\n# Rank 0 has [1.0, 2.0], Rank 1 has [3.0, 4.0, 5.0]\nv = VectorMPI_local([1.0, 2.0])  # on rank 0\nv = VectorMPI_local([3.0, 4.0, 5.0])  # on rank 1\n# Result: distributed vector [1.0, 2.0, 3.0, 4.0, 5.0] with partition [1, 3, 6]\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.MatrixMPI_local","page":"API Reference","title":"LinearAlgebraMPI.MatrixMPI_local","text":"MatrixMPI_local(A_local::Matrix{T}; comm=MPI.COMM_WORLD, col_partition=...) where T\n\nCreate a MatrixMPI from a local matrix on each rank.\n\nUnlike MatrixMPI(M_global) which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.\n\nAll ranks must have local matrices with the same number of columns. A collective error is raised if the column counts don't match.\n\nKeyword Arguments\n\ncomm::MPI.Comm: MPI communicator (default: MPI.COMM_WORLD)\ncol_partition::Vector{Int}: Column partition boundaries (default: uniform_partition(size(A_local,2), nranks))\n\nExample\n\n# Rank 0 has 2×3 matrix, Rank 1 has 3×3 matrix\nA = MatrixMPI_local(randn(2, 3))  # on rank 0\nA = MatrixMPI_local(randn(3, 3))  # on rank 1\n# Result: 5×3 distributed matrix with row_partition [1, 3, 6]\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.SparseMatrixMPI_local","page":"API Reference","title":"LinearAlgebraMPI.SparseMatrixMPI_local","text":"SparseMatrixMPI_local(A_local::Transpose{T,SparseMatrixCSC{T,Int}}; comm=MPI.COMM_WORLD, col_partition=...) where T\nSparseMatrixMPI_local(A_local::Adjoint{T,SparseMatrixCSC{T,Int}}; comm=MPI.COMM_WORLD, col_partition=...) where T\n\nCreate a SparseMatrixMPI from a local sparse matrix on each rank.\n\nUnlike SparseMatrixMPI{T}(A_global) which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.\n\nThe input A_local must be a Transpose (or Adjoint) of a SparseMatrixCSC{T,Int} where:\n\nA_local.parent.n = number of local rows on this rank\nA_local.parent.m = global number of columns (must match on all ranks)\nA_local.parent.rowval = global column indices\n\nAll ranks must have local matrices with the same number of columns (block widths must match). A collective error is raised if the column counts don't match.\n\nNote: For Adjoint inputs, the values are conjugated to match the adjoint semantics.\n\nKeyword Arguments\n\ncomm::MPI.Comm: MPI communicator (default: MPI.COMM_WORLD)\ncol_partition::Vector{Int}: Column partition boundaries (default: uniform_partition(A_local.parent.m, nranks))\n\nExample\n\n# Create local rows as transpose of CSC storage\n# Rank 0 owns rows 1-2 of a 5×3 matrix, Rank 1 owns rows 3-5\nlocal_AT = sparse([1, 2, 3], [1, 1, 2], [1.0, 2.0, 3.0], 3, 2)  # 3 cols, 2 local rows\nA = SparseMatrixMPI_local(transpose(local_AT))\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.solve","page":"API Reference","title":"LinearAlgebraMPI.solve","text":"solve(F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T\n\nSolve the linear system A*x = b using the precomputed MUMPS factorization.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.solve!","page":"API Reference","title":"LinearAlgebraMPI.solve!","text":"solve!(x::VectorMPI{T}, F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T\n\nSolve A*x = b in-place using MUMPS factorization.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.finalize!","page":"API Reference","title":"LinearAlgebraMPI.finalize!","text":"finalize!(F::MUMPSFactorizationMPI)\n\nRelease MUMPS resources. Must be called when done with the factorization.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.clear_plan_cache!","page":"API Reference","title":"LinearAlgebraMPI.clear_plan_cache!","text":"clear_plan_cache!()\n\nClear all memoized plan caches.\n\n\n\n\n\n","category":"function"},{"location":"getting-started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This guide will walk you through the basics of using LinearAlgebraMPI.jl for distributed sparse matrix computations.","category":"section"},{"location":"getting-started/#Prerequisites","page":"Getting Started","title":"Prerequisites","text":"Before using LinearAlgebraMPI.jl, ensure you have:\n\nA working MPI installation (OpenMPI, MPICH, or Intel MPI)\nMPI.jl configured to use your MPI installation\n\nYou can verify your MPI setup with:\n\nusing MPI\nMPI.Init()\nprintln(\"Rank $(MPI.Comm_rank(MPI.COMM_WORLD)) of $(MPI.Comm_size(MPI.COMM_WORLD))\")\n\nRun with:\n\nmpiexec -n 4 julia --project=. your_script.jl","category":"section"},{"location":"getting-started/#Creating-Distributed-Matrices","page":"Getting Started","title":"Creating Distributed Matrices","text":"","category":"section"},{"location":"getting-started/#From-a-Global-Sparse-Matrix","page":"Getting Started","title":"From a Global Sparse Matrix","text":"The most common way to create a distributed matrix is from an existing SparseMatrixCSC:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create a sparse matrix - MUST be identical on all ranks\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\n\n# Distribute across MPI ranks\nAdist = SparseMatrixMPI{Float64}(A)\n\nImportant: All MPI ranks must have the same matrix size when constructing distributed types. However, each rank only extracts its own local rows, so the actual data only needs to be correct for each rank's portion.","category":"section"},{"location":"getting-started/#Understanding-Row-Partitioning","page":"Getting Started","title":"Understanding Row Partitioning","text":"The matrix is partitioned roughly equally by rows. For example, with 4 ranks and a 100x100 matrix:\n\nRank 0: rows 1-25\nRank 1: rows 26-50\nRank 2: rows 51-75\nRank 3: rows 76-100","category":"section"},{"location":"getting-started/#Efficient-Local-Only-Construction","page":"Getting Started","title":"Efficient Local-Only Construction","text":"For large matrices, you can avoid replicating data across all ranks by only populating each rank's local portion:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Global dimensions\nm, n = 1000, 1000\n\n# Compute which rows this rank owns\nrows_per_rank = div(m, nranks)\nremainder = mod(m, nranks)\nmy_row_start = 1 + rank * rows_per_rank + min(rank, remainder)\nmy_row_end = my_row_start + rows_per_rank - 1 + (rank < remainder ? 1 : 0)\n\n# Create a sparse matrix with correct size, but only populate local rows\nI, J, V = Int[], Int[], Float64[]\nfor i in my_row_start:my_row_end\n    # Example: tridiagonal matrix\n    if i > 1\n        push!(I, i); push!(J, i-1); push!(V, -1.0)\n    end\n    push!(I, i); push!(J, i); push!(V, 2.0)\n    if i < m\n        push!(I, i); push!(J, i+1); push!(V, -1.0)\n    end\nend\nA = sparse(I, J, V, m, n)\n\n# The constructor extracts only local rows - other rows are ignored\nAdist = SparseMatrixMPI{Float64}(A)\n\nThis pattern is useful when:\n\nThe global matrix is too large to fit in memory on each rank\nYou're generating matrix entries programmatically\nYou want to minimize memory usage during construction","category":"section"},{"location":"getting-started/#Basic-Operations","page":"Getting Started","title":"Basic Operations","text":"","category":"section"},{"location":"getting-started/#Matrix-Multiplication","page":"Getting Started","title":"Matrix Multiplication","text":"# Both matrices must be distributed\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Multiply\nCdist = Adist * Bdist\n\nThe multiplication automatically handles the necessary communication between ranks.","category":"section"},{"location":"getting-started/#Addition-and-Subtraction","page":"Getting Started","title":"Addition and Subtraction","text":"Cdist = Adist + Bdist\nDdist = Adist - Bdist\n\nIf A and B have different row partitions, B's rows are redistributed to match A's partition.","category":"section"},{"location":"getting-started/#Scalar-Multiplication","page":"Getting Started","title":"Scalar Multiplication","text":"Cdist = 2.5 * Adist\nCdist = Adist * 2.5  # Equivalent","category":"section"},{"location":"getting-started/#Transpose","page":"Getting Started","title":"Transpose","text":"# Transpose is lazy (no communication until needed)\nAt = transpose(Adist)\n\n# Use in multiplication - automatically materializes when needed\nCdist = At * Bdist","category":"section"},{"location":"getting-started/#Adjoint-(Conjugate-Transpose)","page":"Getting Started","title":"Adjoint (Conjugate Transpose)","text":"For complex matrices:\n\nAdist = SparseMatrixMPI{ComplexF64}(A)\nAadj = Adist'  # Conjugate transpose (lazy)","category":"section"},{"location":"getting-started/#Computing-Norms","page":"Getting Started","title":"Computing Norms","text":"# Frobenius norm (default)\nf_norm = norm(Adist)\n\n# 1-norm (sum of absolute values)\none_norm = norm(Adist, 1)\n\n# Infinity norm (maximum absolute value)\ninf_norm = norm(Adist, Inf)\n\n# General p-norm\np_norm = norm(Adist, 3)\n\n# Operator norms\ncol_norm = opnorm(Adist, 1)   # Max column sum\nrow_norm = opnorm(Adist, Inf) # Max row sum","category":"section"},{"location":"getting-started/#Running-MPI-Programs","page":"Getting Started","title":"Running MPI Programs","text":"","category":"section"},{"location":"getting-started/#Command-Line","page":"Getting Started","title":"Command Line","text":"mpiexec -n 4 julia --project=. my_program.jl","category":"section"},{"location":"getting-started/#Program-Structure","page":"Getting Started","title":"Program Structure","text":"A typical LinearAlgebraMPI.jl program follows this pattern:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Create matrices (identical on all ranks)\nA = create_my_matrix()  # Your matrix creation function\nB = create_my_matrix()\n\n# Distribute\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Compute\nCdist = Adist * Bdist\n\n# Get results (e.g., norm is computed globally)\nresult_norm = norm(Cdist)\n\nprintln(io0(), \"Result norm: $result_norm\")","category":"section"},{"location":"getting-started/#Performance-Tips","page":"Getting Started","title":"Performance Tips","text":"","category":"section"},{"location":"getting-started/#Reuse-Communication-Plans","page":"Getting Started","title":"Reuse Communication Plans","text":"For repeated operations with the same sparsity pattern, LinearAlgebraMPI.jl automatically caches communication plans:\n\n# First multiplication creates and caches the plan\nC1 = Adist * Bdist\n\n# Subsequent multiplications with same A, B reuse the cached plan\nC2 = Adist * Bdist  # Uses cached plan - much faster","category":"section"},{"location":"getting-started/#Clear-Cache-When-Done","page":"Getting Started","title":"Clear Cache When Done","text":"If you're done with a set of matrices and want to free memory:\n\nclear_plan_cache!()","category":"section"},{"location":"getting-started/#Use-Deterministic-Test-Data","page":"Getting Started","title":"Use Deterministic Test Data","text":"For testing with the simple \"replicate everywhere\" pattern, avoid random matrices since they'll differ across ranks:\n\n# Bad - different random values on each rank\nA = sprand(100, 100, 0.01)\n\n# Good - deterministic formula, same on all ranks\nI = [1:100; 1:99; 2:100]\nJ = [1:100; 2:100; 1:99]\nV = [2.0*ones(100); -0.5*ones(99); -0.5*ones(99)]\nA = sparse(I, J, V, 100, 100)\n\nAlternatively, use the local-only construction pattern where each rank generates only its own rows.","category":"section"},{"location":"getting-started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"See Examples for more detailed usage examples\nRead the API Reference for complete function documentation","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page provides detailed examples of using LinearAlgebraMPI.jl for various distributed sparse matrix operations.","category":"section"},{"location":"examples/#Matrix-Multiplication","page":"Examples","title":"Matrix Multiplication","text":"","category":"section"},{"location":"examples/#Square-Matrices","page":"Examples","title":"Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Create a tridiagonal matrix (same on all ranks)\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Create another tridiagonal matrix\nV2 = [1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]\nB = sparse(I, J, V2, n, n)\n\n# Distribute matrices\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Multiply\nCdist = Adist * Bdist\n\n# Verify against reference\nC_ref = A * B\nC_ref_dist = SparseMatrixMPI{Float64}(C_ref)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nprintln(io0(), \"Multiplication error: $err\")\n","category":"section"},{"location":"examples/#Non-Square-Matrices","page":"Examples","title":"Non-Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# A is 6x8, B is 8x10, result is 6x10\nm, k, n = 6, 8, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nJ_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 2]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, k)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, k, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\nCdist = Adist * Bdist\n\n@assert size(Cdist) == (m, n)\n\nprintln(io0(), \"Result size: $(size(Cdist))\")\n","category":"section"},{"location":"examples/#Complex-Matrices","page":"Examples","title":"Complex Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nn = 8\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# Complex values\nV_A = ComplexF64.([2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]) .+\n      im .* ComplexF64.([0.1*ones(n); 0.2*ones(n-1); -0.2*ones(n-1)])\nA = sparse(I, J, V_A, n, n)\n\nV_B = ComplexF64.([1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]) .+\n      im .* ComplexF64.([-0.1*ones(n); 0.1*ones(n-1); 0.1*ones(n-1)])\nB = sparse(I, J, V_B, n, n)\n\nAdist = SparseMatrixMPI{ComplexF64}(A)\nBdist = SparseMatrixMPI{ComplexF64}(B)\n\n# Multiplication\nCdist = Adist * Bdist\n\n# Conjugate\nAconj = conj(Adist)\n\n# Adjoint (conjugate transpose) - returns lazy wrapper\nAadj = Adist'\n\n# Using adjoint in multiplication (materializes automatically)\nresult = Aadj * Bdist\n\nprintln(io0(), \"Complex matrix operations completed\")\n","category":"section"},{"location":"examples/#Addition-and-Subtraction","page":"Examples","title":"Addition and Subtraction","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nn = 8\n\n# Matrices with different sparsity patterns\n# Matrix A: upper triangular entries\nI_A = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nJ_A = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nV_A = Float64.(1:9)\nA = sparse(I_A, J_A, V_A, n, n)\n\n# Matrix B: lower triangular entries\nI_B = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nJ_B = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nV_B = Float64.(9:-1:1)\nB = sparse(I_B, J_B, V_B, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Addition - handles different sparsity patterns\nCdist = Adist + Bdist\n\n# Subtraction\nDdist = Adist - Bdist\n\n# Verify\nC_ref_dist = SparseMatrixMPI{Float64}(A + B)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nprintln(io0(), \"Addition error: $err\")\n","category":"section"},{"location":"examples/#Transpose-Operations","page":"Examples","title":"Transpose Operations","text":"","category":"section"},{"location":"examples/#Lazy-Transpose","page":"Examples","title":"Lazy Transpose","text":"The transpose function creates a lazy wrapper without transposing the data. This is efficient because the actual transpose is only computed when needed:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nm, n = 8, 6\nI_C = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_C = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nV_C = Float64.(1:length(I_C))\nC = sparse(I_C, J_C, V_C, m, n)\n\nI_D = [1, 2, 3, 4, 5, 6, 1, 2]\nJ_D = [1, 2, 3, 4, 5, 6, 7, 8]\nV_D = Float64.(1:length(I_D))\nD = sparse(I_D, J_D, V_D, n, m)\n\nCdist = SparseMatrixMPI{Float64}(C)\nDdist = SparseMatrixMPI{Float64}(D)\n\n# transpose(C) * transpose(D) = transpose(D * C)\n# This is computed efficiently without explicitly transposing\nresult = transpose(Cdist) * transpose(Ddist)\n\nprintln(io0(), \"Lazy transpose multiplication completed\")\n","category":"section"},{"location":"examples/#Transpose-in-Multiplication","page":"Examples","title":"Transpose in Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# A is 8x6, so A' is 6x8\n# B is 8x10, so A' * B is 6x10\nm, n, p = 8, 6, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, n)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, m, p)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# transpose(A) * B - A is automatically materialized as transpose\nresult_dist = transpose(Adist) * Bdist\n\n# Verify\nref = sparse(A') * B\nref_dist = SparseMatrixMPI{Float64}(ref)\nerr = norm(result_dist - ref_dist, Inf)\n\nprintln(io0(), \"transpose(A) * B error: $err\")\n","category":"section"},{"location":"examples/#Scalar-Multiplication","page":"Examples","title":"Scalar Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3]\nJ = [1, 2, 3, 4, 5, 6, 7, 8]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Scalar times matrix\na = 2.5\nresult1 = a * Adist\nresult2 = Adist * a  # Equivalent\n\n# Scalar times lazy transpose\nAt = transpose(Adist)\nresult3 = a * At  # Returns transpose(a * A)\n\n# Verify\nref_dist = SparseMatrixMPI{Float64}(a * A)\nerr1 = norm(result1 - ref_dist, Inf)\nerr2 = norm(result2 - ref_dist, Inf)\n\nprintln(io0(), \"Scalar multiplication errors: $err1, $err2\")\n","category":"section"},{"location":"examples/#Computing-Norms","page":"Examples","title":"Computing Norms","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3, 2, 4]\nJ = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Element-wise norms (treating matrix as vector)\nfrob_norm = norm(Adist)        # Frobenius (2-norm)\none_norm = norm(Adist, 1)      # Sum of absolute values\ninf_norm = norm(Adist, Inf)    # Max absolute value\np_norm = norm(Adist, 3)        # General p-norm\n\n# Operator norms\nop_1 = opnorm(Adist, 1)        # Max absolute column sum\nop_inf = opnorm(Adist, Inf)    # Max absolute row sum\n\nprintln(io0(), \"Frobenius norm: $frob_norm\")\nprintln(io0(), \"1-norm: $one_norm\")\nprintln(io0(), \"Inf-norm: $inf_norm\")\nprintln(io0(), \"3-norm: $p_norm\")\nprintln(io0(), \"Operator 1-norm: $op_1\")\nprintln(io0(), \"Operator Inf-norm: $op_inf\")\n","category":"section"},{"location":"examples/#Iterative-Methods-Example","page":"Examples","title":"Iterative Methods Example","text":"Here's an example of using LinearAlgebraMPI.jl for power iteration to find the dominant eigenvalue:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Create a symmetric positive definite matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# For power iteration, we need matrix-vector products\n# Currently LinearAlgebraMPI focuses on matrix-matrix products\n# This example shows how to use A*A for related computations\n\n# Compute A^2\nA2dist = Adist * Adist\n\n# Compute the Frobenius norm of A^2\nnorm_A2 = norm(A2dist)\n\nprintln(io0(), \"||A^2||_F = $norm_A2\")\n# For SPD matrices, this relates to the eigenvalues\n","category":"section"},{"location":"examples/#Solving-Linear-Systems","page":"Examples","title":"Solving Linear Systems","text":"LinearAlgebraMPI provides distributed sparse direct solvers using the multifrontal method.","category":"section"},{"location":"examples/#LDLT-Factorization-(Symmetric-Matrices)","page":"Examples","title":"LDLT Factorization (Symmetric Matrices)","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Create a symmetric positive definite tridiagonal matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Distribute the matrix\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Compute LDLT factorization\nF = ldlt(Adist)\n\n# Create right-hand side\nb = VectorMPI(ones(n))\n\n# Solve Ax = b\nx = solve(F, b)\n\n# Or use backslash syntax\nx = F \\ b\n\n# Verify solution\nx_full = Vector(x)\nresidual = norm(A * x_full - ones(n), Inf)\n\nprintln(io0(), \"LDLT solve residual: $residual\")\n","category":"section"},{"location":"examples/#LU-Factorization-(General-Matrices)","page":"Examples","title":"LU Factorization (General Matrices)","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Create a general (non-symmetric) tridiagonal matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.8*ones(n-1)]  # Non-symmetric\nA = sparse(I, J, V, n, n)\n\n# Distribute and factorize\nAdist = SparseMatrixMPI{Float64}(A)\nF = lu(Adist)\n\n# Solve\nb = VectorMPI(ones(n))\nx = solve(F, b)\n\n# Verify\nx_full = Vector(x)\nresidual = norm(A * x_full - ones(n), Inf)\n\nprintln(io0(), \"LU solve residual: $residual\")\n","category":"section"},{"location":"examples/#Symmetric-Indefinite-Matrices","page":"Examples","title":"Symmetric Indefinite Matrices","text":"LDLT uses Bunch-Kaufman pivoting to handle symmetric indefinite matrices:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Symmetric indefinite matrix (alternating signs on diagonal)\nn = 50\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\ndiag_vals = [(-1.0)^i * 2.0 for i in 1:n]  # Alternating signs\nV = [diag_vals; -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nF = ldlt(Adist)\n\nb = VectorMPI(collect(1.0:n))\nx = solve(F, b)\n\nx_full = Vector(x)\nresidual = norm(A * x_full - collect(1.0:n), Inf)\n\nprintln(io0(), \"Indefinite LDLT residual: $residual\")\n","category":"section"},{"location":"examples/#Reusing-Symbolic-Factorization","page":"Examples","title":"Reusing Symbolic Factorization","text":"For sequences of matrices with the same sparsity pattern, the symbolic factorization is cached and reused:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# First matrix\nV1 = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA1 = sparse(I, J, V1, n, n)\nA1dist = SparseMatrixMPI{Float64}(A1)\n\n# First factorization - computes symbolic phase\nF1 = ldlt(A1dist; reuse_symbolic=true)\n\n# Second matrix - same structure, different values\nV2 = [8.0*ones(n); -2.0*ones(n-1); -2.0*ones(n-1)]\nA2 = sparse(I, J, V2, n, n)\nA2dist = SparseMatrixMPI{Float64}(A2)\n\n# Second factorization - reuses cached symbolic phase (faster)\nF2 = ldlt(A2dist; reuse_symbolic=true)\n\n# Both factorizations work\nb = VectorMPI(ones(n))\nx1 = solve(F1, b)\nx2 = solve(F2, b)\n\nx1_full = Vector(x1)\nx2_full = Vector(x2)\nprintln(io0(), \"F1 residual: \", norm(A1 * x1_full - ones(n), Inf))\nprintln(io0(), \"F2 residual: \", norm(A2 * x2_full - ones(n), Inf))\n","category":"section"},{"location":"examples/#Plan-Caching-and-Management","page":"Examples","title":"Plan Caching and Management","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\nB = spdiagm(0 => 1.5*ones(n), 1 => 0.5*ones(n-1), -1 => 0.5*ones(n-1))\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# First multiplication - creates and caches the plan\nC1 = Adist * Bdist\n\n# Second multiplication - reuses cached plan (faster)\nC2 = Adist * Bdist\n\n# Third multiplication - still uses cached plan\nC3 = Adist * Bdist\n\n# Clear caches when done to free memory\nclear_plan_cache!()  # Clears all caches including factorization\n\n# Or clear specific caches:\n# clear_symbolic_cache!()           # Symbolic factorizations only\n# clear_factorization_plan_cache!() # Factorization plans only\n\nprintln(io0(), \"Cached multiplication completed\")\n","category":"section"},{"location":"examples/#Dense-Matrix-Operations-with-mapslices","page":"Examples","title":"Dense Matrix Operations with mapslices","text":"The mapslices function applies a function to each row or column of a distributed dense matrix. This is useful for computing row-wise or column-wise statistics.","category":"section"},{"location":"examples/#Row-wise-Operations-(dims2)","page":"Examples","title":"Row-wise Operations (dims=2)","text":"Row-wise operations are local - no MPI communication is needed since rows are already distributed:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\n# Create a deterministic dense matrix (same on all ranks)\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\n# Distribute\nAdist = MatrixMPI(A_global)\n\n# Compute row statistics: for each row, compute [norm, max, sum]\n# This transforms 100×10 matrix to 100×3 matrix\nrow_stats = mapslices(x -> [norm(x), maximum(x), sum(x)], Adist; dims=2)\n\nprintln(io0(), \"Row statistics shape: $(size(row_stats))\")  # (100, 3)\n","category":"section"},{"location":"examples/#Column-wise-Operations-(dims1)","page":"Examples","title":"Column-wise Operations (dims=1)","text":"Column-wise operations require MPI communication to gather each full column:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\n# Create a deterministic dense matrix\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\nAdist = MatrixMPI(A_global)\n\n# Compute column statistics: for each column, compute [norm, max]\n# This transforms 100×10 matrix to 2×10 matrix\ncol_stats = mapslices(x -> [norm(x), maximum(x)], Adist; dims=1)\n\nprintln(io0(), \"Column statistics shape: $(size(col_stats))\")  # (2, 10)\n","category":"section"},{"location":"examples/#Use-Case:-Replacing-vcat(f.(eachrow(A))...)","page":"Examples","title":"Use Case: Replacing vcat(f.(eachrow(A))...)","text":"The standard Julia pattern vcat(f.(eachrow(A))...) doesn't work with distributed matrices because the type information is lost after broadcasting. Use mapslices instead:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\n# Standard Julia pattern (for comparison):\n# A = randn(5, 2)\n# f(x) = transpose([norm(x), maximum(x)])\n# B = vcat(f.(eachrow(A))...)\n\n# MPI-compatible equivalent:\nA_global = Float64.([i + 0.1*j for i in 1:100, j in 1:10])\nAdist = MatrixMPI(A_global)\n\n# Use mapslices with dims=2 to apply function to each row\n# The function returns a vector, which becomes a row in the result\ng(x) = [norm(x), maximum(x)]\nBdist = mapslices(g, Adist; dims=2)\n\nprintln(io0(), \"Result: $(size(Bdist))\")  # (100, 2)\n","category":"section"},{"location":"#LinearAlgebraMPI.jl","page":"Home","title":"LinearAlgebraMPI.jl","text":"Distributed sparse matrix operations using MPI for parallel computing across multiple ranks.\n\nLinearAlgebraMPI.jl provides a high-performance implementation of distributed sparse matrices in Julia, enabling parallel sparse linear algebra operations across multiple MPI processes. The package is designed for large-scale computations where matrices are too large to fit on a single node or where parallel speedup is desired.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Row-partitioned sparse matrices: Matrices are distributed by rows across MPI ranks\nMatrix multiplication: Efficient sparse matrix-matrix product with memoized communication plans\nAddition and subtraction: Element-wise operations with automatic data redistribution\nTranspose operations: Both eager and lazy transpose support\nConjugate and adjoint: Full support for complex matrices\nScalar multiplication: Efficient scalar-matrix products\nNorms: Frobenius norm, 1-norm, infinity norm, and general p-norms\nOperator norms: 1-norm and infinity-norm of operators\nType stability: Generic implementation supporting Float64, ComplexF64, and other numeric types\nPlan caching: Communication plans are memoized for repeated operations with the same sparsity pattern","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create a sparse matrix (must be identical on all ranks)\nA = sprand(1000, 1000, 0.01)\nB = sprand(1000, 1000, 0.01)\n\n# Distribute matrices across MPI ranks\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Perform distributed operations\nC = Adist * Bdist    # Matrix multiplication\nD = Adist + Bdist    # Addition\nE = Adist - Bdist    # Subtraction\nF = 2.0 * Adist      # Scalar multiplication\n\n# Compute norms\nfrobenius_norm = norm(Adist)\nmax_col_sum = opnorm(Adist, 1)\n","category":"section"},{"location":"#Package-Overview","page":"Home","title":"Package Overview","text":"Pages = [\"getting-started.md\", \"examples.md\", \"api.md\"]\nDepth = 2","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"Add LinearAlgebraMPI.jl to your project:\n\nusing Pkg\nPkg.add(\"LinearAlgebraMPI\")\n\nOr for development:\n\nPkg.develop(path=\"/path/to/LinearAlgebraMPI.jl\")","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.10+\nMPI.jl with a working MPI implementation\nSparseArrays.jl\nLinearAlgebra.jl\nBlake3Hash.jl","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MIT License","category":"section"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"}]
}
