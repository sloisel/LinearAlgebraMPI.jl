var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"This page documents the public API of LinearAlgebraMPI.jl.","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"","category":"section"},{"location":"api/#SparseMatrixMPI","page":"API Reference","title":"SparseMatrixMPI","text":"","category":"section"},{"location":"api/#MatrixMPI","page":"API Reference","title":"MatrixMPI","text":"","category":"section"},{"location":"api/#VectorMPI","page":"API Reference","title":"VectorMPI","text":"","category":"section"},{"location":"api/#Sparse-Matrix-Operations","page":"API Reference","title":"Sparse Matrix Operations","text":"","category":"section"},{"location":"api/#Arithmetic","page":"API Reference","title":"Arithmetic","text":"A * B          # Matrix multiplication\nA + B          # Addition\nA - B          # Subtraction\na * A          # Scalar multiplication\nA * a          # Scalar multiplication","category":"section"},{"location":"api/#Transpose-and-Adjoint","page":"API Reference","title":"Transpose and Adjoint","text":"transpose(A)   # Lazy transpose\nconj(A)        # Conjugate (new matrix)\nA'             # Adjoint (conjugate transpose, lazy)","category":"section"},{"location":"api/#Matrix-Vector-Multiplication","page":"API Reference","title":"Matrix-Vector Multiplication","text":"y = A * x      # Returns VectorMPI\nmul!(y, A, x)  # In-place version","category":"section"},{"location":"api/#Vector-Matrix-Multiplication","page":"API Reference","title":"Vector-Matrix Multiplication","text":"transpose(v) * A   # Row vector times matrix\nv' * A             # Conjugate row vector times matrix","category":"section"},{"location":"api/#Norms","page":"API Reference","title":"Norms","text":"norm(A)        # Frobenius norm (default)\nnorm(A, 1)     # Sum of absolute values\nnorm(A, Inf)   # Maximum absolute value\nnorm(A, p)     # General p-norm\n\nopnorm(A, 1)   # Maximum absolute column sum\nopnorm(A, Inf) # Maximum absolute row sum","category":"section"},{"location":"api/#Properties","page":"API Reference","title":"Properties","text":"size(A)        # Global dimensions (m, n)\nsize(A, d)     # Size along dimension d\neltype(A)      # Element type\nnnz(A)         # Number of nonzeros\nissparse(A)    # Returns true","category":"section"},{"location":"api/#Reductions","page":"API Reference","title":"Reductions","text":"sum(A)         # Sum of all elements\nsum(A; dims=1) # Column sums (returns VectorMPI)\nsum(A; dims=2) # Row sums (returns VectorMPI)\nmaximum(A)     # Maximum of stored values\nminimum(A)     # Minimum of stored values\ntr(A)          # Trace (sum of diagonal)","category":"section"},{"location":"api/#Element-wise-Operations","page":"API Reference","title":"Element-wise Operations","text":"abs(A)         # Absolute value\nabs2(A)        # Squared absolute value\nreal(A)        # Real part\nimag(A)        # Imaginary part\nfloor(A)       # Floor\nceil(A)        # Ceiling\nround(A)       # Round","category":"section"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"copy(A)        # Deep copy\ndropzeros(A)   # Remove stored zeros\ndiag(A)        # Main diagonal (returns VectorMPI)\ndiag(A, k)     # k-th diagonal\ntriu(A)        # Upper triangular\ntriu(A, k)     # Upper triangular from k-th diagonal\ntril(A)        # Lower triangular\ntril(A, k)     # Lower triangular from k-th diagonal","category":"section"},{"location":"api/#Block-Operations","page":"API Reference","title":"Block Operations","text":"cat(A, B, C; dims=1)       # Vertical concatenation\ncat(A, B, C; dims=2)       # Horizontal concatenation\ncat(A, B, C, D; dims=(2,2)) # 2x2 block matrix [A B; C D]\nvcat(A, B, C)              # Vertical concatenation\nhcat(A, B, C)              # Horizontal concatenation\nblockdiag(A, B, C)         # Block diagonal matrix","category":"section"},{"location":"api/#Diagonal-Matrix-Construction","page":"API Reference","title":"Diagonal Matrix Construction","text":"spdiagm(v)                 # Diagonal matrix from VectorMPI\nspdiagm(m, n, v)           # m x n diagonal matrix\nspdiagm(k => v)            # k-th diagonal\nspdiagm(0 => v, 1 => w)    # Multiple diagonals","category":"section"},{"location":"api/#Dense-Matrix-Operations","page":"API Reference","title":"Dense Matrix Operations","text":"","category":"section"},{"location":"api/#Arithmetic-2","page":"API Reference","title":"Arithmetic","text":"A * x          # Matrix-vector multiplication (returns VectorMPI)\ntranspose(A)   # Lazy transpose\nconj(A)        # Conjugate\nA'             # Adjoint\na * A          # Scalar multiplication","category":"section"},{"location":"api/#mapslices","page":"API Reference","title":"mapslices","text":"Apply a function to rows or columns of a distributed dense matrix.\n\nmapslices(f, A; dims=2)   # Apply f to each row (local, no MPI)\nmapslices(f, A; dims=1)   # Apply f to each column (requires MPI)\n\nExample:\n\nusing LinearAlgebra\n\nA = MatrixMPI(randn(100, 10))\n\n# Compute row statistics: norm, max, sum for each row\n# Transforms 100x10 to 100x3\nB = mapslices(x -> [norm(x), maximum(x), sum(x)], A; dims=2)","category":"section"},{"location":"api/#Block-Operations-2","page":"API Reference","title":"Block Operations","text":"cat(A, B; dims=1)          # Vertical concatenation\ncat(A, B; dims=2)          # Horizontal concatenation\nvcat(A, B)                 # Vertical concatenation\nhcat(A, B)                 # Horizontal concatenation","category":"section"},{"location":"api/#Norms-2","page":"API Reference","title":"Norms","text":"norm(A)        # Frobenius norm\nnorm(A, p)     # General p-norm\nopnorm(A, 1)   # Maximum absolute column sum\nopnorm(A, Inf) # Maximum absolute row sum","category":"section"},{"location":"api/#Vector-Operations","page":"API Reference","title":"Vector Operations","text":"","category":"section"},{"location":"api/#Arithmetic-3","page":"API Reference","title":"Arithmetic","text":"u + v          # Addition (auto-aligns partitions)\nu - v          # Subtraction\n-v             # Negation\na * v          # Scalar multiplication\nv * a          # Scalar multiplication\nv / a          # Scalar division","category":"section"},{"location":"api/#Transpose-and-Adjoint-2","page":"API Reference","title":"Transpose and Adjoint","text":"transpose(v)   # Lazy transpose (row vector)\nconj(v)        # Conjugate\nv'             # Adjoint","category":"section"},{"location":"api/#Norms-3","page":"API Reference","title":"Norms","text":"norm(v)        # 2-norm (default)\nnorm(v, 1)     # 1-norm\nnorm(v, Inf)   # Infinity norm\nnorm(v, p)     # General p-norm","category":"section"},{"location":"api/#Reductions-2","page":"API Reference","title":"Reductions","text":"sum(v)         # Sum of elements\nprod(v)        # Product of elements\nmaximum(v)     # Maximum element\nminimum(v)     # Minimum element\nmean(v)        # Mean of elements","category":"section"},{"location":"api/#Element-wise-Operations-2","page":"API Reference","title":"Element-wise Operations","text":"abs(v)         # Absolute value\nabs2(v)        # Squared absolute value\nreal(v)        # Real part\nimag(v)        # Imaginary part\ncopy(v)        # Deep copy","category":"section"},{"location":"api/#Broadcasting","page":"API Reference","title":"Broadcasting","text":"VectorMPI supports broadcasting for element-wise operations:\n\nv .+ w         # Element-wise addition\nv .* w         # Element-wise multiplication\nsin.(v)        # Apply function element-wise\nv .* 2.0 .+ w  # Compound expressions","category":"section"},{"location":"api/#Block-Operations-3","page":"API Reference","title":"Block Operations","text":"vcat(u, v, w)  # Concatenate vectors (returns VectorMPI)\nhcat(u, v, w)  # Stack as columns (returns MatrixMPI)","category":"section"},{"location":"api/#Properties-2","page":"API Reference","title":"Properties","text":"length(v)      # Global length\nsize(v)        # Returns (length,)\neltype(v)      # Element type","category":"section"},{"location":"api/#Cache-Management","page":"API Reference","title":"Cache Management","text":"","category":"section"},{"location":"api/#Full-API-Index","page":"API Reference","title":"Full API Index","text":"","category":"section"},{"location":"api/#LinearAlgebraMPI.SparseMatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.SparseMatrixMPI","text":"SparseMatrixMPI{T}\n\nA distributed sparse matrix partitioned by rows across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (placeholder for transpose)\ncol_indices::Vector{Int}: Column indices that appear in the local part\nAT::SparseMatrixCSC{T,Int}: Transpose of local rows (columns in AT correspond to local rows)\n\nInvariants\n\ncol_indices, row_partition, and col_partition are sorted\nrow_partition[nranks+1] = total number of rows\ncol_partition[nranks+1] = total number of columns\nsize(AT, 2) == row_partition[rank+1] - row_partition[rank]\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.MatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.MatrixMPI","text":"MatrixMPI{T}\n\nA distributed dense matrix partitioned by rows across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (for transpose)\nA::Matrix{T}: Local rows (NOT transposed), size = (local_nrows, ncols)\n\nInvariants\n\nrow_partition and col_partition are sorted\nrow_partition[nranks+1] = total number of rows + 1\ncol_partition[nranks+1] = total number of columns + 1\nsize(A, 1) == row_partition[rank+2] - row_partition[rank+1]\nsize(A, 2) == col_partition[end] - 1\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.VectorMPI","page":"API Reference","title":"LinearAlgebraMPI.VectorMPI","text":"VectorMPI{T}\n\nA distributed dense vector partitioned across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the partition\npartition::Vector{Int}: Partition boundaries, length = nranks + 1\nv::Vector{T}: Local vector elements owned by this rank\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.mean","page":"API Reference","title":"LinearAlgebraMPI.mean","text":"mean(v::VectorMPI{T}) where T\n\nCompute the mean of all elements in the distributed vector.\n\n\n\n\n\nmean(A::SparseMatrixMPI{T}) where T\n\nCompute the mean of all elements (including implicit zeros) in the distributed sparse matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.clear_plan_cache!","page":"API Reference","title":"LinearAlgebraMPI.clear_plan_cache!","text":"clear_plan_cache!()\n\nClear all memoized plan caches.\n\n\n\n\n\n","category":"function"},{"location":"getting-started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This guide will walk you through the basics of using LinearAlgebraMPI.jl for distributed sparse matrix computations.","category":"section"},{"location":"getting-started/#Prerequisites","page":"Getting Started","title":"Prerequisites","text":"Before using LinearAlgebraMPI.jl, ensure you have:\n\nA working MPI installation (OpenMPI, MPICH, or Intel MPI)\nMPI.jl configured to use your MPI installation\n\nYou can verify your MPI setup with:\n\nusing MPI\nMPI.Init()\nprintln(\"Rank $(MPI.Comm_rank(MPI.COMM_WORLD)) of $(MPI.Comm_size(MPI.COMM_WORLD))\")\nMPI.Finalize()\n\nRun with:\n\nmpiexec -n 4 julia --project=. your_script.jl","category":"section"},{"location":"getting-started/#Creating-Distributed-Matrices","page":"Getting Started","title":"Creating Distributed Matrices","text":"","category":"section"},{"location":"getting-started/#From-a-Global-Sparse-Matrix","page":"Getting Started","title":"From a Global Sparse Matrix","text":"The most common way to create a distributed matrix is from an existing SparseMatrixCSC:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create a sparse matrix - MUST be identical on all ranks\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\n\n# Distribute across MPI ranks\nAdist = SparseMatrixMPI{Float64}(A)\n\nImportant: All MPI ranks must have identical copies of the input matrix when constructing SparseMatrixMPI. The matrix is then automatically partitioned by rows across ranks.","category":"section"},{"location":"getting-started/#Understanding-Row-Partitioning","page":"Getting Started","title":"Understanding Row Partitioning","text":"The matrix is partitioned roughly equally by rows. For example, with 4 ranks and a 100x100 matrix:\n\nRank 0: rows 1-25\nRank 1: rows 26-50\nRank 2: rows 51-75\nRank 3: rows 76-100","category":"section"},{"location":"getting-started/#Basic-Operations","page":"Getting Started","title":"Basic Operations","text":"","category":"section"},{"location":"getting-started/#Matrix-Multiplication","page":"Getting Started","title":"Matrix Multiplication","text":"# Both matrices must be distributed\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Multiply\nCdist = Adist * Bdist\n\nThe multiplication automatically handles the necessary communication between ranks.","category":"section"},{"location":"getting-started/#Addition-and-Subtraction","page":"Getting Started","title":"Addition and Subtraction","text":"Cdist = Adist + Bdist\nDdist = Adist - Bdist\n\nIf A and B have different row partitions, B's rows are redistributed to match A's partition.","category":"section"},{"location":"getting-started/#Scalar-Multiplication","page":"Getting Started","title":"Scalar Multiplication","text":"Cdist = 2.5 * Adist\nCdist = Adist * 2.5  # Equivalent","category":"section"},{"location":"getting-started/#Transpose","page":"Getting Started","title":"Transpose","text":"# Transpose is lazy (no communication until needed)\nAt = transpose(Adist)\n\n# Use in multiplication - automatically materializes when needed\nCdist = At * Bdist","category":"section"},{"location":"getting-started/#Adjoint-(Conjugate-Transpose)","page":"Getting Started","title":"Adjoint (Conjugate Transpose)","text":"For complex matrices:\n\nAdist = SparseMatrixMPI{ComplexF64}(A)\nAadj = Adist'  # Conjugate transpose (lazy)","category":"section"},{"location":"getting-started/#Computing-Norms","page":"Getting Started","title":"Computing Norms","text":"# Frobenius norm (default)\nf_norm = norm(Adist)\n\n# 1-norm (sum of absolute values)\none_norm = norm(Adist, 1)\n\n# Infinity norm (maximum absolute value)\ninf_norm = norm(Adist, Inf)\n\n# General p-norm\np_norm = norm(Adist, 3)\n\n# Operator norms\ncol_norm = opnorm(Adist, 1)   # Max column sum\nrow_norm = opnorm(Adist, Inf) # Max row sum","category":"section"},{"location":"getting-started/#Running-MPI-Programs","page":"Getting Started","title":"Running MPI Programs","text":"","category":"section"},{"location":"getting-started/#Command-Line","page":"Getting Started","title":"Command Line","text":"mpiexec -n 4 julia --project=. my_program.jl","category":"section"},{"location":"getting-started/#Program-Structure","page":"Getting Started","title":"Program Structure","text":"A typical LinearAlgebraMPI.jl program follows this pattern:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Create matrices (identical on all ranks)\nA = create_my_matrix()  # Your matrix creation function\nB = create_my_matrix()\n\n# Distribute\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Compute\nCdist = Adist * Bdist\n\n# Get results (e.g., norm is computed globally)\nresult_norm = norm(Cdist)\n\nif rank == 0\n    println(\"Result norm: $result_norm\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"getting-started/#Performance-Tips","page":"Getting Started","title":"Performance Tips","text":"","category":"section"},{"location":"getting-started/#Reuse-Communication-Plans","page":"Getting Started","title":"Reuse Communication Plans","text":"For repeated operations with the same sparsity pattern, LinearAlgebraMPI.jl automatically caches communication plans:\n\n# First multiplication creates and caches the plan\nC1 = Adist * Bdist\n\n# Subsequent multiplications with same A, B reuse the cached plan\nC2 = Adist * Bdist  # Uses cached plan - much faster","category":"section"},{"location":"getting-started/#Clear-Cache-When-Done","page":"Getting Started","title":"Clear Cache When Done","text":"If you're done with a set of matrices and want to free memory:\n\nclear_plan_cache!()","category":"section"},{"location":"getting-started/#Use-Deterministic-Test-Data","page":"Getting Started","title":"Use Deterministic Test Data","text":"For testing, avoid random matrices since they'll differ across ranks:\n\n# Bad - different on each rank\nA = sprand(100, 100, 0.01)\n\n# Good - deterministic, identical on all ranks\nI = [1:100; 1:99; 2:100]\nJ = [1:100; 2:100; 1:99]\nV = [2.0*ones(100); -0.5*ones(99); -0.5*ones(99)]\nA = sparse(I, J, V, 100, 100)","category":"section"},{"location":"getting-started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"See Examples for more detailed usage examples\nRead the API Reference for complete function documentation","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page provides detailed examples of using LinearAlgebraMPI.jl for various distributed sparse matrix operations.","category":"section"},{"location":"examples/#Matrix-Multiplication","page":"Examples","title":"Matrix Multiplication","text":"","category":"section"},{"location":"examples/#Square-Matrices","page":"Examples","title":"Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\n\n# Create a tridiagonal matrix (same on all ranks)\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Create another tridiagonal matrix\nV2 = [1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]\nB = sparse(I, J, V2, n, n)\n\n# Distribute matrices\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Multiply\nCdist = Adist * Bdist\n\n# Verify against reference\nC_ref = A * B\nC_ref_dist = SparseMatrixMPI{Float64}(C_ref)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nif rank == 0\n    println(\"Multiplication error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Non-Square-Matrices","page":"Examples","title":"Non-Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# A is 6x8, B is 8x10, result is 6x10\nm, k, n = 6, 8, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nJ_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 2]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, k)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, k, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\nCdist = Adist * Bdist\n\n@assert size(Cdist) == (m, n)\n\nif rank == 0\n    println(\"Result size: $(size(Cdist))\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Complex-Matrices","page":"Examples","title":"Complex Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 8\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# Complex values\nV_A = ComplexF64.([2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]) .+\n      im .* ComplexF64.([0.1*ones(n); 0.2*ones(n-1); -0.2*ones(n-1)])\nA = sparse(I, J, V_A, n, n)\n\nV_B = ComplexF64.([1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]) .+\n      im .* ComplexF64.([-0.1*ones(n); 0.1*ones(n-1); 0.1*ones(n-1)])\nB = sparse(I, J, V_B, n, n)\n\nAdist = SparseMatrixMPI{ComplexF64}(A)\nBdist = SparseMatrixMPI{ComplexF64}(B)\n\n# Multiplication\nCdist = Adist * Bdist\n\n# Conjugate\nAconj = conj(Adist)\n\n# Adjoint (conjugate transpose) - returns lazy wrapper\nAadj = Adist'\n\n# Using adjoint in multiplication (materializes automatically)\nresult = Aadj * Bdist\n\nif rank == 0\n    println(\"Complex matrix operations completed\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Addition-and-Subtraction","page":"Examples","title":"Addition and Subtraction","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 8\n\n# Matrices with different sparsity patterns\n# Matrix A: upper triangular entries\nI_A = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nJ_A = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nV_A = Float64.(1:9)\nA = sparse(I_A, J_A, V_A, n, n)\n\n# Matrix B: lower triangular entries\nI_B = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nJ_B = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nV_B = Float64.(9:-1:1)\nB = sparse(I_B, J_B, V_B, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Addition - handles different sparsity patterns\nCdist = Adist + Bdist\n\n# Subtraction\nDdist = Adist - Bdist\n\n# Verify\nC_ref_dist = SparseMatrixMPI{Float64}(A + B)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nif rank == 0\n    println(\"Addition error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Transpose-Operations","page":"Examples","title":"Transpose Operations","text":"","category":"section"},{"location":"examples/#Lazy-Transpose","page":"Examples","title":"Lazy Transpose","text":"The transpose function creates a lazy wrapper without transposing the data. This is efficient because the actual transpose is only computed when needed:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 8, 6\nI_C = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_C = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nV_C = Float64.(1:length(I_C))\nC = sparse(I_C, J_C, V_C, m, n)\n\nI_D = [1, 2, 3, 4, 5, 6, 1, 2]\nJ_D = [1, 2, 3, 4, 5, 6, 7, 8]\nV_D = Float64.(1:length(I_D))\nD = sparse(I_D, J_D, V_D, n, m)\n\nCdist = SparseMatrixMPI{Float64}(C)\nDdist = SparseMatrixMPI{Float64}(D)\n\n# transpose(C) * transpose(D) = transpose(D * C)\n# This is computed efficiently without explicitly transposing\nresult = transpose(Cdist) * transpose(Ddist)\n\nif rank == 0\n    println(\"Lazy transpose multiplication completed\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Transpose-in-Multiplication","page":"Examples","title":"Transpose in Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# A is 8x6, so A' is 6x8\n# B is 8x10, so A' * B is 6x10\nm, n, p = 8, 6, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, n)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, m, p)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# transpose(A) * B - A is automatically materialized as transpose\nresult_dist = transpose(Adist) * Bdist\n\n# Verify\nref = sparse(A') * B\nref_dist = SparseMatrixMPI{Float64}(ref)\nerr = norm(result_dist - ref_dist, Inf)\n\nif rank == 0\n    println(\"transpose(A) * B error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Scalar-Multiplication","page":"Examples","title":"Scalar Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3]\nJ = [1, 2, 3, 4, 5, 6, 7, 8]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Scalar times matrix\na = 2.5\nresult1 = a * Adist\nresult2 = Adist * a  # Equivalent\n\n# Scalar times lazy transpose\nAt = transpose(Adist)\nresult3 = a * At  # Returns transpose(a * A)\n\n# Verify\nref_dist = SparseMatrixMPI{Float64}(a * A)\nerr1 = norm(result1 - ref_dist, Inf)\nerr2 = norm(result2 - ref_dist, Inf)\n\nif rank == 0\n    println(\"Scalar multiplication errors: $err1, $err2\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Computing-Norms","page":"Examples","title":"Computing Norms","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3, 2, 4]\nJ = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Element-wise norms (treating matrix as vector)\nfrob_norm = norm(Adist)        # Frobenius (2-norm)\none_norm = norm(Adist, 1)      # Sum of absolute values\ninf_norm = norm(Adist, Inf)    # Max absolute value\np_norm = norm(Adist, 3)        # General p-norm\n\n# Operator norms\nop_1 = opnorm(Adist, 1)        # Max absolute column sum\nop_inf = opnorm(Adist, Inf)    # Max absolute row sum\n\nif rank == 0\n    println(\"Frobenius norm: $frob_norm\")\n    println(\"1-norm: $one_norm\")\n    println(\"Inf-norm: $inf_norm\")\n    println(\"3-norm: $p_norm\")\n    println(\"Operator 1-norm: $op_1\")\n    println(\"Operator Inf-norm: $op_inf\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Iterative-Methods-Example","page":"Examples","title":"Iterative Methods Example","text":"Here's an example of using LinearAlgebraMPI.jl for power iteration to find the dominant eigenvalue:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\n\n# Create a symmetric positive definite matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# For power iteration, we need matrix-vector products\n# Currently LinearAlgebraMPI focuses on matrix-matrix products\n# This example shows how to use A*A for related computations\n\n# Compute A^2\nA2dist = Adist * Adist\n\n# Compute the Frobenius norm of A^2\nnorm_A2 = norm(A2dist)\n\nif rank == 0\n    println(\"||A^2||_F = $norm_A2\")\n    # For SPD matrices, this relates to the eigenvalues\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Plan-Caching-and-Management","page":"Examples","title":"Plan Caching and Management","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\nB = spdiagm(0 => 1.5*ones(n), 1 => 0.5*ones(n-1), -1 => 0.5*ones(n-1))\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# First multiplication - creates and caches the plan\nC1 = Adist * Bdist\n\n# Second multiplication - reuses cached plan (faster)\nC2 = Adist * Bdist\n\n# Third multiplication - still uses cached plan\nC3 = Adist * Bdist\n\n# Clear caches when done to free memory\nclear_plan_cache!()\n\nif rank == 0\n    println(\"Cached multiplication completed\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Dense-Matrix-Operations-with-mapslices","page":"Examples","title":"Dense Matrix Operations with mapslices","text":"The mapslices function applies a function to each row or column of a distributed dense matrix. This is useful for computing row-wise or column-wise statistics.","category":"section"},{"location":"examples/#Row-wise-Operations-(dims2)","page":"Examples","title":"Row-wise Operations (dims=2)","text":"Row-wise operations are local - no MPI communication is needed since rows are already distributed:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# Create a deterministic dense matrix (same on all ranks)\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\n# Distribute\nAdist = MatrixMPI(A_global)\n\n# Compute row statistics: for each row, compute [norm, max, sum]\n# This transforms 100×10 matrix to 100×3 matrix\nrow_stats = mapslices(x -> [norm(x), maximum(x), sum(x)], Adist; dims=2)\n\nif rank == 0\n    println(\"Row statistics shape: $(size(row_stats))\")  # (100, 3)\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Column-wise-Operations-(dims1)","page":"Examples","title":"Column-wise Operations (dims=1)","text":"Column-wise operations require MPI communication to gather each full column:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# Create a deterministic dense matrix\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\nAdist = MatrixMPI(A_global)\n\n# Compute column statistics: for each column, compute [norm, max]\n# This transforms 100×10 matrix to 2×10 matrix\ncol_stats = mapslices(x -> [norm(x), maximum(x)], Adist; dims=1)\n\nif rank == 0\n    println(\"Column statistics shape: $(size(col_stats))\")  # (2, 10)\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Use-Case:-Replacing-vcat(f.(eachrow(A))...)","page":"Examples","title":"Use Case: Replacing vcat(f.(eachrow(A))...)","text":"The standard Julia pattern vcat(f.(eachrow(A))...) doesn't work with distributed matrices because the type information is lost after broadcasting. Use mapslices instead:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# Standard Julia pattern (for comparison):\n# A = randn(5, 2)\n# f(x) = transpose([norm(x), maximum(x)])\n# B = vcat(f.(eachrow(A))...)\n\n# MPI-compatible equivalent:\nA_global = Float64.([i + 0.1*j for i in 1:100, j in 1:10])\nAdist = MatrixMPI(A_global)\n\n# Use mapslices with dims=2 to apply function to each row\n# The function returns a vector, which becomes a row in the result\ng(x) = [norm(x), maximum(x)]\nBdist = mapslices(g, Adist; dims=2)\n\nif rank == 0\n    println(\"Result: $(size(Bdist))\")  # (100, 2)\nend\n\nMPI.Finalize()","category":"section"},{"location":"#LinearAlgebraMPI.jl","page":"Home","title":"LinearAlgebraMPI.jl","text":"Distributed sparse matrix operations using MPI for parallel computing across multiple ranks.\n\nLinearAlgebraMPI.jl provides a high-performance implementation of distributed sparse matrices in Julia, enabling parallel sparse linear algebra operations across multiple MPI processes. The package is designed for large-scale computations where matrices are too large to fit on a single node or where parallel speedup is desired.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Row-partitioned sparse matrices: Matrices are distributed by rows across MPI ranks\nMatrix multiplication: Efficient sparse matrix-matrix product with memoized communication plans\nAddition and subtraction: Element-wise operations with automatic data redistribution\nTranspose operations: Both eager and lazy transpose support\nConjugate and adjoint: Full support for complex matrices\nScalar multiplication: Efficient scalar-matrix products\nNorms: Frobenius norm, 1-norm, infinity norm, and general p-norms\nOperator norms: 1-norm and infinity-norm of operators\nType stability: Generic implementation supporting Float64, ComplexF64, and other numeric types\nPlan caching: Communication plans are memoized for repeated operations with the same sparsity pattern","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create a sparse matrix (must be identical on all ranks)\nA = sprand(1000, 1000, 0.01)\nB = sprand(1000, 1000, 0.01)\n\n# Distribute matrices across MPI ranks\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Perform distributed operations\nC = Adist * Bdist    # Matrix multiplication\nD = Adist + Bdist    # Addition\nE = Adist - Bdist    # Subtraction\nF = 2.0 * Adist      # Scalar multiplication\n\n# Compute norms\nfrobenius_norm = norm(Adist)\nmax_col_sum = opnorm(Adist, 1)\n\nMPI.Finalize()","category":"section"},{"location":"#Package-Overview","page":"Home","title":"Package Overview","text":"Pages = [\"getting-started.md\", \"examples.md\", \"api.md\", \"internals.md\"]\nDepth = 2","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(url=\"https://github.com/your-username/LinearAlgebraMPI.jl\")\n\nOr in the Julia REPL package mode:\n\npkg> add https://github.com/your-username/LinearAlgebraMPI.jl","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.10+\nMPI.jl with a working MPI implementation\nSparseArrays.jl\nLinearAlgebra.jl\nBlake3Hash.jl","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MIT License","category":"section"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"}]
}
