var documenterSearchIndex = {"docs":
[{"location":"guide/#User-Guide","page":"User Guide","title":"User Guide","text":"This guide covers the essential workflows for using LinearAlgebraMPI.jl.","category":"section"},{"location":"guide/#Core-Types","page":"User Guide","title":"Core Types","text":"LinearAlgebraMPI provides three distributed types:\n\nType Description Storage\nVectorMPI{T} Distributed vector Row-partitioned\nMatrixMPI{T} Distributed dense matrix Row-partitioned\nSparseMatrixMPI{T,Ti} Distributed sparse matrix Row-partitioned CSR\n\nAll types are row-partitioned across MPI ranks, meaning each rank owns a contiguous range of rows.","category":"section"},{"location":"guide/#Internal-Storage:-CSR-Format","page":"User Guide","title":"Internal Storage: CSR Format","text":"Internally, SparseMatrixMPI stores local rows in CSR (Compressed Sparse Row) format using the SparseMatrixCSR type. This enables efficient row-wise iteration for a row-partitioned distributed matrix.\n\nIn Julia, SparseMatrixCSR{T,Ti} is a type alias for Transpose{T, SparseMatrixCSC{T,Ti}}. You don't need to worry about this for normal usage - it's handled automatically.","category":"section"},{"location":"guide/#Creating-Distributed-Types","page":"User Guide","title":"Creating Distributed Types","text":"","category":"section"},{"location":"guide/#From-Native-Julia-Types","page":"User Guide","title":"From Native Julia Types","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create from native types (data is distributed automatically)\nv = VectorMPI(randn(100))\nA = MatrixMPI(randn(50, 30))\nS = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))","category":"section"},{"location":"guide/#Local-Constructors","page":"User Guide","title":"Local Constructors","text":"For performance-critical code, use local constructors that avoid global communication:\n\n# Create from local data (each rank provides its own rows)\nv_local = VectorMPI_local(my_local_vector)\nA_local = MatrixMPI_local(my_local_matrix)\nS_local = SparseMatrixMPI_local(my_local_sparse)","category":"section"},{"location":"guide/#Efficient-Local-Only-Construction","page":"User Guide","title":"Efficient Local-Only Construction","text":"For large matrices, avoid replicating data across all ranks by only populating each rank's local portion:\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Global dimensions\nm, n = 1000, 1000\n\n# Compute which rows this rank owns\nrows_per_rank = div(m, nranks)\nremainder = mod(m, nranks)\nmy_row_start = 1 + rank * rows_per_rank + min(rank, remainder)\nmy_row_end = my_row_start + rows_per_rank - 1 + (rank < remainder ? 1 : 0)\n\n# Create sparse matrix with correct size, but only populate local rows\nI, J, V = Int[], Int[], Float64[]\nfor i in my_row_start:my_row_end\n    # Example: tridiagonal matrix\n    if i > 1\n        push!(I, i); push!(J, i-1); push!(V, -1.0)\n    end\n    push!(I, i); push!(J, i); push!(V, 2.0)\n    if i < m\n        push!(I, i); push!(J, i+1); push!(V, -1.0)\n    end\nend\nA = sparse(I, J, V, m, n)\n\n# The constructor extracts only local rows - other rows are ignored\nAdist = SparseMatrixMPI{Float64}(A)","category":"section"},{"location":"guide/#Basic-Operations","page":"User Guide","title":"Basic Operations","text":"","category":"section"},{"location":"guide/#Vector-Operations","page":"User Guide","title":"Vector Operations","text":"v = VectorMPI(randn(100))\nw = VectorMPI(randn(100))\n\n# Arithmetic\nu = v + w\nu = v - w\nu = 2.0 * v\nu = v * 2.0\n\n# Linear algebra\nn = norm(v)\nd = dot(v, w)\nc = conj(v)","category":"section"},{"location":"guide/#Matrix-Vector-Products","page":"User Guide","title":"Matrix-Vector Products","text":"A = MatrixMPI(randn(50, 100))\nv = VectorMPI(randn(100))\n\n# Matrix-vector multiply\ny = A * v","category":"section"},{"location":"guide/#Sparse-Operations","page":"User Guide","title":"Sparse Operations","text":"using SparseArrays\n\nA = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))\nv = VectorMPI(randn(100))\n\n# Matrix-vector multiply\ny = A * v\n\n# Matrix-matrix multiply\nB = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))\nC = A * B","category":"section"},{"location":"guide/#Solving-Linear-Systems","page":"User Guide","title":"Solving Linear Systems","text":"","category":"section"},{"location":"guide/#Direct-Solve-with-Backslash","page":"User Guide","title":"Direct Solve with Backslash","text":"using SparseArrays\n\n# Create a well-conditioned sparse matrix\nA = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1) + 10I)\nb = VectorMPI(randn(100))\n\n# Solve A * x = b\nx = A \\ b","category":"section"},{"location":"guide/#Symmetric-Systems-(Faster)","page":"User Guide","title":"Symmetric Systems (Faster)","text":"For symmetric matrices, wrap with Symmetric to use faster LDLT factorization:\n\nusing LinearAlgebra\n\n# Create symmetric positive definite matrix\nA_base = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))\nA_spd = A_base + SparseMatrixMPI(transpose(A_base)) + \n        SparseMatrixMPI{Float64}(sparse(10.0I, 100, 100))\n\nb = VectorMPI(randn(100))\n\n# Use Symmetric wrapper for faster solve\nx = Symmetric(A_spd) \\ b","category":"section"},{"location":"guide/#Reusing-Factorizations","page":"User Guide","title":"Reusing Factorizations","text":"For repeated solves with the same matrix, compute the factorization once:\n\nusing LinearAlgebra\n\n# LU factorization\nF = lu(A)\nx1 = F \\ b1\nx2 = F \\ b2\nfinalize!(F)  # Clean up MUMPS resources\n\n# LDLT factorization (symmetric matrices)\nF = ldlt(A_spd)\nx = F \\ b\nfinalize!(F)","category":"section"},{"location":"guide/#Threading","page":"User Guide","title":"Threading","text":"LinearAlgebraMPI uses two threading mechanisms for the MUMPS sparse direct solver:\n\nOpenMP threads (OMP_NUM_THREADS) - Affects MUMPS algorithm-level parallelism\nBLAS threads (OPENBLAS_NUM_THREADS) - Affects dense matrix operations in both Julia and MUMPS","category":"section"},{"location":"guide/#MUMPS-Solver-Threading","page":"User Guide","title":"MUMPS Solver Threading","text":"LinearAlgebraMPI uses the MUMPS (MUltifrontal Massively Parallel Solver) library for sparse direct solves via lu() and ldlt(). MUMPS has two independent threading mechanisms that can be tuned for performance.\n\nOpenMP threads (OMP_NUM_THREADS)\n\nControls MUMPS's algorithm-level parallelism\nThe multifrontal method builds an elimination tree of \"frontal matrices\"\nOpenMP threads process independent subtrees in parallel\nThis is coarse-grained: different threads work on different parts of the matrix\n\nBLAS threads (OPENBLAS_NUM_THREADS)\n\nControls parallelism inside dense matrix operations\nWhen MUMPS factors a frontal matrix, it calls BLAS routines (DGEMM, etc.)\nOpenBLAS can parallelize these dense operations\nThis is fine-grained: threads cooperate on the same dense block\n\nNote on BLAS libraries: Julia and MUMPS use separate OpenBLAS libraries (libopenblas64_.dylib for Julia's ILP64 interface, libopenblas.dylib for MUMPS's LP64 interface). Both libraries read OPENBLAS_NUM_THREADS at initialization, so this environment variable affects both.","category":"section"},{"location":"guide/#Recommended-Configuration","page":"User Guide","title":"Recommended Configuration","text":"For behavior that closely matches Julia's built-in sparse solver (UMFPACK):\n\nexport OMP_NUM_THREADS=1\nexport OPENBLAS_NUM_THREADS=<number_of_cores>\n\nThis configuration uses only BLAS-level threading, which is the same strategy Julia's built-in solver uses.","category":"section"},{"location":"guide/#Performance-Comparison-(Single-Rank)","page":"User Guide","title":"Performance Comparison (Single-Rank)","text":"The following table compares MUMPS (OMP_NUM_THREADS=1, OPENBLAS_NUM_THREADS=10) against Julia's built-in sparse solver (also using the same settings) on a 2D Laplacian problem. This is a single-rank comparison to establish baseline overhead; multi-rank MPI parallelism provides additional speedup. Benchmarks were run on a 2025 M4 MacBook Pro with 10 CPU cores:\n\nn Julia (ms) MUMPS (ms) Ratio\n9 0.004 0.041 9.7x\n100 0.023 0.070 3.0x\n992 0.269 0.418 1.6x\n10,000 4.28 5.60 1.31x\n99,856 51.2 56.9 1.11x\n1,000,000 665 666 1.0x\n\nKey observations:\n\nAt small problem sizes, MUMPS has initialization overhead (~0.04ms)\nAt large problem sizes (n ≥ 100,000), MUMPS is within 11% of Julia's built-in solver\nAt n = 1,000,000, MUMPS matches Julia's speed exactly (1.0x ratio)","category":"section"},{"location":"guide/#Default-Behavior","page":"User Guide","title":"Default Behavior","text":"For optimal performance, set threading environment variables before starting Julia:\n\nexport OMP_NUM_THREADS=1\nexport OPENBLAS_NUM_THREADS=10  # or your number of CPU cores\njulia your_script.jl\n\nEnvironment variables must be set before starting Julia because OpenBLAS creates its thread pool during library initialization. LinearAlgebraMPI attempts to set sensible defaults programmatically, but this may not always take effect if the thread pool is already initialized.\n\nYou can also add these to your shell profile (.bashrc, .zshrc, etc.) or Julia's startup.jl:\n\n# In ~/.julia/config/startup.jl\nENV[\"OMP_NUM_THREADS\"] = \"1\"\nENV[\"OPENBLAS_NUM_THREADS\"] = string(Sys.CPU_THREADS)","category":"section"},{"location":"guide/#Advanced:-Combined-OMP-and-BLAS-Threading","page":"User Guide","title":"Advanced: Combined OMP and BLAS Threading","text":"For some problems, combining OMP and BLAS threading can be faster:\n\nexport OMP_NUM_THREADS=4\nexport OPENBLAS_NUM_THREADS=4\njulia your_script.jl\n\nThis MUMPS configuration (OMP=4, BLAS=4) achieved 14% faster performance than Julia's built-in solver on a 1M DOF 2D Laplacian in testing. However, the optimal configuration depends on your specific problem structure and hardware.\n\nImportant caveat: OPENBLAS_NUM_THREADS is a process-wide setting that affects both MUMPS and Julia's built-in sparse solver (UMFPACK). If you set OPENBLAS_NUM_THREADS=4 to optimize MUMPS, Julia's built-in solver will also be limited to 4 BLAS threads.","category":"section"},{"location":"guide/#Row-wise-Operations-with-map_rows","page":"User Guide","title":"Row-wise Operations with map_rows","text":"The map_rows function applies a function to corresponding rows across distributed arrays:\n\nA = MatrixMPI(randn(50, 10))\n\n# Compute row norms\nnorms = map_rows(row -> norm(row), A)  # Returns VectorMPI\n\n# Compute row sums and products\nstats = map_rows(row -> [sum(row), prod(row)]', A)  # Returns MatrixMPI\n\n# Combine multiple inputs\nv = VectorMPI(randn(50))\nweighted = map_rows((row, w) -> sum(row) * w[1], A, v)","category":"section"},{"location":"guide/#Result-Types","page":"User Guide","title":"Result Types","text":"f returns Result type\nScalar VectorMPI\nColumn vector VectorMPI (concatenated)\nRow vector (v') MatrixMPI\nMatrix MatrixMPI","category":"section"},{"location":"guide/#Type-Conversions","page":"User Guide","title":"Type Conversions","text":"","category":"section"},{"location":"guide/#Gathering-to-Native-Types","page":"User Guide","title":"Gathering to Native Types","text":"Convert distributed types back to native Julia arrays (gathers data to all ranks):\n\nv_mpi = VectorMPI(randn(100))\nv_native = Vector(v_mpi)  # Full vector on all ranks\n\nA_mpi = MatrixMPI(randn(50, 30))\nA_native = Matrix(A_mpi)  # Full matrix on all ranks\n\nS_mpi = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1))\nS_native = SparseMatrixCSC(S_mpi)  # Full sparse matrix","category":"section"},{"location":"guide/#IO-and-Output","page":"User Guide","title":"IO and Output","text":"","category":"section"},{"location":"guide/#Printing-from-Rank-0","page":"User Guide","title":"Printing from Rank 0","text":"Use io0() to print from rank 0 only:\n\nprintln(io0(), \"This prints once from rank 0!\")\n\n# Custom rank selection\nprintln(io0(r=Set([0, 1])), \"Hello from ranks 0 and 1!\")","category":"section"},{"location":"guide/#MPI-Rank-Information","page":"User Guide","title":"MPI Rank Information","text":"using MPI\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)   # Current rank (0 to nranks-1)\nnranks = MPI.Comm_size(MPI.COMM_WORLD) # Total number of ranks","category":"section"},{"location":"guide/#Repartitioning","page":"User Guide","title":"Repartitioning","text":"Redistribute data to match a different partition:\n\nv = VectorMPI(randn(100))\n\n# Get current partition\nold_partition = v.partition\n\n# Create new partition\nnew_partition = uniform_partition(100, MPI.Comm_size(MPI.COMM_WORLD))\n\n# Repartition\nv_new = repartition(v, new_partition)","category":"section"},{"location":"guide/#Cache-Management","page":"User Guide","title":"Cache Management","text":"LinearAlgebraMPI caches communication plans for efficiency. Clear caches when needed:\n\nclear_plan_cache!()  # Clears all plan caches including MUMPS analysis cache","category":"section"},{"location":"guide/#MPI-Collective-Operations","page":"User Guide","title":"MPI Collective Operations","text":"warning: All Operations Are Collective\nMost LinearAlgebraMPI functions are MPI collective operations. All ranks must:Call the function together\nUse the same parameters\nAvoid conditional execution based on rank\n\nCorrect:\n\n# All ranks execute this together\nx = A \\ b\n\nIncorrect (causes deadlock):\n\nif rank == 0\n    x = A \\ b  # Only rank 0 calls - DEADLOCK!\nend","category":"section"},{"location":"guide/#Next-Steps","page":"User Guide","title":"Next Steps","text":"See Examples for detailed code examples\nSee the API Reference for detailed function documentation","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"This page provides detailed documentation for all exported types and functions in LinearAlgebraMPI.jl.\n\nnote: MPI Collective Operations\nUnless otherwise noted, all functions are MPI collective operations. Every MPI rank must call these functions together.","category":"section"},{"location":"api/#Distributed-Types","page":"API Reference","title":"Distributed Types","text":"","category":"section"},{"location":"api/#VectorMPI","page":"API Reference","title":"VectorMPI","text":"","category":"section"},{"location":"api/#MatrixMPI","page":"API Reference","title":"MatrixMPI","text":"","category":"section"},{"location":"api/#SparseMatrixMPI","page":"API Reference","title":"SparseMatrixMPI","text":"","category":"section"},{"location":"api/#SparseMatrixCSR","page":"API Reference","title":"SparseMatrixCSR","text":"","category":"section"},{"location":"api/#Local-Constructors","page":"API Reference","title":"Local Constructors","text":"These constructors create distributed types from local data without global communication.","category":"section"},{"location":"api/#Row-wise-Operations","page":"API Reference","title":"Row-wise Operations","text":"","category":"section"},{"location":"api/#Linear-System-Solvers","page":"API Reference","title":"Linear System Solvers","text":"","category":"section"},{"location":"api/#Partition-Utilities","page":"API Reference","title":"Partition Utilities","text":"","category":"section"},{"location":"api/#Cache-Management","page":"API Reference","title":"Cache Management","text":"","category":"section"},{"location":"api/#IO-Utilities","page":"API Reference","title":"IO Utilities","text":"","category":"section"},{"location":"api/#Type-Mappings","page":"API Reference","title":"Type Mappings","text":"","category":"section"},{"location":"api/#Native-to-MPI-Conversions","page":"API Reference","title":"Native to MPI Conversions","text":"Native Type MPI Type Description\nVector{T} VectorMPI{T} Distributed vector\nMatrix{T} MatrixMPI{T} Distributed dense matrix\nSparseMatrixCSC{T,Ti} SparseMatrixMPI{T,Ti} Distributed sparse matrix","category":"section"},{"location":"api/#MPI-to-Native-Conversions","page":"API Reference","title":"MPI to Native Conversions","text":"MPI Type Native Type Function\nVectorMPI{T} Vector{T} Vector(v)\nMatrixMPI{T} Matrix{T} Matrix(A)\nSparseMatrixMPI{T,Ti} SparseMatrixCSC{T,Ti} SparseMatrixCSC(A)","category":"section"},{"location":"api/#Supported-Operations","page":"API Reference","title":"Supported Operations","text":"","category":"section"},{"location":"api/#VectorMPI-Operations","page":"API Reference","title":"VectorMPI Operations","text":"Arithmetic: +, -, * (scalar)\nLinear algebra: norm, dot, conj\nIndexing: v[i] (global index)\nConversion: Vector(v)","category":"section"},{"location":"api/#MatrixMPI-Operations","page":"API Reference","title":"MatrixMPI Operations","text":"Arithmetic: * (scalar), matrix-vector product\nTranspose: transpose(A)\nIndexing: A[i, j] (global indices)\nConversion: Matrix(A)","category":"section"},{"location":"api/#SparseMatrixMPI-Operations","page":"API Reference","title":"SparseMatrixMPI Operations","text":"Arithmetic: +, -, * (scalar, matrix-vector, matrix-matrix)\nTranspose: transpose(A)\nLinear solve: A \\ b, Symmetric(A) \\ b\nUtilities: nnz, norm, issymmetric\nConversion: SparseMatrixCSC(A)","category":"section"},{"location":"api/#Factorization-Types","page":"API Reference","title":"Factorization Types","text":"LinearAlgebraMPI uses MUMPS for sparse direct solves:\n\nlu(A): LU factorization (general matrices)\nldlt(A): LDLT factorization (symmetric matrices, faster)\n\nBoth return factorization objects that support:\n\nF \\ b: Solve with factorization\nfinalize!(F): Release MUMPS resources","category":"section"},{"location":"api/#LinearAlgebraMPI.VectorMPI","page":"API Reference","title":"LinearAlgebraMPI.VectorMPI","text":"VectorMPI{T}\n\nA distributed dense vector partitioned across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the partition\npartition::Vector{Int}: Partition boundaries, length = nranks + 1\nv::Vector{T}: Local vector elements owned by this rank\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.MatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.MatrixMPI","text":"MatrixMPI{T}\n\nA distributed dense matrix partitioned by rows across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (for transpose)\nA::Matrix{T}: Local rows (NOT transposed), size = (local_nrows, ncols)\n\nInvariants\n\nrow_partition and col_partition are sorted\nrow_partition[nranks+1] = total number of rows + 1\ncol_partition[nranks+1] = total number of columns + 1\nsize(A, 1) == row_partition[rank+2] - row_partition[rank+1]\nsize(A, 2) == col_partition[end] - 1\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.SparseMatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.SparseMatrixMPI","text":"SparseMatrixMPI{T,Ti}\n\nA distributed sparse matrix partitioned by rows across MPI ranks.\n\nType Parameters\n\nT: Element type (e.g., Float64, ComplexF64)\nTi: Index type (e.g., Int, Int32), defaults to Int\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (placeholder for transpose)\ncol_indices::Vector{Int}: Global column indices that appear in the local part (local→global mapping)\nA::SparseMatrixCSR{T,Ti}: Local rows in CSR format for efficient row-wise iteration\ncached_transpose: Cached materialized transpose (bidirectionally linked)\n\nInvariants\n\ncol_indices, row_partition, and col_partition are sorted\nrow_partition[nranks+1] = total number of rows\ncol_partition[nranks+1] = total number of columns\nsize(A, 1) == row_partition[rank+1] - row_partition[rank] (number of local rows)\nsize(A.parent, 1) == length(col_indices) (compressed column dimension)\nA.parent.rowval contains local indices in 1:length(col_indices)\n\nStorage Details\n\nThe local rows are stored in CSR format (Compressed Sparse Row), which enables efficient row-wise iteration - essential for a row-partitioned distributed matrix.\n\nIn Julia, SparseMatrixCSR{T,Ti} is a type alias for Transpose{T, SparseMatrixCSC{T,Ti}}. This type has a dual interpretation:\n\nSemantic view: A lazy transpose of a CSC matrix\nStorage view: Row-major (CSR) access to the data\n\nThe underlying A.parent::SparseMatrixCSC stores the transposed data with:\n\nA.parent.m = length(col_indices) (compressed, not global ncols)\nA.parent.n = number of local rows (columns in the parent = rows in CSR)\nA.parent.colptr = row pointers for the CSR format\nA.parent.rowval = LOCAL column indices (1:length(col_indices))\ncol_indices[local_idx] maps local→global column indices\n\nThis compression avoids \"hypersparse\" storage where the column dimension would be the global number of columns even if only a few columns have nonzeros locally.\n\nAccess the underlying CSC storage via A.parent when needed for low-level operations.\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.SparseMatrixCSR","page":"API Reference","title":"LinearAlgebraMPI.SparseMatrixCSR","text":"SparseMatrixCSR{Tv,Ti} = Transpose{Tv, SparseMatrixCSC{Tv,Ti}}\n\nType alias for CSR (Compressed Sparse Row) storage format.\n\nThe Dual Life of Transpose{SparseMatrixCSC}\n\nIn Julia, the type Transpose{Tv, SparseMatrixCSC{Tv,Ti}} has two interpretations:\n\nSemantic interpretation: A lazy transpose wrapper around a CSC matrix. When you call transpose(A) on a SparseMatrixCSC, you get this wrapper that represents A^T without copying data.\nStorage interpretation: CSR (row-major) access to sparse data. The underlying CSC stores columns contiguously, but through the transpose wrapper, we can iterate efficiently over rows instead of columns.\n\nThis alias clarifies intent: use SparseMatrixCSR when you want row-major storage semantics, and transpose(A) when you want the mathematical transpose.\n\nCSR vs CSC Storage\n\nCSC (Compressed Sparse Column): Julia's native sparse format. Efficient for column-wise operations, matrix-vector products with column access.\nCSR (Compressed Sparse Row): Efficient for row-wise operations, matrix-vector products with row access, and row-partitioned distributed matrices.\n\nFor SparseMatrixCSR, the underlying parent::SparseMatrixCSC stores the transposed matrix. If B = SparseMatrixCSR(A) represents matrix M, then B.parent is a CSC storing M^T. This means:\n\nB.parent.colptr acts as row pointers for M\nB.parent.rowval contains column indices for M\nB.parent.nzval contains values in row-major order\n\nUsage Note\n\nJulia will still display this type as Transpose{Float64, SparseMatrixCSC{...}}, not as SparseMatrixCSR. The alias improves code clarity but doesn't affect type printing.\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.VectorMPI_local","page":"API Reference","title":"LinearAlgebraMPI.VectorMPI_local","text":"VectorMPI_local(v_local::Vector{T}, comm::MPI.Comm=MPI.COMM_WORLD) where T\n\nCreate a VectorMPI from a local vector on each rank.\n\nUnlike VectorMPI(v_global) which takes a global vector and partitions it, this constructor takes only the local portion of the vector that each rank owns. The partition is computed by gathering the local sizes from all ranks.\n\nExample\n\n# Rank 0 has [1.0, 2.0], Rank 1 has [3.0, 4.0, 5.0]\nv = VectorMPI_local([1.0, 2.0])  # on rank 0\nv = VectorMPI_local([3.0, 4.0, 5.0])  # on rank 1\n# Result: distributed vector [1.0, 2.0, 3.0, 4.0, 5.0] with partition [1, 3, 6]\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.MatrixMPI_local","page":"API Reference","title":"LinearAlgebraMPI.MatrixMPI_local","text":"MatrixMPI_local(A_local::Matrix{T}; comm=MPI.COMM_WORLD, col_partition=...) where T\n\nCreate a MatrixMPI from a local matrix on each rank.\n\nUnlike MatrixMPI(M_global) which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.\n\nAll ranks must have local matrices with the same number of columns. A collective error is raised if the column counts don't match.\n\nKeyword Arguments\n\ncomm::MPI.Comm: MPI communicator (default: MPI.COMM_WORLD)\ncol_partition::Vector{Int}: Column partition boundaries (default: uniform_partition(size(A_local,2), nranks))\n\nExample\n\n# Rank 0 has 2×3 matrix, Rank 1 has 3×3 matrix\nA = MatrixMPI_local(randn(2, 3))  # on rank 0\nA = MatrixMPI_local(randn(3, 3))  # on rank 1\n# Result: 5×3 distributed matrix with row_partition [1, 3, 6]\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.SparseMatrixMPI_local","page":"API Reference","title":"LinearAlgebraMPI.SparseMatrixMPI_local","text":"SparseMatrixMPI_local(A_local::SparseMatrixCSR{T,Ti}; comm=MPI.COMM_WORLD, col_partition=...) where {T,Ti}\nSparseMatrixMPI_local(A_local::Adjoint{T,SparseMatrixCSC{T,Ti}}; comm=MPI.COMM_WORLD, col_partition=...) where {T,Ti}\n\nCreate a SparseMatrixMPI from a local sparse matrix on each rank.\n\nUnlike SparseMatrixMPI{T}(A_global) which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.\n\nThe input A_local must be a SparseMatrixCSR{T,Ti} (or Adjoint of SparseMatrixCSC{T,Ti}) where:\n\nA_local.parent.n = number of local rows on this rank\nA_local.parent.m = global number of columns (must match on all ranks)\nA_local.parent.rowval = global column indices\n\nAll ranks must have local matrices with the same number of columns (block widths must match). A collective error is raised if the column counts don't match.\n\nNote: For Adjoint inputs, the values are conjugated to match the adjoint semantics.\n\nKeyword Arguments\n\ncomm::MPI.Comm: MPI communicator (default: MPI.COMM_WORLD)\ncol_partition::Vector{Int}: Column partition boundaries (default: uniform_partition(A_local.parent.m, nranks))\n\nExample\n\n# Create local rows in CSR format\n# Rank 0 owns rows 1-2 of a 5×3 matrix, Rank 1 owns rows 3-5\nlocal_csc = sparse([1, 1, 2], [1, 2, 3], [1.0, 2.0, 3.0], 2, 3)  # 2 local rows, 3 cols\nA = SparseMatrixMPI_local(SparseMatrixCSR(local_csc))\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.map_rows","page":"API Reference","title":"LinearAlgebraMPI.map_rows","text":"map_rows(f, A...)\n\nApply function f to corresponding rows of distributed vectors/matrices.\n\nEach argument in A... must be either a VectorMPI or MatrixMPI. All inputs are repartitioned to match the partition of the first argument before applying f.\n\nFor each row index i, f is called with the i-th row from each input:\n\nFor VectorMPI, the i-th \"row\" is a length-1 view of element i\nFor MatrixMPI, the i-th row is a row vector (a view into the local matrix)\n\nResult Type (vcat semantics)\n\nThe result type depends on what f returns, matching the behavior of vcat:\n\nf returns Julia type Result\nscalar Number VectorMPI (one element per input row)\ncolumn vector AbstractVector VectorMPI (vcat concatenates all vectors)\nrow vector Transpose, Adjoint MatrixMPI (vcat stacks as rows)\nmatrix AbstractMatrix MatrixMPI (vcat stacks rows)\n\nLazy Wrappers\n\nJulia's transpose(v) and v' (adjoint) return lazy wrappers that are subtypes of AbstractMatrix, so they produce MatrixMPI results:\n\nmap_rows(r -> [1,2,3], A)           # Vector → VectorMPI (length 3n)\nmap_rows(r -> [1,2,3]', A)          # Adjoint → MatrixMPI (n×3)\nmap_rows(r -> transpose([1,2,3]), A) # Transpose → MatrixMPI (n×3)\nmap_rows(r -> conj([1,2,3]), A)     # Vector → VectorMPI (length 3n)\nmap_rows(r -> [1 2 3], A)           # Matrix literal → MatrixMPI (n×3)\n\nExamples\n\n# Element-wise product of two vectors\nu = VectorMPI([1.0, 2.0, 3.0])\nv = VectorMPI([4.0, 5.0, 6.0])\nw = map_rows((a, b) -> a[1] * b[1], u, v)  # VectorMPI([4.0, 10.0, 18.0])\n\n# Row norms of a matrix\nA = MatrixMPI(randn(5, 3))\nnorms = map_rows(r -> norm(r), A)  # VectorMPI of row norms\n\n# Expand each row to multiple elements (vcat behavior)\nA = MatrixMPI(randn(3, 2))\nresult = map_rows(r -> [1, 2, 3], A)  # VectorMPI of length 9\n\n# Return row vectors to build a matrix\nA = MatrixMPI(randn(3, 2))\nresult = map_rows(r -> [1, 2, 3]', A)  # 3×3 MatrixMPI\n\n# Variable-length output per row\nv = VectorMPI([1.0, 2.0, 3.0])\nresult = map_rows(r -> ones(Int(r[1])), v)  # VectorMPI of length 6 (1+2+3)\n\n# Mixed inputs: matrix rows weighted by vector elements\nA = MatrixMPI(randn(4, 3))\nw = VectorMPI([1.0, 2.0, 3.0, 4.0])\nresult = map_rows((row, wi) -> sum(row) * wi[1], A, w)  # VectorMPI\n\nThis is the MPI-distributed version of:\n\nmap_rows(f, A...) = vcat((f.((eachrow.(A))...))...)\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.solve","page":"API Reference","title":"LinearAlgebraMPI.solve","text":"solve(F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T\n\nSolve the linear system A*x = b using the precomputed MUMPS factorization.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.solve!","page":"API Reference","title":"LinearAlgebraMPI.solve!","text":"solve!(x::VectorMPI{T}, F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T\n\nSolve A*x = b in-place using MUMPS factorization.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.finalize!","page":"API Reference","title":"LinearAlgebraMPI.finalize!","text":"finalize!(F::MUMPSFactorizationMPI)\n\nRelease MUMPS resources. Must be called on all ranks together.\n\nNote: If the MUMPS object is shared with the analysis cache (ownsmumps=false), this only removes the factorization from the registry. The MUMPS object itself is finalized when `clearmumpsanalysiscache!()` is called.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.uniform_partition","page":"API Reference","title":"LinearAlgebraMPI.uniform_partition","text":"uniform_partition(n::Int, nranks::Int) -> Vector{Int}\n\nCompute a balanced partition of n elements across nranks ranks. Returns a vector of length nranks + 1 with 1-indexed partition boundaries.\n\nThe first mod(n, nranks) ranks get div(n, nranks) + 1 elements, the remaining ranks get div(n, nranks) elements.\n\nExample\n\npartition = uniform_partition(10, 4)  # [1, 4, 7, 9, 11]\n# Rank 0: 1:3 (3 elements)\n# Rank 1: 4:6 (3 elements)\n# Rank 2: 7:8 (2 elements)\n# Rank 3: 9:10 (2 elements)\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.repartition","page":"API Reference","title":"LinearAlgebraMPI.repartition","text":"repartition(x::VectorMPI{T}, p::Vector{Int}) where T\n\nRedistribute a VectorMPI to a new partition p.\n\nThe partition p must be a valid partition vector of length nranks + 1 with p[1] == 1 and p[end] == length(x) + 1.\n\nReturns a new VectorMPI with the same data but partition == p.\n\nExample\n\nv = VectorMPI([1.0, 2.0, 3.0, 4.0])  # uniform partition\nnew_partition = [1, 2, 5]  # rank 0 gets 1 element, rank 1 gets 3\nv_repart = repartition(v, new_partition)\n\n\n\n\n\nrepartition(A::MatrixMPI{T}, p::Vector{Int}) where T\n\nRedistribute a MatrixMPI to a new row partition p. The col_partition remains unchanged.\n\nThe partition p must be a valid partition vector of length nranks + 1 with p[1] == 1 and p[end] == size(A, 1) + 1.\n\nReturns a new MatrixMPI with the same data but row_partition == p.\n\nExample\n\nA = MatrixMPI(randn(6, 4))  # uniform partition\nnew_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank\nA_repart = repartition(A, new_partition)\n\n\n\n\n\nrepartition(A::SparseMatrixMPI{T}, p::Vector{Int}) where T\n\nRedistribute a SparseMatrixMPI to a new row partition p. The col_partition remains unchanged.\n\nThe partition p must be a valid partition vector of length nranks + 1 with p[1] == 1 and p[end] == size(A, 1) + 1.\n\nReturns a new SparseMatrixMPI with the same data but row_partition == p.\n\nExample\n\nA = SparseMatrixMPI{Float64}(sprand(6, 4, 0.5))  # uniform partition\nnew_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank\nA_repart = repartition(A, new_partition)\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.clear_plan_cache!","page":"API Reference","title":"LinearAlgebraMPI.clear_plan_cache!","text":"clear_plan_cache!()\n\nClear all memoized plan caches, including the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.clear_mumps_analysis_cache!","page":"API Reference","title":"LinearAlgebraMPI.clear_mumps_analysis_cache!","text":"clear_mumps_analysis_cache!()\n\nClear the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.io0","page":"API Reference","title":"LinearAlgebraMPI.io0","text":"io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)\n\nReturn io if the current MPI rank is in set r, otherwise return dn (default: devnull).\n\nThis is useful for printing only from specific ranks:\n\nprintln(io0(), \"Hello from rank 0!\")\nprintln(io0(r=Set([0,1])), \"Hello from ranks 0 and 1!\")\n\nWith string interpolation:\n\nprintln(io0(), \"Matrix A = $A\")\n\n\n\n\n\n","category":"function"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page provides detailed examples of using LinearAlgebraMPI.jl for various distributed sparse matrix operations.","category":"section"},{"location":"examples/#Matrix-Multiplication","page":"Examples","title":"Matrix Multiplication","text":"","category":"section"},{"location":"examples/#Square-Matrices","page":"Examples","title":"Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Create a tridiagonal matrix (same on all ranks)\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Create another tridiagonal matrix\nV2 = [1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]\nB = sparse(I, J, V2, n, n)\n\n# Distribute matrices\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Multiply\nCdist = Adist * Bdist\n\n# Verify against reference\nC_ref = A * B\nC_ref_dist = SparseMatrixMPI{Float64}(C_ref)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nprintln(io0(), \"Multiplication error: $err\")\n","category":"section"},{"location":"examples/#Non-Square-Matrices","page":"Examples","title":"Non-Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# A is 6x8, B is 8x10, result is 6x10\nm, k, n = 6, 8, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nJ_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 2]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, k)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, k, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\nCdist = Adist * Bdist\n\n@assert size(Cdist) == (m, n)\n\nprintln(io0(), \"Result size: $(size(Cdist))\")\n","category":"section"},{"location":"examples/#Complex-Matrices","page":"Examples","title":"Complex Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nn = 8\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# Complex values\nV_A = ComplexF64.([2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]) .+\n      im .* ComplexF64.([0.1*ones(n); 0.2*ones(n-1); -0.2*ones(n-1)])\nA = sparse(I, J, V_A, n, n)\n\nV_B = ComplexF64.([1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]) .+\n      im .* ComplexF64.([-0.1*ones(n); 0.1*ones(n-1); 0.1*ones(n-1)])\nB = sparse(I, J, V_B, n, n)\n\nAdist = SparseMatrixMPI{ComplexF64}(A)\nBdist = SparseMatrixMPI{ComplexF64}(B)\n\n# Multiplication\nCdist = Adist * Bdist\n\n# Conjugate\nAconj = conj(Adist)\n\n# Adjoint (conjugate transpose) - returns lazy wrapper\nAadj = Adist'\n\n# Using adjoint in multiplication (materializes automatically)\nresult = Aadj * Bdist\n\nprintln(io0(), \"Complex matrix operations completed\")\n","category":"section"},{"location":"examples/#Addition-and-Subtraction","page":"Examples","title":"Addition and Subtraction","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nn = 8\n\n# Matrices with different sparsity patterns\n# Matrix A: upper triangular entries\nI_A = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nJ_A = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nV_A = Float64.(1:9)\nA = sparse(I_A, J_A, V_A, n, n)\n\n# Matrix B: lower triangular entries\nI_B = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nJ_B = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nV_B = Float64.(9:-1:1)\nB = sparse(I_B, J_B, V_B, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Addition - handles different sparsity patterns\nCdist = Adist + Bdist\n\n# Subtraction\nDdist = Adist - Bdist\n\n# Verify\nC_ref_dist = SparseMatrixMPI{Float64}(A + B)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nprintln(io0(), \"Addition error: $err\")\n","category":"section"},{"location":"examples/#Transpose-Operations","page":"Examples","title":"Transpose Operations","text":"","category":"section"},{"location":"examples/#Lazy-Transpose","page":"Examples","title":"Lazy Transpose","text":"The transpose function creates a lazy wrapper without transposing the data. This is efficient because the actual transpose is only computed when needed:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nm, n = 8, 6\nI_C = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_C = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nV_C = Float64.(1:length(I_C))\nC = sparse(I_C, J_C, V_C, m, n)\n\nI_D = [1, 2, 3, 4, 5, 6, 1, 2]\nJ_D = [1, 2, 3, 4, 5, 6, 7, 8]\nV_D = Float64.(1:length(I_D))\nD = sparse(I_D, J_D, V_D, n, m)\n\nCdist = SparseMatrixMPI{Float64}(C)\nDdist = SparseMatrixMPI{Float64}(D)\n\n# transpose(C) * transpose(D) = transpose(D * C)\n# This is computed efficiently without explicitly transposing\nresult = transpose(Cdist) * transpose(Ddist)\n\nprintln(io0(), \"Lazy transpose multiplication completed\")\n","category":"section"},{"location":"examples/#Transpose-in-Multiplication","page":"Examples","title":"Transpose in Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# A is 8x6, so A' is 6x8\n# B is 8x10, so A' * B is 6x10\nm, n, p = 8, 6, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, n)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, m, p)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# transpose(A) * B - A is automatically materialized as transpose\nresult_dist = transpose(Adist) * Bdist\n\n# Verify\nref = sparse(A') * B\nref_dist = SparseMatrixMPI{Float64}(ref)\nerr = norm(result_dist - ref_dist, Inf)\n\nprintln(io0(), \"transpose(A) * B error: $err\")\n","category":"section"},{"location":"examples/#Scalar-Multiplication","page":"Examples","title":"Scalar Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3]\nJ = [1, 2, 3, 4, 5, 6, 7, 8]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Scalar times matrix\na = 2.5\nresult1 = a * Adist\nresult2 = Adist * a  # Equivalent\n\n# Scalar times lazy transpose\nAt = transpose(Adist)\nresult3 = a * At  # Returns transpose(a * A)\n\n# Verify\nref_dist = SparseMatrixMPI{Float64}(a * A)\nerr1 = norm(result1 - ref_dist, Inf)\nerr2 = norm(result2 - ref_dist, Inf)\n\nprintln(io0(), \"Scalar multiplication errors: $err1, $err2\")\n","category":"section"},{"location":"examples/#Computing-Norms","page":"Examples","title":"Computing Norms","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3, 2, 4]\nJ = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Element-wise norms (treating matrix as vector)\nfrob_norm = norm(Adist)        # Frobenius (2-norm)\none_norm = norm(Adist, 1)      # Sum of absolute values\ninf_norm = norm(Adist, Inf)    # Max absolute value\np_norm = norm(Adist, 3)        # General p-norm\n\n# Operator norms\nop_1 = opnorm(Adist, 1)        # Max absolute column sum\nop_inf = opnorm(Adist, Inf)    # Max absolute row sum\n\nprintln(io0(), \"Frobenius norm: $frob_norm\")\nprintln(io0(), \"1-norm: $one_norm\")\nprintln(io0(), \"Inf-norm: $inf_norm\")\nprintln(io0(), \"3-norm: $p_norm\")\nprintln(io0(), \"Operator 1-norm: $op_1\")\nprintln(io0(), \"Operator Inf-norm: $op_inf\")\n","category":"section"},{"location":"examples/#Iterative-Methods-Example","page":"Examples","title":"Iterative Methods Example","text":"Here's an example of using LinearAlgebraMPI.jl for power iteration to find the dominant eigenvalue:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Create a symmetric positive definite matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# For power iteration, we need matrix-vector products\n# Currently LinearAlgebraMPI focuses on matrix-matrix products\n# This example shows how to use A*A for related computations\n\n# Compute A^2\nA2dist = Adist * Adist\n\n# Compute the Frobenius norm of A^2\nnorm_A2 = norm(A2dist)\n\nprintln(io0(), \"||A^2||_F = $norm_A2\")\n# For SPD matrices, this relates to the eigenvalues\n","category":"section"},{"location":"examples/#Solving-Linear-Systems","page":"Examples","title":"Solving Linear Systems","text":"LinearAlgebraMPI provides distributed sparse direct solvers using the multifrontal method.","category":"section"},{"location":"examples/#LDLT-Factorization-(Symmetric-Matrices)","page":"Examples","title":"LDLT Factorization (Symmetric Matrices)","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Create a symmetric positive definite tridiagonal matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Distribute the matrix\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Compute LDLT factorization\nF = ldlt(Adist)\n\n# Create right-hand side\nb = VectorMPI(ones(n))\n\n# Solve Ax = b\nx = solve(F, b)\n\n# Or use backslash syntax\nx = F \\ b\n\n# Verify solution\nx_full = Vector(x)\nresidual = norm(A * x_full - ones(n), Inf)\n\nprintln(io0(), \"LDLT solve residual: $residual\")\n","category":"section"},{"location":"examples/#LU-Factorization-(General-Matrices)","page":"Examples","title":"LU Factorization (General Matrices)","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Create a general (non-symmetric) tridiagonal matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.8*ones(n-1)]  # Non-symmetric\nA = sparse(I, J, V, n, n)\n\n# Distribute and factorize\nAdist = SparseMatrixMPI{Float64}(A)\nF = lu(Adist)\n\n# Solve\nb = VectorMPI(ones(n))\nx = solve(F, b)\n\n# Verify\nx_full = Vector(x)\nresidual = norm(A * x_full - ones(n), Inf)\n\nprintln(io0(), \"LU solve residual: $residual\")\n","category":"section"},{"location":"examples/#Symmetric-Indefinite-Matrices","page":"Examples","title":"Symmetric Indefinite Matrices","text":"LDLT uses Bunch-Kaufman pivoting to handle symmetric indefinite matrices:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\n# Symmetric indefinite matrix (alternating signs on diagonal)\nn = 50\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\ndiag_vals = [(-1.0)^i * 2.0 for i in 1:n]  # Alternating signs\nV = [diag_vals; -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nF = ldlt(Adist)\n\nb = VectorMPI(collect(1.0:n))\nx = solve(F, b)\n\nx_full = Vector(x)\nresidual = norm(A * x_full - collect(1.0:n), Inf)\n\nprintln(io0(), \"Indefinite LDLT residual: $residual\")\n","category":"section"},{"location":"examples/#Reusing-Symbolic-Factorization","page":"Examples","title":"Reusing Symbolic Factorization","text":"For sequences of matrices with the same sparsity pattern, the symbolic factorization is cached and reused:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# First matrix\nV1 = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA1 = sparse(I, J, V1, n, n)\nA1dist = SparseMatrixMPI{Float64}(A1)\n\n# First factorization - computes symbolic phase\nF1 = ldlt(A1dist; reuse_symbolic=true)\n\n# Second matrix - same structure, different values\nV2 = [8.0*ones(n); -2.0*ones(n-1); -2.0*ones(n-1)]\nA2 = sparse(I, J, V2, n, n)\nA2dist = SparseMatrixMPI{Float64}(A2)\n\n# Second factorization - reuses cached symbolic phase (faster)\nF2 = ldlt(A2dist; reuse_symbolic=true)\n\n# Both factorizations work\nb = VectorMPI(ones(n))\nx1 = solve(F1, b)\nx2 = solve(F2, b)\n\nx1_full = Vector(x1)\nx2_full = Vector(x2)\nprintln(io0(), \"F1 residual: \", norm(A1 * x1_full - ones(n), Inf))\nprintln(io0(), \"F2 residual: \", norm(A2 * x2_full - ones(n), Inf))\n","category":"section"},{"location":"examples/#Plan-Caching-and-Management","page":"Examples","title":"Plan Caching and Management","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\nB = spdiagm(0 => 1.5*ones(n), 1 => 0.5*ones(n-1), -1 => 0.5*ones(n-1))\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# First multiplication - creates and caches the plan\nC1 = Adist * Bdist\n\n# Second multiplication - reuses cached plan (faster)\nC2 = Adist * Bdist\n\n# Third multiplication - still uses cached plan\nC3 = Adist * Bdist\n\n# Clear caches when done to free memory\nclear_plan_cache!()  # Clears all caches including factorization\n\n# Or clear specific caches:\n# clear_symbolic_cache!()           # Symbolic factorizations only\n# clear_factorization_plan_cache!() # Factorization plans only\n\nprintln(io0(), \"Cached multiplication completed\")\n","category":"section"},{"location":"examples/#Dense-Matrix-Operations-with-mapslices","page":"Examples","title":"Dense Matrix Operations with mapslices","text":"The mapslices function applies a function to each row or column of a distributed dense matrix. This is useful for computing row-wise or column-wise statistics.","category":"section"},{"location":"examples/#Row-wise-Operations-(dims2)","page":"Examples","title":"Row-wise Operations (dims=2)","text":"Row-wise operations are local - no MPI communication is needed since rows are already distributed:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\n# Create a deterministic dense matrix (same on all ranks)\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\n# Distribute\nAdist = MatrixMPI(A_global)\n\n# Compute row statistics: for each row, compute [norm, max, sum]\n# This transforms 100×10 matrix to 100×3 matrix\nrow_stats = mapslices(x -> [norm(x), maximum(x), sum(x)], Adist; dims=2)\n\nprintln(io0(), \"Row statistics shape: $(size(row_stats))\")  # (100, 3)\n","category":"section"},{"location":"examples/#Column-wise-Operations-(dims1)","page":"Examples","title":"Column-wise Operations (dims=1)","text":"Column-wise operations require MPI communication to gather each full column:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\n# Create a deterministic dense matrix\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\nAdist = MatrixMPI(A_global)\n\n# Compute column statistics: for each column, compute [norm, max]\n# This transforms 100×10 matrix to 2×10 matrix\ncol_stats = mapslices(x -> [norm(x), maximum(x)], Adist; dims=1)\n\nprintln(io0(), \"Column statistics shape: $(size(col_stats))\")  # (2, 10)\n","category":"section"},{"location":"examples/#Use-Case:-Replacing-vcat(f.(eachrow(A))...)","page":"Examples","title":"Use Case: Replacing vcat(f.(eachrow(A))...)","text":"The standard Julia pattern vcat(f.(eachrow(A))...) doesn't work with distributed matrices because the type information is lost after broadcasting. Use mapslices instead:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\n# Standard Julia pattern (for comparison):\n# A = randn(5, 2)\n# f(x) = transpose([norm(x), maximum(x)])\n# B = vcat(f.(eachrow(A))...)\n\n# MPI-compatible equivalent:\nA_global = Float64.([i + 0.1*j for i in 1:100, j in 1:10])\nAdist = MatrixMPI(A_global)\n\n# Use mapslices with dims=2 to apply function to each row\n# The function returns a vector, which becomes a row in the result\ng(x) = [norm(x), maximum(x)]\nBdist = mapslices(g, Adist; dims=2)\n\nprintln(io0(), \"Result: $(size(Bdist))\")  # (100, 2)\n","category":"section"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#Prerequisites","page":"Installation","title":"Prerequisites","text":"","category":"section"},{"location":"installation/#MPI","page":"Installation","title":"MPI","text":"LinearAlgebraMPI.jl requires an MPI implementation. When you install the package, Julia automatically provides MPI.jl with MPI_jll (bundled MPI implementation).\n\nFor HPC environments, you may want to configure MPI.jl to use your system's MPI installation. See the MPI.jl documentation for details.","category":"section"},{"location":"installation/#MUMPS","page":"Installation","title":"MUMPS","text":"The package uses MUMPS for sparse direct solves. MUMPS is typically available through your system's package manager or HPC module system.","category":"section"},{"location":"installation/#Package-Installation","page":"Installation","title":"Package Installation","text":"","category":"section"},{"location":"installation/#From-GitHub","page":"Installation","title":"From GitHub","text":"using Pkg\nPkg.add(url=\"https://github.com/sloisel/LinearAlgebraMPI.jl\")","category":"section"},{"location":"installation/#Development-Installation","page":"Installation","title":"Development Installation","text":"git clone https://github.com/sloisel/LinearAlgebraMPI.jl\ncd LinearAlgebraMPI.jl\njulia --project -e 'using Pkg; Pkg.instantiate()'","category":"section"},{"location":"installation/#Verification","page":"Installation","title":"Verification","text":"Test your installation with MPI:\n\ncd LinearAlgebraMPI.jl\nmpiexec -n 2 julia --project test/runtests.jl","category":"section"},{"location":"installation/#Initialization-Pattern","page":"Installation","title":"Initialization Pattern","text":"tip: Initialization Pattern\nInitialize MPI first, then load the package:\n\n# CORRECT\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\n# Now you can use the package","category":"section"},{"location":"installation/#Running-MPI-Programs","page":"Installation","title":"Running MPI Programs","text":"Create a script file (e.g., my_program.jl):\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create distributed matrix\nA = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1) + 10I)\nb = VectorMPI(randn(100))\n\n# Solve\nx = A \\ b\n\nprintln(io0(), \"Solution computed!\")\n\nRun with MPI:\n\nmpiexec -n 4 julia --project my_program.jl","category":"section"},{"location":"installation/#Troubleshooting","page":"Installation","title":"Troubleshooting","text":"","category":"section"},{"location":"installation/#MPI-Issues","page":"Installation","title":"MPI Issues","text":"If you see MPI-related errors, try rebuilding MPI.jl:\n\nusing Pkg; Pkg.build(\"MPI\")","category":"section"},{"location":"installation/#MUMPS-Issues","page":"Installation","title":"MUMPS Issues","text":"If MUMPS fails to load, ensure it's properly installed on your system.","category":"section"},{"location":"installation/#Next-Steps","page":"Installation","title":"Next Steps","text":"Once installed, proceed to the User Guide to learn how to use the package.","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Markdown\nusing Pkg\nusing LinearAlgebraMPI\nv = string(pkgversion(LinearAlgebraMPI))\nmd\"# LinearAlgebraMPI.jl $v\"\n\nPure Julia distributed linear algebra with MPI.","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"LinearAlgebraMPI.jl provides distributed matrix and vector types for parallel computing with MPI. It offers a pure Julia implementation of distributed linear algebra, with MUMPS for sparse direct solves.","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"Distributed Types: VectorMPI, MatrixMPI, and SparseMatrixMPI for row-partitioned distributed storage\nMUMPS Solver: Direct solves using MUMPS for sparse linear systems\nRow-wise Operations: map_rows for efficient distributed row operations\nSeamless Integration: Works with standard Julia linear algebra operations\nPlan Caching: Efficient repeated operations through memoized communication plans","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create distributed sparse matrix\nA = SparseMatrixMPI{Float64}(sprandn(100, 100, 0.1) + 10I)\nb = VectorMPI(randn(100))\n\n# Solve linear system\nx = A \\ b\n\n# Row-wise operations\nnorms = map_rows(row -> norm(row), MatrixMPI(randn(50, 10)))\n\nprintln(io0(), \"Solution computed!\")\n\nRun with MPI:\n\nmpiexec -n 4 julia --project example.jl","category":"section"},{"location":"#Documentation-Contents","page":"Home","title":"Documentation Contents","text":"Pages = [\"installation.md\", \"guide.md\", \"examples.md\", \"api.md\"]\nDepth = 2","category":"section"},{"location":"#Related-Packages","page":"Home","title":"Related Packages","text":"MultiGridBarrierMPI.jl: Multigrid barrier methods using LinearAlgebraMPI\nMultiGridBarrier.jl: Core multigrid barrier method implementation\nMPI.jl: Julia MPI bindings","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.10 or later (LTS version)\nMPI installation (OpenMPI, MPICH, or Intel MPI)\nMUMPS for sparse direct solves","category":"section"},{"location":"#License","page":"Home","title":"License","text":"This package is licensed under the MIT License.","category":"section"}]
}
