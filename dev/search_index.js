var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"This page documents the public API of LinearAlgebraMPI.jl.","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"","category":"section"},{"location":"api/#SparseMatrixMPI","page":"API Reference","title":"SparseMatrixMPI","text":"The main distributed sparse matrix type. Rows are partitioned across MPI ranks.\n\nFields:\n\nstructural_hash::NTuple{32,UInt8}: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries (length = nranks + 1)\ncol_partition::Vector{Int}: Column partition boundaries (length = nranks + 1)\ncol_indices::Vector{Int}: Column indices that appear in the local part\nAT::SparseMatrixCSC{T,Int}: Transpose of local rows","category":"section"},{"location":"api/#MatrixPlan","page":"API Reference","title":"MatrixPlan","text":"A communication plan for gathering rows from an SparseMatrixMPI for matrix multiplication.","category":"section"},{"location":"api/#TransposePlan","page":"API Reference","title":"TransposePlan","text":"A communication plan for computing the transpose of an SparseMatrixMPI.","category":"section"},{"location":"api/#VectorMPI","page":"API Reference","title":"VectorMPI","text":"A distributed vector type. Elements are partitioned across MPI ranks.\n\nFields:\n\nstructural_hash::NTuple{32,UInt8}: 256-bit Blake3 hash of the partition pattern\npartition::Vector{Int}: Element partition boundaries (length = nranks + 1)\nv::Vector{T}: Local elements owned by this rank","category":"section"},{"location":"api/#VectorPlan","page":"API Reference","title":"VectorPlan","text":"A communication plan for gathering vector elements for matrix-vector multiplication or vector alignment.","category":"section"},{"location":"api/#Constructors","page":"API Reference","title":"Constructors","text":"","category":"section"},{"location":"api/#SparseMatrixMPI-Constructor","page":"API Reference","title":"SparseMatrixMPI Constructor","text":"Create a distributed sparse matrix from a global sparse matrix.\n\nSignature:\n\nSparseMatrixMPI{T}(A::SparseMatrixCSC{T,Int}) where T\n\nArguments:\n\nA::SparseMatrixCSC{T,Int}: The global sparse matrix (must be identical on all ranks)\n\nReturns:\n\nSparseMatrixMPI{T}: The distributed matrix\n\nExample:\n\nusing SparseArrays\nA = sprand(100, 100, 0.01)\nAdist = SparseMatrixMPI{Float64}(A)","category":"section"},{"location":"api/#MatrixPlan-Constructors","page":"API Reference","title":"MatrixPlan Constructors","text":"Create communication plans for matrix operations.\n\nSignatures:\n\nMatrixPlan(row_indices::Vector{Int}, B::SparseMatrixMPI{T}) where T\nMatrixPlan(A::SparseMatrixMPI{T}, B::SparseMatrixMPI{T}) where T\n\nThe second form creates a memoized plan for A * B based on structural hashes.","category":"section"},{"location":"api/#TransposePlan-Constructor","page":"API Reference","title":"TransposePlan Constructor","text":"Create a communication plan for transpose.\n\nSignature:\n\nTransposePlan(A::SparseMatrixMPI{T}) where T","category":"section"},{"location":"api/#VectorMPI-Constructor","page":"API Reference","title":"VectorMPI Constructor","text":"Create a distributed vector from a global vector.\n\nSignature:\n\nVectorMPI(v_global::Vector{T}, comm::MPI.Comm=MPI.COMM_WORLD) where T\n\nArguments:\n\nv_global::Vector{T}: The global vector (must be identical on all ranks)\ncomm::MPI.Comm: MPI communicator (optional, defaults to COMM_WORLD)\n\nReturns:\n\nVectorMPI{T}: The distributed vector\n\nExample:\n\nv = collect(1.0:100.0)\nvdist = VectorMPI(v)","category":"section"},{"location":"api/#VectorPlan-Constructors","page":"API Reference","title":"VectorPlan Constructors","text":"Create communication plans for vector operations.\n\nSignatures:\n\nVectorPlan(A::SparseMatrixMPI{T}, x::VectorMPI{T}) where T\nVectorPlan(target_partition::Vector{Int}, source::VectorMPI{T}) where T\n\nThe first form creates a plan for gathering x[A.col_indices] for matrix-vector multiplication. The second form creates a plan for aligning a source vector to a target partition.","category":"section"},{"location":"api/#Executing-Plans","page":"API Reference","title":"Executing Plans","text":"Execute a pre-computed communication plan.","category":"section"},{"location":"api/#Matrix-Operations","page":"API Reference","title":"Matrix Operations","text":"","category":"section"},{"location":"api/#Multiplication","page":"API Reference","title":"Multiplication","text":"*(A::SparseMatrixMPI{T}, B::SparseMatrixMPI{T}) where T\n\nMultiply two distributed sparse matrices. Communication plans are automatically cached.\n\nExample:\n\nC = A * B","category":"section"},{"location":"api/#Addition","page":"API Reference","title":"Addition","text":"+(A::SparseMatrixMPI{T}, B::SparseMatrixMPI{T}) where T\n\nAdd two distributed sparse matrices. The result has A's row partition.\n\nExample:\n\nC = A + B","category":"section"},{"location":"api/#Subtraction","page":"API Reference","title":"Subtraction","text":"-(A::SparseMatrixMPI{T}, B::SparseMatrixMPI{T}) where T\n\nSubtract two distributed sparse matrices. The result has A's row partition.\n\nExample:\n\nC = A - B","category":"section"},{"location":"api/#Scalar-Multiplication","page":"API Reference","title":"Scalar Multiplication","text":"*(a::Number, A::SparseMatrixMPI{T}) where T\n*(A::SparseMatrixMPI{T}, a::Number) where T\n\nMultiply a distributed matrix by a scalar.\n\nExample:\n\nB = 2.0 * A\nB = A * 2.0  # Equivalent","category":"section"},{"location":"api/#Transpose","page":"API Reference","title":"Transpose","text":"Return a lazy transpose wrapper. The transpose is not computed until needed.\n\nSignature:\n\ntranspose(A::SparseMatrixMPI{T}) where T\n\nExample:\n\nAt = transpose(A)  # Lazy, no communication\n\nTo materialize:\n\nplan = TransposePlan(A)\nAt_materialized = execute_plan!(plan, A)","category":"section"},{"location":"api/#Conjugate","page":"API Reference","title":"Conjugate","text":"Return a new SparseMatrixMPI with conjugated values.\n\nSignature:\n\nconj(A::SparseMatrixMPI{T}) where T\n\nExample:\n\nAconj = conj(A)","category":"section"},{"location":"api/#Adjoint","page":"API Reference","title":"Adjoint","text":"Return the conjugate transpose (lazy).\n\nSignature:\n\nadjoint(A::SparseMatrixMPI{T}) where T\n\nExample:\n\nAadj = A'  # Equivalent to transpose(conj(A))","category":"section"},{"location":"api/#Lazy-Transpose-Operations","page":"API Reference","title":"Lazy Transpose Operations","text":"The following operations work with lazy transposes:\n\n# transpose(A) * transpose(B) = transpose(B * A)\ntranspose(A) * transpose(B)\n\n# transpose(A) * B - materializes transpose(A) first\ntranspose(A) * B\n\n# A * transpose(B) - materializes transpose(B) first\nA * transpose(B)\n\n# Scalar times lazy transpose\na * transpose(A)  # Returns transpose(a * A)\ntranspose(A) * a  # Returns transpose(a * A)","category":"section"},{"location":"api/#Vector-Operations","page":"API Reference","title":"Vector Operations","text":"","category":"section"},{"location":"api/#Matrix-Vector-Multiplication","page":"API Reference","title":"Matrix-Vector Multiplication","text":"*(A::SparseMatrixMPI{T}, x::VectorMPI{T}) where T\nmul!(y::VectorMPI{T}, A::SparseMatrixMPI{T}, x::VectorMPI{T}) where T\n\nMultiply a distributed sparse matrix by a distributed vector. The result vector has A.row_partition as its partition.\n\nExample:\n\ny = A * x\nmul!(y, A, x)  # In-place version","category":"section"},{"location":"api/#Vector-Matrix-Multiplication","page":"API Reference","title":"Vector-Matrix Multiplication","text":"*(vt::Transpose{<:Any, VectorMPI{T}}, A::SparseMatrixMPI{T}) where T\n\nMultiply a transposed vector by a matrix: transpose(v) * A = transpose(transpose(A) * v).\n\nExample:\n\nwt = transpose(v) * A","category":"section"},{"location":"api/#Vector-Addition-and-Subtraction","page":"API Reference","title":"Vector Addition and Subtraction","text":"+(u::VectorMPI{T}, v::VectorMPI{T}) where T\n-(u::VectorMPI{T}, v::VectorMPI{T}) where T\n-(v::VectorMPI{T}) where T\n\nAdd or subtract distributed vectors. If partitions differ, the second operand is automatically aligned to match the first operand's partition.\n\nExample:\n\nw = u + v  # Result has u's partition\nw = u - v\nw = -v","category":"section"},{"location":"api/#Vector-Scalar-Multiplication","page":"API Reference","title":"Vector Scalar Multiplication","text":"*(a::Number, v::VectorMPI{T}) where T\n*(v::VectorMPI{T}, a::Number) where T\n/(v::VectorMPI{T}, a::Number) where T\n\nMultiply or divide a distributed vector by a scalar.\n\nExample:\n\nw = 2.0 * v\nw = v * 2.0\nw = v / 2.0","category":"section"},{"location":"api/#Vector-Transpose-and-Adjoint","page":"API Reference","title":"Vector Transpose and Adjoint","text":"transpose(v::VectorMPI{T}) where T\nconj(v::VectorMPI{T}) where T\nadjoint(v::VectorMPI{T}) where T\n\nReturn transpose (lazy wrapper), conjugate (new VectorMPI), or adjoint (transpose of conjugate).\n\nExample:\n\nvt = transpose(v)  # Lazy\nvc = conj(v)       # New vector with conjugated values\nva = v'            # Equivalent to transpose(conj(v))","category":"section"},{"location":"api/#Vector-Norms","page":"API Reference","title":"Vector Norms","text":"norm(v::VectorMPI, p=2)\n\nCompute the p-norm of a distributed vector.\n\nArguments:\n\np=2 (default): Euclidean norm\np=1: Sum of absolute values\np=Inf: Maximum absolute value\nOther p: General p-norm","category":"section"},{"location":"api/#Vector-Reductions","page":"API Reference","title":"Vector Reductions","text":"sum(v::VectorMPI)\nprod(v::VectorMPI)\nmaximum(v::VectorMPI)\nminimum(v::VectorMPI)\n\nCompute reductions across all elements of a distributed vector.","category":"section"},{"location":"api/#Vector-Properties","page":"API Reference","title":"Vector Properties","text":"length(v::VectorMPI) -> Int\nsize(v::VectorMPI) -> (Int,)\neltype(v::VectorMPI{T}) -> T\n\nReturn the global length, size tuple, or element type.","category":"section"},{"location":"api/#Norms","page":"API Reference","title":"Norms","text":"","category":"section"},{"location":"api/#Element-wise-Norms","page":"API Reference","title":"Element-wise Norms","text":"Compute the p-norm of A treated as a vector of elements.\n\nSignature:\n\nnorm(A::SparseMatrixMPI, p=2)\n\nArguments:\n\np=2 (default): Frobenius norm\np=1: Sum of absolute values\np=Inf: Maximum absolute value\nOther p: General p-norm","category":"section"},{"location":"api/#Operator-Norms","page":"API Reference","title":"Operator Norms","text":"Compute the induced operator norm.\n\nSignature:\n\nopnorm(A::SparseMatrixMPI, p=1)\n\nArguments:\n\np=1: Maximum absolute column sum\np=Inf: Maximum absolute row sum\n\nNote: opnorm(A, 2) (spectral norm) is not implemented.","category":"section"},{"location":"api/#Matrix-Properties","page":"API Reference","title":"Matrix Properties","text":"","category":"section"},{"location":"api/#Size","page":"API Reference","title":"Size","text":"size(A::SparseMatrixMPI) -> (Int, Int)\nsize(A::SparseMatrixMPI, d::Integer) -> Int\n\nReturn the global size of the distributed matrix.","category":"section"},{"location":"api/#Element-Type","page":"API Reference","title":"Element Type","text":"eltype(A::SparseMatrixMPI{T}) -> T\n\nReturn the element type of the matrix.","category":"section"},{"location":"api/#Cache-Management","page":"API Reference","title":"Cache Management","text":"Clear all memoized plan caches.\n\nExample:\n\nclear_plan_cache!()","category":"section"},{"location":"api/#Dense-Matrix-Types","page":"API Reference","title":"Dense Matrix Types","text":"","category":"section"},{"location":"api/#MatrixMPI","page":"API Reference","title":"MatrixMPI","text":"A distributed dense matrix partitioned by rows across MPI ranks.\n\nFields:\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries (length = nranks + 1)\ncol_partition::Vector{Int}: Column partition boundaries (length = nranks + 1)\nA::Matrix{T}: Local rows (NOT transposed), size = (local_nrows, ncols)","category":"section"},{"location":"api/#DenseMatrixVectorPlan","page":"API Reference","title":"DenseMatrixVectorPlan","text":"A communication plan for gathering vector elements needed for MatrixMPI * x.","category":"section"},{"location":"api/#DenseTransposePlan","page":"API Reference","title":"DenseTransposePlan","text":"A communication plan for computing the transpose of a MatrixMPI.","category":"section"},{"location":"api/#Type-Aliases","page":"API Reference","title":"Type Aliases","text":"const TransposedSparseMatrixMPI{T} = Transpose{T, SparseMatrixMPI{T}}\nconst TransposedMatrixMPI{T} = Transpose{T, MatrixMPI{T}}\n\nType aliases for lazy transpose of SparseMatrixMPI and MatrixMPI.","category":"section"},{"location":"api/#Full-API-Index","page":"API Reference","title":"Full API Index","text":"","category":"section"},{"location":"api/#LinearAlgebraMPI.SparseMatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.SparseMatrixMPI","text":"SparseMatrixMPI{T}\n\nA distributed sparse matrix partitioned by rows across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (placeholder for transpose)\ncol_indices::Vector{Int}: Column indices that appear in the local part\nAT::SparseMatrixCSC{T,Int}: Transpose of local rows (columns in AT correspond to local rows)\n\nInvariants\n\ncol_indices, row_partition, and col_partition are sorted\nrow_partition[nranks+1] = total number of rows\ncol_partition[nranks+1] = total number of columns\nsize(AT, 2) == row_partition[rank+1] - row_partition[rank]\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.MatrixPlan","page":"API Reference","title":"LinearAlgebraMPI.MatrixPlan","text":"MatrixPlan{T}\n\nA communication plan for gathering rows from an SparseMatrixMPI.\n\nFields\n\nrank_ids::Vector{Int}: Ranks that requested data from us (0-indexed)\nsend_ranges::Vector{Vector{UnitRange{Int}}}: For each rank, ranges into B.AT.nzval to send\nsend_bufs::Vector{Vector{T}}: Pre-allocated send buffers for each rank\nsend_reqs::Vector{MPI.Request}: Pre-allocated send request handles\nrecv_rank_ids::Vector{Int}: Ranks we need to receive data from (0-indexed)\nrecv_bufs::Vector{Vector{T}}: Pre-allocated receive buffers for each rank\nrecv_reqs::Vector{MPI.Request}: Pre-allocated receive request handles\nrecv_offsets::Vector{Int}: Starting offsets into AT.nzval for each recvrankids\nlocal_ranges::Vector{Tuple{UnitRange{Int}, Int}}: (srcrange, dstoffset) for local copies\nAT::SparseMatrixCSC{T,Int}: Transposed matrix structure for gathered rows (values zeroed)\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.TransposePlan","page":"API Reference","title":"LinearAlgebraMPI.TransposePlan","text":"TransposePlan{T}\n\nA communication plan for computing the transpose of an SparseMatrixMPI.\n\nThe transpose of A (with rowpartition R and colpartition C) will have:\n\nrow_partition = C (columns of A become rows of A^T)\ncol_partition = R (rows of A become columns of A^T)\n\nFields\n\nrank_ids::Vector{Int}: Ranks we send data to (0-indexed)\nsend_indices::Vector{Vector{Int}}: For each rank, indices into A.AT.nzval to send\nsend_bufs::Vector{Vector{T}}: Pre-allocated send buffers\nsend_reqs::Vector{MPI.Request}: Pre-allocated send request handles\nrecv_rank_ids::Vector{Int}: Ranks we receive data from (0-indexed)\nrecv_bufs::Vector{Vector{T}}: Pre-allocated receive buffers\nrecv_reqs::Vector{MPI.Request}: Pre-allocated receive request handles\nrecv_perm::Vector{Vector{Int}}: For each recv rank, permutation into AT.nzval\nlocal_src_indices::Vector{Int}: Source indices for local copy\nlocal_dst_indices::Vector{Int}: Destination indices for local copy\nAT::SparseMatrixCSC{T,Int}: Transposed matrix structure (values zeroed)\nrow_partition::Vector{Int}: Row partition for the transposed matrix\ncol_partition::Vector{Int}: Col partition for the transposed matrix\ncol_indices::Vector{Int}: Column indices for the transposed matrix\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.VectorMPI","page":"API Reference","title":"LinearAlgebraMPI.VectorMPI","text":"VectorMPI{T}\n\nA distributed dense vector partitioned across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the partition\npartition::Vector{Int}: Partition boundaries, length = nranks + 1\nv::Vector{T}: Local vector elements owned by this rank\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.VectorPlan","page":"API Reference","title":"LinearAlgebraMPI.VectorPlan","text":"VectorPlan{T}\n\nA communication plan for gathering vector elements needed for A * x.\n\nFields\n\nsend_rank_ids::Vector{Int}: Ranks we send elements to (0-indexed)\nsend_indices::Vector{Vector{Int}}: For each rank, local indices to send\nsend_bufs::Vector{Vector{T}}: Pre-allocated send buffers\nsend_reqs::Vector{MPI.Request}: Pre-allocated send request handles\nrecv_rank_ids::Vector{Int}: Ranks we receive elements from (0-indexed)\nrecv_bufs::Vector{Vector{T}}: Pre-allocated receive buffers\nrecv_reqs::Vector{MPI.Request}: Pre-allocated receive request handles\nrecv_perm::Vector{Vector{Int}}: For each recv rank, indices into gathered\nlocal_src_indices::Vector{Int}: Source indices for local copy (into x.v)\nlocal_dst_indices::Vector{Int}: Destination indices for local copy (into gathered)\ngathered::Vector{T}: Pre-allocated buffer for gathered elements\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.execute_plan!","page":"API Reference","title":"LinearAlgebraMPI.execute_plan!","text":"execute_plan!(plan::VectorPlan{T}, x::VectorMPI{T}) where T\n\nExecute a vector communication plan to gather elements from x. Returns plan.gathered containing x[A.col_indices] for the associated matrix A.\n\n\n\n\n\nexecute_plan!(plan::DenseMatrixVectorPlan{T}, x::VectorMPI{T}) where T\n\nExecute a dense vector communication plan to gather elements from x. Returns plan.gathered containing x[1:ncols] for the associated matrix.\n\n\n\n\n\nexecute_plan!(plan::DenseTransposePlan{T}, A::MatrixMPI{T}) where T\n\nExecute a dense transpose plan to compute A^T. Returns a MatrixMPI representing the transpose.\n\n\n\n\n\nexecute_plan!(plan::DenseTransposeVectorPlan{T}, x::VectorMPI{T}) where T\n\nExecute the transpose vector plan to gather x elements.\n\n\n\n\n\nexecute_plan!(plan::MatrixPlan{T}, B::SparseMatrixMPI{T}) where T\n\nExecute a communication plan to gather rows from B into plan.AT. After execution, plan.AT contains the values from B for the requested rows. This function is allocation-free (all buffers are pre-allocated in the plan).\n\n\n\n\n\nexecute_plan!(plan::TransposePlan{T}, A::SparseMatrixMPI{T}) where T\n\nExecute a transpose plan to compute A^T. Returns an SparseMatrixMPI representing the transpose.\n\nNote: The returned matrix has its own copy of the sparse data, so the plan can be safely reused for subsequent transposes.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.clear_plan_cache!","page":"API Reference","title":"LinearAlgebraMPI.clear_plan_cache!","text":"clear_plan_cache!()\n\nClear all memoized plan caches.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.MatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.MatrixMPI","text":"MatrixMPI{T}\n\nA distributed dense matrix partitioned by rows across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (for transpose)\nA::Matrix{T}: Local rows (NOT transposed), size = (local_nrows, ncols)\n\nInvariants\n\nrow_partition and col_partition are sorted\nrow_partition[nranks+1] = total number of rows + 1\ncol_partition[nranks+1] = total number of columns + 1\nsize(A, 1) == row_partition[rank+2] - row_partition[rank+1]\nsize(A, 2) == col_partition[end] - 1\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.DenseMatrixVectorPlan","page":"API Reference","title":"LinearAlgebraMPI.DenseMatrixVectorPlan","text":"DenseMatrixVectorPlan{T}\n\nA communication plan for gathering vector elements needed for MatrixMPI * x.\n\nFor a dense matrix A with ncols columns, we need all elements x[1:ncols]. The plan gathers these elements from the distributed vector x based on x's partition.\n\nFields\n\nsend_rank_ids::Vector{Int}: Ranks we send elements to (0-indexed)\nsend_indices::Vector{Vector{Int}}: For each rank, local indices to send\nsend_bufs::Vector{Vector{T}}: Pre-allocated send buffers\nsend_reqs::Vector{MPI.Request}: Pre-allocated send request handles\nrecv_rank_ids::Vector{Int}: Ranks we receive elements from (0-indexed)\nrecv_bufs::Vector{Vector{T}}: Pre-allocated receive buffers\nrecv_reqs::Vector{MPI.Request}: Pre-allocated receive request handles\nrecv_perm::Vector{Vector{Int}}: For each recv rank, indices into gathered\nlocal_src_indices::Vector{Int}: Source indices for local copy (into x.v)\nlocal_dst_indices::Vector{Int}: Destination indices for local copy (into gathered)\ngathered::Vector{T}: Pre-allocated buffer for gathered elements (full x vector)\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.DenseTransposePlan","page":"API Reference","title":"LinearAlgebraMPI.DenseTransposePlan","text":"DenseTransposePlan{T}\n\nA communication plan for computing the transpose of a MatrixMPI.\n\nThe transpose of A (with rowpartition R and colpartition C) will have:\n\nrow_partition = C (columns of A become rows of A^T)\ncol_partition = R (rows of A become columns of A^T)\n\nFields\n\nrank_ids::Vector{Int}: Ranks we send data to (0-indexed)\nsend_row_ranges::Vector{UnitRange{Int}}: For each rank, local row range to send\nsend_col_ranges::Vector{UnitRange{Int}}: For each rank, column range to send\nsend_bufs::Vector{Vector{T}}: Pre-allocated send buffers\nsend_reqs::Vector{MPI.Request}: Pre-allocated send request handles\nrecv_rank_ids::Vector{Int}: Ranks we receive data from (0-indexed)\nrecv_row_ranges::Vector{UnitRange{Int}}: For each recv rank, row range in result\nrecv_col_ranges::Vector{UnitRange{Int}}: For each recv rank, column range in result\nrecv_bufs::Vector{Vector{T}}: Pre-allocated receive buffers\nrecv_reqs::Vector{MPI.Request}: Pre-allocated receive request handles\nlocal_row_range::UnitRange{Int}: Local rows that stay on this rank\nlocal_col_range::UnitRange{Int}: Local columns that stay on this rank\nAT::Matrix{T}: Pre-allocated result matrix\nrow_partition::Vector{Int}: Row partition for the transposed matrix\ncol_partition::Vector{Int}: Col partition for the transposed matrix\n\n\n\n\n\n","category":"type"},{"location":"getting-started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This guide will walk you through the basics of using LinearAlgebraMPI.jl for distributed sparse matrix computations.","category":"section"},{"location":"getting-started/#Prerequisites","page":"Getting Started","title":"Prerequisites","text":"Before using LinearAlgebraMPI.jl, ensure you have:\n\nA working MPI installation (OpenMPI, MPICH, or Intel MPI)\nMPI.jl configured to use your MPI installation\n\nYou can verify your MPI setup with:\n\nusing MPI\nMPI.Init()\nprintln(\"Rank $(MPI.Comm_rank(MPI.COMM_WORLD)) of $(MPI.Comm_size(MPI.COMM_WORLD))\")\nMPI.Finalize()\n\nRun with:\n\nmpiexec -n 4 julia --project=. your_script.jl","category":"section"},{"location":"getting-started/#Creating-Distributed-Matrices","page":"Getting Started","title":"Creating Distributed Matrices","text":"","category":"section"},{"location":"getting-started/#From-a-Global-Sparse-Matrix","page":"Getting Started","title":"From a Global Sparse Matrix","text":"The most common way to create a distributed matrix is from an existing SparseMatrixCSC:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create a sparse matrix - MUST be identical on all ranks\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\n\n# Distribute across MPI ranks\nAdist = SparseMatrixMPI{Float64}(A)\n\nImportant: All MPI ranks must have identical copies of the input matrix when constructing SparseMatrixMPI. The matrix is then automatically partitioned by rows across ranks.","category":"section"},{"location":"getting-started/#Understanding-Row-Partitioning","page":"Getting Started","title":"Understanding Row Partitioning","text":"The matrix is partitioned roughly equally by rows. For example, with 4 ranks and a 100x100 matrix:\n\nRank 0: rows 1-25\nRank 1: rows 26-50\nRank 2: rows 51-75\nRank 3: rows 76-100\n\nYou can inspect the partition:\n\nprintln(\"Row partition: \", Adist.row_partition)\n# Output: [1, 26, 51, 76, 101]","category":"section"},{"location":"getting-started/#Basic-Operations","page":"Getting Started","title":"Basic Operations","text":"","category":"section"},{"location":"getting-started/#Matrix-Multiplication","page":"Getting Started","title":"Matrix Multiplication","text":"# Both matrices must be distributed\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Multiply\nCdist = Adist * Bdist\n\nThe multiplication automatically handles the necessary communication between ranks.","category":"section"},{"location":"getting-started/#Addition-and-Subtraction","page":"Getting Started","title":"Addition and Subtraction","text":"Cdist = Adist + Bdist\nDdist = Adist - Bdist\n\nIf A and B have different row partitions, B's rows are redistributed to match A's partition.","category":"section"},{"location":"getting-started/#Scalar-Multiplication","page":"Getting Started","title":"Scalar Multiplication","text":"Cdist = 2.5 * Adist\nCdist = Adist * 2.5  # Equivalent","category":"section"},{"location":"getting-started/#Transpose","page":"Getting Started","title":"Transpose","text":"LinearAlgebraMPI.jl supports both lazy and eager transpose:\n\n# Lazy transpose (no communication)\nAt = transpose(Adist)\n\n# Eager transpose (materializes the transposed matrix)\nplan = TransposePlan(Adist)\nAt_materialized = execute_plan!(plan, Adist)\n\nLazy transposes are automatically materialized when needed in operations.","category":"section"},{"location":"getting-started/#Adjoint-(Conjugate-Transpose)","page":"Getting Started","title":"Adjoint (Conjugate Transpose)","text":"For complex matrices:\n\nAdist = SparseMatrixMPI{ComplexF64}(A)\nAadj = Adist'  # Conjugate transpose (lazy)","category":"section"},{"location":"getting-started/#Computing-Norms","page":"Getting Started","title":"Computing Norms","text":"# Frobenius norm (default)\nf_norm = norm(Adist)\n\n# 1-norm (sum of absolute values)\none_norm = norm(Adist, 1)\n\n# Infinity norm (maximum absolute value)\ninf_norm = norm(Adist, Inf)\n\n# General p-norm\np_norm = norm(Adist, 3)\n\n# Operator norms\ncol_norm = opnorm(Adist, 1)   # Max column sum\nrow_norm = opnorm(Adist, Inf) # Max row sum","category":"section"},{"location":"getting-started/#Running-MPI-Programs","page":"Getting Started","title":"Running MPI Programs","text":"","category":"section"},{"location":"getting-started/#Command-Line","page":"Getting Started","title":"Command Line","text":"mpiexec -n 4 julia --project=. my_program.jl","category":"section"},{"location":"getting-started/#Program-Structure","page":"Getting Started","title":"Program Structure","text":"A typical LinearAlgebraMPI.jl program follows this pattern:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Create matrices (identical on all ranks)\nA = create_my_matrix()  # Your matrix creation function\nB = create_my_matrix()\n\n# Distribute\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Compute\nCdist = Adist * Bdist\n\n# Get results (e.g., norm is computed globally)\nresult_norm = norm(Cdist)\n\nif rank == 0\n    println(\"Result norm: $result_norm\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"getting-started/#Performance-Tips","page":"Getting Started","title":"Performance Tips","text":"","category":"section"},{"location":"getting-started/#Reuse-Communication-Plans","page":"Getting Started","title":"Reuse Communication Plans","text":"For repeated operations with the same sparsity pattern, LinearAlgebraMPI.jl automatically caches communication plans:\n\n# First multiplication creates and caches the plan\nC1 = Adist * Bdist\n\n# Subsequent multiplications with same A, B reuse the cached plan\nC2 = Adist * Bdist  # Uses cached plan - much faster","category":"section"},{"location":"getting-started/#Clear-Cache-When-Done","page":"Getting Started","title":"Clear Cache When Done","text":"If you're done with a set of matrices and want to free memory:\n\nclear_plan_cache!()","category":"section"},{"location":"getting-started/#Use-Deterministic-Test-Data","page":"Getting Started","title":"Use Deterministic Test Data","text":"For testing, avoid random matrices since they'll differ across ranks:\n\n# Bad - different on each rank\nA = sprand(100, 100, 0.01)\n\n# Good - deterministic, identical on all ranks\nI = [1:100; 1:99; 2:100]\nJ = [1:100; 2:100; 1:99]\nV = [2.0*ones(100); -0.5*ones(99); -0.5*ones(99)]\nA = sparse(I, J, V, 100, 100)","category":"section"},{"location":"getting-started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"See Examples for more detailed usage examples\nRead the API Reference for complete function documentation\nUnderstand the Internals for implementation details","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page provides detailed examples of using LinearAlgebraMPI.jl for various distributed sparse matrix operations.","category":"section"},{"location":"examples/#Matrix-Multiplication","page":"Examples","title":"Matrix Multiplication","text":"","category":"section"},{"location":"examples/#Square-Matrices","page":"Examples","title":"Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\n\n# Create a tridiagonal matrix (same on all ranks)\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Create another tridiagonal matrix\nV2 = [1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]\nB = sparse(I, J, V2, n, n)\n\n# Distribute matrices\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Multiply\nCdist = Adist * Bdist\n\n# Verify against reference\nC_ref = A * B\nC_ref_dist = SparseMatrixMPI{Float64}(C_ref)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nif rank == 0\n    println(\"Multiplication error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Non-Square-Matrices","page":"Examples","title":"Non-Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# A is 6x8, B is 8x10, result is 6x10\nm, k, n = 6, 8, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nJ_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 2]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, k)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, k, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\nCdist = Adist * Bdist\n\n@assert size(Cdist) == (m, n)\n\nif rank == 0\n    println(\"Result size: $(size(Cdist))\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Complex-Matrices","page":"Examples","title":"Complex Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 8\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# Complex values\nV_A = ComplexF64.([2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]) .+\n      im .* ComplexF64.([0.1*ones(n); 0.2*ones(n-1); -0.2*ones(n-1)])\nA = sparse(I, J, V_A, n, n)\n\nV_B = ComplexF64.([1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]) .+\n      im .* ComplexF64.([-0.1*ones(n); 0.1*ones(n-1); 0.1*ones(n-1)])\nB = sparse(I, J, V_B, n, n)\n\nAdist = SparseMatrixMPI{ComplexF64}(A)\nBdist = SparseMatrixMPI{ComplexF64}(B)\n\n# Multiplication\nCdist = Adist * Bdist\n\n# Conjugate\nAconj = conj(Adist)\n\n# Adjoint (conjugate transpose) - returns lazy wrapper\nAadj = Adist'\n\n# Materialize adjoint\nplan = TransposePlan(Aadj.parent)\nAadj_mat = execute_plan!(plan, Aadj.parent)\n\nif rank == 0\n    println(\"Complex matrix operations completed\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Addition-and-Subtraction","page":"Examples","title":"Addition and Subtraction","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 8\n\n# Matrices with different sparsity patterns\n# Matrix A: upper triangular entries\nI_A = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nJ_A = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nV_A = Float64.(1:9)\nA = sparse(I_A, J_A, V_A, n, n)\n\n# Matrix B: lower triangular entries\nI_B = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nJ_B = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nV_B = Float64.(9:-1:1)\nB = sparse(I_B, J_B, V_B, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Addition - handles different sparsity patterns\nCdist = Adist + Bdist\n\n# Subtraction\nDdist = Adist - Bdist\n\n# Verify\nC_ref_dist = SparseMatrixMPI{Float64}(A + B)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nif rank == 0\n    println(\"Addition error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Transpose-Operations","page":"Examples","title":"Transpose Operations","text":"","category":"section"},{"location":"examples/#Lazy-Transpose","page":"Examples","title":"Lazy Transpose","text":"The lazy transpose creates a wrapper without actually transposing the data:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 8, 6\nI_C = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_C = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nV_C = Float64.(1:length(I_C))\nC = sparse(I_C, J_C, V_C, m, n)\n\nI_D = [1, 2, 3, 4, 5, 6, 1, 2]\nJ_D = [1, 2, 3, 4, 5, 6, 7, 8]\nV_D = Float64.(1:length(I_D))\nD = sparse(I_D, J_D, V_D, n, m)\n\nCdist = SparseMatrixMPI{Float64}(C)\nDdist = SparseMatrixMPI{Float64}(D)\n\n# transpose(C) * transpose(D) = transpose(D * C)\n# This is computed lazily\nresult_lazy = transpose(Cdist) * transpose(Ddist)\n\n# The result is wrapped in Transpose\n# Materialize to get the actual matrix\nplan = TransposePlan(result_lazy.parent)\nresult_dist = execute_plan!(plan, result_lazy.parent)\n\n# Verify\nref = sparse((D * C)')\nref_dist = SparseMatrixMPI{Float64}(ref)\nerr = norm(result_dist - ref_dist, Inf)\n\nif rank == 0\n    println(\"Lazy transpose multiplication error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Eager-Transpose","page":"Examples","title":"Eager Transpose","text":"When you need the transposed data immediately:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 8, 6\nI = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Create transpose plan\nplan = TransposePlan(Adist)\n\n# Execute to get transposed matrix\nAt_dist = execute_plan!(plan, Adist)\n\n# At_dist is now an SparseMatrixMPI representing A^T\n@assert size(At_dist) == (n, m)\n\nif rank == 0\n    println(\"Transpose size: $(size(At_dist))\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Mixed-Transpose-Multiplication","page":"Examples","title":"Mixed Transpose Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# A is 8x6, so A' is 6x8\n# B is 8x10, so A' * B is 6x10\nm, n, p = 8, 6, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, n)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, m, p)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# transpose(A) * B - A is automatically materialized as transpose\nresult_dist = transpose(Adist) * Bdist\n\n# Verify\nref = sparse(A') * B\nref_dist = SparseMatrixMPI{Float64}(ref)\nerr = norm(result_dist - ref_dist, Inf)\n\nif rank == 0\n    println(\"transpose(A) * B error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Scalar-Multiplication","page":"Examples","title":"Scalar Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3]\nJ = [1, 2, 3, 4, 5, 6, 7, 8]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Scalar times matrix\na = 2.5\nresult1 = a * Adist\nresult2 = Adist * a  # Equivalent\n\n# Scalar times lazy transpose\nAt = transpose(Adist)\nresult3 = a * At  # Returns transpose(a * A)\n\n# Verify\nref_dist = SparseMatrixMPI{Float64}(a * A)\nerr1 = norm(result1 - ref_dist, Inf)\nerr2 = norm(result2 - ref_dist, Inf)\n\nif rank == 0\n    println(\"Scalar multiplication errors: $err1, $err2\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Computing-Norms","page":"Examples","title":"Computing Norms","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3, 2, 4]\nJ = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Element-wise norms (treating matrix as vector)\nfrob_norm = norm(Adist)        # Frobenius (2-norm)\none_norm = norm(Adist, 1)      # Sum of absolute values\ninf_norm = norm(Adist, Inf)    # Max absolute value\np_norm = norm(Adist, 3)        # General p-norm\n\n# Operator norms\nop_1 = opnorm(Adist, 1)        # Max absolute column sum\nop_inf = opnorm(Adist, Inf)    # Max absolute row sum\n\nif rank == 0\n    println(\"Frobenius norm: $frob_norm\")\n    println(\"1-norm: $one_norm\")\n    println(\"Inf-norm: $inf_norm\")\n    println(\"3-norm: $p_norm\")\n    println(\"Operator 1-norm: $op_1\")\n    println(\"Operator Inf-norm: $op_inf\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Iterative-Methods-Example","page":"Examples","title":"Iterative Methods Example","text":"Here's an example of using LinearAlgebraMPI.jl for power iteration to find the dominant eigenvalue:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\n\n# Create a symmetric positive definite matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# For power iteration, we need matrix-vector products\n# Currently LinearAlgebraMPI focuses on matrix-matrix products\n# This example shows how to use A*A for related computations\n\n# Compute A^2\nA2dist = Adist * Adist\n\n# Compute the Frobenius norm of A^2\nnorm_A2 = norm(A2dist)\n\nif rank == 0\n    println(\"||A^2||_F = $norm_A2\")\n    # For SPD matrices, this relates to the eigenvalues\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Plan-Caching-and-Management","page":"Examples","title":"Plan Caching and Management","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\nB = spdiagm(0 => 1.5*ones(n), 1 => 0.5*ones(n-1), -1 => 0.5*ones(n-1))\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# First multiplication - creates and caches the plan\nC1 = Adist * Bdist\n\n# Second multiplication - reuses cached plan (faster)\nC2 = Adist * Bdist\n\n# Third multiplication - still uses cached plan\nC3 = Adist * Bdist\n\n# Clear caches when done to free memory\nclear_plan_cache!()\n\nif rank == 0\n    println(\"Cached multiplication completed\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"#LinearAlgebraMPI.jl","page":"Home","title":"LinearAlgebraMPI.jl","text":"Distributed sparse matrix operations using MPI for parallel computing across multiple ranks.\n\nLinearAlgebraMPI.jl provides a high-performance implementation of distributed sparse matrices in Julia, enabling parallel sparse linear algebra operations across multiple MPI processes. The package is designed for large-scale computations where matrices are too large to fit on a single node or where parallel speedup is desired.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Row-partitioned sparse matrices: Matrices are distributed by rows across MPI ranks\nMatrix multiplication: Efficient sparse matrix-matrix product with memoized communication plans\nAddition and subtraction: Element-wise operations with automatic data redistribution\nTranspose operations: Both eager and lazy transpose support\nConjugate and adjoint: Full support for complex matrices\nScalar multiplication: Efficient scalar-matrix products\nNorms: Frobenius norm, 1-norm, infinity norm, and general p-norms\nOperator norms: 1-norm and infinity-norm of operators\nType stability: Generic implementation supporting Float64, ComplexF64, and other numeric types\nPlan caching: Communication plans are memoized for repeated operations with the same sparsity pattern","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create a sparse matrix (must be identical on all ranks)\nA = sprand(1000, 1000, 0.01)\nB = sprand(1000, 1000, 0.01)\n\n# Distribute matrices across MPI ranks\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Perform distributed operations\nC = Adist * Bdist    # Matrix multiplication\nD = Adist + Bdist    # Addition\nE = Adist - Bdist    # Subtraction\nF = 2.0 * Adist      # Scalar multiplication\n\n# Compute norms\nfrobenius_norm = norm(Adist)\nmax_col_sum = opnorm(Adist, 1)\n\nMPI.Finalize()","category":"section"},{"location":"#Package-Overview","page":"Home","title":"Package Overview","text":"Pages = [\"getting-started.md\", \"examples.md\", \"api.md\", \"internals.md\"]\nDepth = 2","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"using Pkg\nPkg.add(url=\"https://github.com/your-username/LinearAlgebraMPI.jl\")\n\nOr in the Julia REPL package mode:\n\npkg> add https://github.com/your-username/LinearAlgebraMPI.jl","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.10+\nMPI.jl with a working MPI implementation\nSparseArrays.jl\nLinearAlgebra.jl\nBlake3Hash.jl","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MIT License","category":"section"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"internals/#Internals","page":"Internals","title":"Internals","text":"This page describes the internal architecture and implementation details of LinearAlgebraMPI.jl. Understanding these details can help with debugging, performance optimization, and extending the library.","category":"section"},{"location":"internals/#Data-Structures","page":"Internals","title":"Data Structures","text":"","category":"section"},{"location":"internals/#SparseMatrixMPI-Storage","page":"Internals","title":"SparseMatrixMPI Storage","text":"SparseMatrixMPI{T} stores a distributed sparse matrix with the following key design decisions:\n\nRow partitioning: Rows are distributed roughly equally across MPI ranks\nTransposed storage: Local rows are stored as the transpose (AT::SparseMatrixCSC)\nGlobal indices: Column indices in AT.rowval are global (not local) indices","category":"section"},{"location":"internals/#Why-Store-the-Transpose?","page":"Internals","title":"Why Store the Transpose?","text":"Storing the transpose (columns of AT = rows of A) has several advantages:\n\nEfficient row access: CSC format provides O(1) access to column slices. Since we store AT, we get O(1) access to rows of A.\nNatural for multiplication: In A * B, we need to iterate over rows of A and columns of B. With AT stored, iterating over rows of A is efficient.\nSimplified communication: When gathering rows from B for multiplication, we can directly send column slices of B.AT.","category":"section"},{"location":"internals/#Field-Descriptions","page":"Internals","title":"Field Descriptions","text":"struct SparseMatrixMPI{T}\n    structural_hash::NTuple{32,UInt8}  # Blake3 hash for plan caching\n    row_partition::Vector{Int}          # [1, r1_end+1, r2_end+1, ..., nrows+1]\n    col_partition::Vector{Int}          # Similar for columns (for transpose)\n    col_indices::Vector{Int}            # Unique sorted column indices in local part\n    AT::SparseMatrixCSC{T,Int}          # Transposed local rows\nend\n\nInvariants:\n\nrow_partition[1] = 1\nrow_partition[end] = nrows + 1\nsize(AT, 2) = row_partition[rank+2] - row_partition[rank+1] (local row count)\nAT.rowval contains global column indices","category":"section"},{"location":"internals/#Structural-Hashing","page":"Internals","title":"Structural Hashing","text":"Each SparseMatrixMPI has a 256-bit Blake3 hash of its structure. This hash is:\n\nComputed collectively: Uses Allgather to ensure identical hash on all ranks\nStructure-only: Includes partition, indices, colptr, rowval (not values)\nUsed for plan caching: Same structure = same communication pattern\n\nThe hash computation:\n\nEach rank hashes its local data (rowpartition, colindices, AT.colptr, AT.rowval)\nAll local hashes are gathered via MPI.Allgather\nThe gathered hashes are hashed together to produce the global hash","category":"section"},{"location":"internals/#Matrix-Multiplication-Algorithm","page":"Internals","title":"Matrix Multiplication Algorithm","text":"","category":"section"},{"location":"internals/#Overview","page":"Internals","title":"Overview","text":"Computing C = A * B requires three phases:\n\nPlan creation: Determine which rows of B are needed and exchange sparsity structure\nValue exchange: Send/receive the actual matrix values\nLocal computation: Multiply the gathered data locally","category":"section"},{"location":"internals/#Phase-1:-Plan-Creation","page":"Internals","title":"Phase 1: Plan Creation","text":"When creating a MatrixPlan(A, B):\n\nIdentify needed rows: A.col_indices tells us which columns of A (= rows of B) we need\nDetermine owners: For each needed row, find which rank owns it using B.row_partition\nExchange requests: Use Alltoall to exchange counts, then point-to-point for row lists\nExchange structure: Send colptr and rowval for requested rows\nBuild gathered structure: Construct plan.AT with zeros (sparsity pattern only)\nSetup buffers: Pre-allocate send/receive buffers and track offsets","category":"section"},{"location":"internals/#Phase-2:-Value-Exchange-(execute_plan!)","page":"Internals","title":"Phase 2: Value Exchange (execute_plan!)","text":"Local copy: Copy values for locally-owned rows directly into plan.AT.nzval\nPack and send: For each remote rank, pack requested values into buffer and Isend\nReceive: Irecv values from ranks we requested data from\nUnpack: Copy received values into plan.AT.nzval at correct offsets","category":"section"},{"location":"internals/#Phase-3:-Local-Computation","page":"Internals","title":"Phase 3: Local Computation","text":"# C^T = B^T * A^T = plan.AT * A.AT_reindexed\nresult_AT = plan.AT * A_AT_reindexed\n\nThe key insight is that we need to reindex A.AT.rowval from global indices to local indices (1:n_gathered) since plan.AT has only the gathered rows, not all of B.","category":"section"},{"location":"internals/#Plan-Caching","page":"Internals","title":"Plan Caching","text":"Plans are cached in a global dictionary keyed by (A.structural_hash, B.structural_hash, T). This means:\n\nRepeated multiplications with the same structure reuse plans\nThe structure can be different values (plans only depend on sparsity)\nPlans must be cleared manually with clear_plan_cache!() when done","category":"section"},{"location":"internals/#Transpose-Algorithm","page":"Internals","title":"Transpose Algorithm","text":"","category":"section"},{"location":"internals/#TransposePlan-Creation","page":"Internals","title":"TransposePlan Creation","text":"The transpose of A (with rowpartition R and colpartition C) has:\n\nrow_partition = C (columns become rows)\ncol_partition = R (rows become columns)\n\nAlgorithm:\n\nCategorize nonzeros: For each A[i,j], determine which rank owns row j in A^T\nExchange structure: Send (row, col) pairs to destination ranks\nBuild sparse structure: Construct CSC format for the transposed matrix\nSetup communication: Track permutations for scattering received values","category":"section"},{"location":"internals/#execute_plan!-for-Transpose","page":"Internals","title":"execute_plan! for Transpose","text":"Copy local values (entries that stay on the same rank)\nPack and send values to destination ranks\nReceive values and scatter into result using pre-computed permutation","category":"section"},{"location":"internals/#Addition/Subtraction","page":"Internals","title":"Addition/Subtraction","text":"For A + B or A - B:\n\nGet addition plan: Gather B's rows to match A's partition\nExecute plan: Redistribute B's values\nLocal operation: Use SparseArrays' built-in A.AT + plan.AT or A.AT - plan.AT\n\nThe result inherits A's partition.","category":"section"},{"location":"internals/#Lazy-Transpose","page":"Internals","title":"Lazy Transpose","text":"Lazy transpose uses Julia's LinearAlgebra.Transpose wrapper:\n\ntranspose(A::SparseMatrixMPI{T}) = Transpose(A)\n\nOperations with lazy transposes are handled by specialized methods:\n\ntranspose(A) * transpose(B) returns transpose(B * A) (computed lazily)\ntranspose(A) * B materializes A^T first, then multiplies\nA * transpose(B) materializes B^T first, then multiplies\na * transpose(A) returns transpose(a * A)","category":"section"},{"location":"internals/#Communication-Patterns","page":"Internals","title":"Communication Patterns","text":"","category":"section"},{"location":"internals/#Alltoall-Pattern","page":"Internals","title":"Alltoall Pattern","text":"Used for exchanging counts and sizes:\n\nrecv_counts = MPI.Alltoall(MPI.UBuffer(send_counts, 1), comm)","category":"section"},{"location":"internals/#Point-to-Point-Pattern","page":"Internals","title":"Point-to-Point Pattern","text":"Used for variable-size data:\n\n# Non-blocking send\nreq = MPI.Isend(msg, comm; dest=r, tag=1)\n\n# Non-blocking receive\nreq = MPI.Irecv!(buf, comm; source=r, tag=1)\n\n# Wait for completion\nMPI.Waitall(reqs)","category":"section"},{"location":"internals/#Collective-Reduction","page":"Internals","title":"Collective Reduction","text":"Used for norms:\n\nglobal_sum = MPI.Allreduce(local_sum, MPI.SUM, comm)\nglobal_max = MPI.Allreduce(local_max, MPI.MAX, comm)","category":"section"},{"location":"internals/#Memory-Management","page":"Internals","title":"Memory Management","text":"","category":"section"},{"location":"internals/#Pre-allocated-Buffers","page":"Internals","title":"Pre-allocated Buffers","text":"Plans pre-allocate all buffers during construction:\n\nsend_bufs: One per destination rank\nrecv_bufs: One per source rank\nplan.AT.nzval: Output values array\n\nThis makes execute_plan! allocation-free for the communication portions.","category":"section"},{"location":"internals/#Plan-Reuse","page":"Internals","title":"Plan Reuse","text":"Plans store all communication metadata:\n\nWhich ranks to communicate with\nBuffer sizes and offsets\nValue permutations\n\nReusing plans avoids the expensive setup phase.","category":"section"},{"location":"internals/#Testing-Considerations","page":"Internals","title":"Testing Considerations","text":"","category":"section"},{"location":"internals/#Deterministic-Test-Data","page":"Internals","title":"Deterministic Test Data","text":"Tests must use deterministic data to ensure all ranks have identical input:\n\n# Bad: different on each rank\nA = sprand(100, 100, 0.01)\n\n# Good: deterministic\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]\nA = sparse(I, J, V, n, n)","category":"section"},{"location":"internals/#Test-Harness","page":"Internals","title":"Test Harness","text":"Tests run under mpiexec via the test harness:\n\nmpiexec -n 4 julia --project=. test/test_*.jl\n\nThe runtests.jl file automates this using run_mpi_test().","category":"section"},{"location":"internals/#Performance-Considerations","page":"Internals","title":"Performance Considerations","text":"","category":"section"},{"location":"internals/#When-to-Clear-Cache","page":"Internals","title":"When to Clear Cache","text":"Clear the plan cache when:\n\nYou're done with a set of matrices\nMemory usage is a concern\nYou're about to work with different matrices\n\nclear_plan_cache!()","category":"section"},{"location":"internals/#Optimal-Matrix-Sizes","page":"Internals","title":"Optimal Matrix Sizes","text":"Too small: Communication overhead dominates\nToo large per rank: Memory limits\nSweet spot: Large enough matrices with good sparsity distribution","category":"section"},{"location":"internals/#Sparsity-Pattern-Impact","page":"Internals","title":"Sparsity Pattern Impact","text":"Banded matrices: Good locality, less communication\nRandom sparse: More communication, but still efficient\nBlock diagonal: Excellent if blocks align with partitions","category":"section"},{"location":"internals/#Extending-the-Library","page":"Internals","title":"Extending the Library","text":"","category":"section"},{"location":"internals/#Adding-New-Operations","page":"Internals","title":"Adding New Operations","text":"To add a new operation:\n\nDetermine the communication pattern\nCreate a plan type if needed\nImplement the plan constructor\nImplement execute_plan!\nImplement the high-level operation (e.g., Base.:*(...))\nAdd caching if the operation will be repeated","category":"section"},{"location":"internals/#Supporting-New-Element-Types","page":"Internals","title":"Supporting New Element Types","text":"The library is generic over element type T. To support a new type:\n\nEnsure it works with SparseArrays\nEnsure MPI can serialize it (or provide custom serialization)\nEnsure Blake3Hash can hash it","category":"section"},{"location":"internals/#Debugging-Tips","page":"Internals","title":"Debugging Tips","text":"","category":"section"},{"location":"internals/#Verify-Identical-Input","page":"Internals","title":"Verify Identical Input","text":"A_hash = compute_structural_hash(...)\n# Hash should be identical on all ranks","category":"section"},{"location":"internals/#Check-Partitions","page":"Internals","title":"Check Partitions","text":"println(\"Rank $rank: rows $(A.row_partition[rank+1]) to $(A.row_partition[rank+2]-1)\")","category":"section"},{"location":"internals/#Trace-Communication","page":"Internals","title":"Trace Communication","text":"for r in plan.rank_ids\n    println(\"Rank $rank sends to $r: $(length(plan.send_bufs[i])) values\")\nend","category":"section"}]
}
