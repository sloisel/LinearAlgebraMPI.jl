var documenterSearchIndex = {"docs":
[{"location":"guide/#User-Guide","page":"User Guide","title":"User Guide","text":"This guide covers the essential workflows for using HPCLinearAlgebra.jl.","category":"section"},{"location":"guide/#Core-Types","page":"User Guide","title":"Core Types","text":"HPCLinearAlgebra provides three distributed types:\n\nType Description Storage\nHPCVector{T,B} Distributed vector Row-partitioned\nHPCMatrix{T,B} Distributed dense matrix Row-partitioned\nHPCSparseMatrix{T,Ti,B} Distributed sparse matrix Row-partitioned CSR\n\nThe type parameters are:\n\nT: Element type (Float64, Float32, ComplexF64, etc.)\nB<:HPCBackend: Backend configuration (device, communication, solver)\nTi: Index type for sparse matrices (typically Int)","category":"section"},{"location":"guide/#Backends","page":"User Guide","title":"Backends","text":"The HPCBackend{Device, Comm, Solver} type encapsulates three concerns:\n\nDevice: Where data lives (DeviceCPU, DeviceMetal, DeviceCUDA)\nComm: How ranks communicate (CommSerial, CommMPI)\nSolver: Sparse direct solver (SolverMUMPS, cuDSS variants)\n\nPre-constructed backends for common use cases:\n\nBACKEND_CPU_SERIAL: CPU with no MPI (single-process)\nBACKEND_CPU_MPI: CPU with MPI communication (most common)\n\nGPU backends are created via factory functions after loading the GPU package:\n\nbackend_metal_mpi(comm): Metal GPU with MPI\nbackend_cuda_mpi(comm): CUDA GPU with MPI\n\nAll types are row-partitioned across MPI ranks, meaning each rank owns a contiguous range of rows.","category":"section"},{"location":"guide/#Internal-Storage:-CSR-Format","page":"User Guide","title":"Internal Storage: CSR Format","text":"Internally, HPCSparseMatrix stores local rows in CSR (Compressed Sparse Row) format using the SparseMatrixCSR type. This enables efficient row-wise iteration for a row-partitioned distributed matrix.\n\nIn Julia, SparseMatrixCSR{T,Ti} is a type alias for Transpose{T, SparseMatrixCSC{T,Ti}}. You don't need to worry about this for normal usage - it's handled automatically.","category":"section"},{"location":"guide/#Creating-Distributed-Types","page":"User Guide","title":"Creating Distributed Types","text":"","category":"section"},{"location":"guide/#From-Native-Julia-Types","page":"User Guide","title":"From Native Julia Types","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\n\n# Use the default MPI backend\nbackend = BACKEND_CPU_MPI\n\n# Create from native types (data is distributed automatically)\nv = HPCVector(randn(100), backend)\nA = HPCMatrix(randn(50, 30), backend)\nS = HPCSparseMatrix(sprandn(100, 100, 0.1), backend)","category":"section"},{"location":"guide/#Local-Constructors","page":"User Guide","title":"Local Constructors","text":"For performance-critical code, use local constructors that avoid global communication:\n\n# Create from local data (each rank provides its own rows)\nv_local = HPCVector_local(my_local_vector)\nA_local = HPCMatrix_local(my_local_matrix)\nS_local = HPCSparseMatrix_local(my_local_sparse)","category":"section"},{"location":"guide/#Efficient-Local-Only-Construction","page":"User Guide","title":"Efficient Local-Only Construction","text":"For large matrices, avoid replicating data across all ranks by only populating each rank's local portion:\n\nbackend = BACKEND_CPU_MPI\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Global dimensions\nm, n = 1000, 1000\n\n# Compute which rows this rank owns\nrows_per_rank = div(m, nranks)\nremainder = mod(m, nranks)\nmy_row_start = 1 + rank * rows_per_rank + min(rank, remainder)\nmy_row_end = my_row_start + rows_per_rank - 1 + (rank < remainder ? 1 : 0)\n\n# Create sparse matrix with correct size, but only populate local rows\nI, J, V = Int[], Int[], Float64[]\nfor i in my_row_start:my_row_end\n    # Example: tridiagonal matrix\n    if i > 1\n        push!(I, i); push!(J, i-1); push!(V, -1.0)\n    end\n    push!(I, i); push!(J, i); push!(V, 2.0)\n    if i < m\n        push!(I, i); push!(J, i+1); push!(V, -1.0)\n    end\nend\nA = sparse(I, J, V, m, n)\n\n# The constructor extracts only local rows - other rows are ignored\nAdist = HPCSparseMatrix(A, backend)","category":"section"},{"location":"guide/#Basic-Operations","page":"User Guide","title":"Basic Operations","text":"","category":"section"},{"location":"guide/#Vector-Operations","page":"User Guide","title":"Vector Operations","text":"backend = BACKEND_CPU_MPI\nv = HPCVector(randn(100), backend)\nw = HPCVector(randn(100), backend)\n\n# Arithmetic (backend is inferred from operands)\nu = v + w\nu = v - w\nu = 2.0 * v\nu = v * 2.0\n\n# Linear algebra\nn = norm(v)\nd = dot(v, w)\nc = conj(v)","category":"section"},{"location":"guide/#Matrix-Vector-Products","page":"User Guide","title":"Matrix-Vector Products","text":"backend = BACKEND_CPU_MPI\nA = HPCMatrix(randn(50, 100), backend)\nv = HPCVector(randn(100), backend)\n\n# Matrix-vector multiply\ny = A * v","category":"section"},{"location":"guide/#Sparse-Operations","page":"User Guide","title":"Sparse Operations","text":"using SparseArrays\n\nbackend = BACKEND_CPU_MPI\nA = HPCSparseMatrix(sprandn(100, 100, 0.1), backend)\nv = HPCVector(randn(100), backend)\n\n# Matrix-vector multiply\ny = A * v\n\n# Matrix-matrix multiply\nB = HPCSparseMatrix(sprandn(100, 100, 0.1), backend)\nC = A * B","category":"section"},{"location":"guide/#Solving-Linear-Systems","page":"User Guide","title":"Solving Linear Systems","text":"","category":"section"},{"location":"guide/#Direct-Solve-with-Backslash","page":"User Guide","title":"Direct Solve with Backslash","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\n\nbackend = BACKEND_CPU_MPI\n\n# Create a well-conditioned sparse matrix\nA = HPCSparseMatrix(sprandn(100, 100, 0.1) + 10I, backend)\nb = HPCVector(randn(100), backend)\n\n# Solve A * x = b\nx = A \\ b","category":"section"},{"location":"guide/#Symmetric-Systems-(Faster)","page":"User Guide","title":"Symmetric Systems (Faster)","text":"For symmetric matrices, wrap with Symmetric to use faster LDLT factorization:\n\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# Create symmetric positive definite matrix\nA_base = HPCSparseMatrix(sprandn(100, 100, 0.1), backend)\nA_spd = A_base + HPCSparseMatrix(transpose(A_base), backend) +\n        HPCSparseMatrix(sparse(10.0I, 100, 100), backend)\n\nb = HPCVector(randn(100), backend)\n\n# Use Symmetric wrapper for faster solve\nx = Symmetric(A_spd) \\ b","category":"section"},{"location":"guide/#Reusing-Factorizations","page":"User Guide","title":"Reusing Factorizations","text":"For repeated solves with the same matrix, compute the factorization once:\n\nusing LinearAlgebra\n\n# LU factorization\nF = lu(A)\nx1 = F \\ b1\nx2 = F \\ b2\nfinalize!(F)  # Clean up MUMPS resources\n\n# LDLT factorization (symmetric matrices)\nF = ldlt(A_spd)\nx = F \\ b\nfinalize!(F)","category":"section"},{"location":"guide/#Threading","page":"User Guide","title":"Threading","text":"HPCLinearAlgebra uses two threading mechanisms for the MUMPS sparse direct solver:\n\nOpenMP threads (OMP_NUM_THREADS) - Affects MUMPS algorithm-level parallelism\nBLAS threads (OPENBLAS_NUM_THREADS) - Affects dense matrix operations in both Julia and MUMPS","category":"section"},{"location":"guide/#MUMPS-Solver-Threading","page":"User Guide","title":"MUMPS Solver Threading","text":"HPCLinearAlgebra uses the MUMPS (MUltifrontal Massively Parallel Solver) library for sparse direct solves via lu() and ldlt(). MUMPS has two independent threading mechanisms that can be tuned for performance.\n\nOpenMP threads (OMP_NUM_THREADS)\n\nControls MUMPS's algorithm-level parallelism\nThe multifrontal method builds an elimination tree of \"frontal matrices\"\nOpenMP threads process independent subtrees in parallel\nThis is coarse-grained: different threads work on different parts of the matrix\n\nBLAS threads (OPENBLAS_NUM_THREADS)\n\nControls parallelism inside dense matrix operations\nWhen MUMPS factors a frontal matrix, it calls BLAS routines (DGEMM, etc.)\nOpenBLAS can parallelize these dense operations\nThis is fine-grained: threads cooperate on the same dense block\n\nNote on BLAS libraries: Julia and MUMPS use separate OpenBLAS libraries (libopenblas64_.dylib for Julia's ILP64 interface, libopenblas.dylib for MUMPS's LP64 interface). Both libraries read OPENBLAS_NUM_THREADS at initialization, so this environment variable affects both.","category":"section"},{"location":"guide/#Recommended-Configuration","page":"User Guide","title":"Recommended Configuration","text":"For behavior that closely matches Julia's built-in sparse solver (UMFPACK):\n\nexport OMP_NUM_THREADS=1\nexport OPENBLAS_NUM_THREADS=<number_of_cores>\n\nThis configuration uses only BLAS-level threading, which is the same strategy Julia's built-in solver uses.","category":"section"},{"location":"guide/#Performance-Comparison-(Single-Rank)","page":"User Guide","title":"Performance Comparison (Single-Rank)","text":"The following table compares MUMPS (OMP_NUM_THREADS=1, OPENBLAS_NUM_THREADS=10) against Julia's built-in sparse solver (also using the same settings) on a 2D Laplacian problem. This is a single-rank comparison to establish baseline overhead; multi-rank MPI parallelism provides additional speedup. Benchmarks were run on a 2025 M4 MacBook Pro with 10 CPU cores:\n\nn Julia (ms) MUMPS (ms) Ratio\n9 0.004 0.024 6.1x\n100 0.020 0.044 2.3x\n961 0.226 0.276 1.2x\n10,000 3.99 3.76 0.9x\n99,856 48.6 44.8 0.9x\n1,000,000 597 550 0.9x\n\nKey observations:\n\nAt small problem sizes, MUMPS has initialization overhead (~0.02ms)\nAt large problem sizes (n ≥ 10,000), MUMPS is faster than Julia's built-in solver\nCached symbolic analysis and vectorized value copying minimize repeated factorization overhead","category":"section"},{"location":"guide/#Default-Behavior","page":"User Guide","title":"Default Behavior","text":"For optimal performance, set threading environment variables before starting Julia:\n\nexport OMP_NUM_THREADS=1\nexport OPENBLAS_NUM_THREADS=10  # or your number of CPU cores\njulia your_script.jl\n\nEnvironment variables must be set before starting Julia because OpenBLAS creates its thread pool during library initialization. HPCLinearAlgebra attempts to set sensible defaults programmatically, but this may not always take effect if the thread pool is already initialized.\n\nYou can also add these to your shell profile (.bashrc, .zshrc, etc.) or Julia's startup.jl:\n\n# In ~/.julia/config/startup.jl\nENV[\"OMP_NUM_THREADS\"] = \"1\"\nENV[\"OPENBLAS_NUM_THREADS\"] = string(Sys.CPU_THREADS)","category":"section"},{"location":"guide/#Advanced:-Combined-OMP-and-BLAS-Threading","page":"User Guide","title":"Advanced: Combined OMP and BLAS Threading","text":"For some problems, combining OMP and BLAS threading can be faster:\n\nexport OMP_NUM_THREADS=4\nexport OPENBLAS_NUM_THREADS=4\njulia your_script.jl\n\nThis MUMPS configuration (OMP=4, BLAS=4) achieved 14% faster performance than Julia's built-in solver on a 1M DOF 2D Laplacian in testing. However, the optimal configuration depends on your specific problem structure and hardware.\n\nImportant caveat: OPENBLAS_NUM_THREADS is a process-wide setting that affects both MUMPS and Julia's built-in sparse solver (UMFPACK). If you set OPENBLAS_NUM_THREADS=4 to optimize MUMPS, Julia's built-in solver will also be limited to 4 BLAS threads.","category":"section"},{"location":"guide/#Row-wise-Operations-with-map_rows","page":"User Guide","title":"Row-wise Operations with map_rows","text":"The map_rows function applies a function to corresponding rows across distributed arrays:\n\nbackend = BACKEND_CPU_MPI\nA = HPCMatrix(randn(50, 10), backend)\n\n# Compute row norms (backend is inferred from A)\nnorms = map_rows(row -> norm(row), A)  # Returns HPCVector\n\n# Compute row sums and products\nstats = map_rows(row -> [sum(row), prod(row)]', A)  # Returns HPCMatrix\n\n# Combine multiple inputs\nv = HPCVector(randn(50), backend)\nweighted = map_rows((row, w) -> sum(row) * w[1], A, v)","category":"section"},{"location":"guide/#Result-Types","page":"User Guide","title":"Result Types","text":"f returns Result type\nScalar HPCVector\nColumn vector HPCVector (concatenated)\nRow vector (v') HPCMatrix\nMatrix HPCMatrix","category":"section"},{"location":"guide/#Type-Conversions","page":"User Guide","title":"Type Conversions","text":"","category":"section"},{"location":"guide/#Gathering-to-Native-Types","page":"User Guide","title":"Gathering to Native Types","text":"Convert distributed types back to native Julia arrays (gathers data to all ranks):\n\nbackend = BACKEND_CPU_MPI\n\nv_mpi = HPCVector(randn(100), backend)\nv_native = Vector(v_mpi)  # Full vector on all ranks\n\nA_mpi = HPCMatrix(randn(50, 30), backend)\nA_native = Matrix(A_mpi)  # Full matrix on all ranks\n\nS_mpi = HPCSparseMatrix(sprandn(100, 100, 0.1), backend)\nS_native = SparseMatrixCSC(S_mpi)  # Full sparse matrix","category":"section"},{"location":"guide/#IO-and-Output","page":"User Guide","title":"IO and Output","text":"","category":"section"},{"location":"guide/#Printing-from-Rank-0","page":"User Guide","title":"Printing from Rank 0","text":"Use io0() to print from rank 0 only:\n\nprintln(io0(), \"This prints once from rank 0!\")\n\n# Custom rank selection\nprintln(io0(r=Set([0, 1])), \"Hello from ranks 0 and 1!\")","category":"section"},{"location":"guide/#MPI-Rank-Information","page":"User Guide","title":"MPI Rank Information","text":"using MPI\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)   # Current rank (0 to nranks-1)\nnranks = MPI.Comm_size(MPI.COMM_WORLD) # Total number of ranks","category":"section"},{"location":"guide/#Repartitioning","page":"User Guide","title":"Repartitioning","text":"Redistribute data to match a different partition:\n\nbackend = BACKEND_CPU_MPI\nv = HPCVector(randn(100), backend)\n\n# Get current partition\nold_partition = v.partition\n\n# Create new partition\nnew_partition = uniform_partition(100, MPI.Comm_size(MPI.COMM_WORLD))\n\n# Repartition\nv_new = repartition(v, new_partition)","category":"section"},{"location":"guide/#GPU-Support","page":"User Guide","title":"GPU Support","text":"HPCLinearAlgebra supports GPU acceleration via Metal.jl (macOS) or CUDA.jl (Linux/Windows). GPU support is optional and loaded as package extensions.","category":"section"},{"location":"guide/#Setup","page":"User Guide","title":"Setup","text":"Load the GPU package before MPI for proper detection:\n\n# For Metal (macOS)\nusing Metal\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\n\n# For CUDA (Linux/Windows)\nusing CUDA, NCCL, CUDSS_jll\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra","category":"section"},{"location":"guide/#Creating-GPU-Backends","page":"User Guide","title":"Creating GPU Backends","text":"Use factory functions to create GPU backends:\n\ncomm = MPI.COMM_WORLD\n\n# Metal backend (macOS)\nmetal_backend = backend_metal_mpi(comm)\n\n# CUDA backend (Linux/Windows)\ncuda_backend = backend_cuda_mpi(comm)","category":"section"},{"location":"guide/#Converting-Between-Backends","page":"User Guide","title":"Converting Between Backends","text":"Use to_backend(obj, backend) to convert between CPU and GPU:\n\ncomm = MPI.COMM_WORLD\ncpu_backend = BACKEND_CPU_MPI\nmetal_backend = backend_metal_mpi(comm)\n\n# Create CPU vector\nx_cpu = HPCVector(rand(Float32, 1000), cpu_backend)\n\n# Convert to GPU (Metal)\nx_gpu = to_backend(x_cpu, metal_backend)  # Returns HPCVector with MtlVector storage\n\n# GPU operations work transparently\ny_gpu = x_gpu + x_gpu  # Native GPU addition\nz_gpu = 2.0f0 * x_gpu  # Native GPU scalar multiply\n\n# Convert back to CPU\ny_cpu = to_backend(y_gpu, cpu_backend)\n\nFor CUDA:\n\ncomm = MPI.COMM_WORLD\ncpu_backend = BACKEND_CPU_MPI\ncuda_backend = backend_cuda_mpi(comm)\n\n# Create CPU vector\nx_cpu = HPCVector(rand(Float64, 1000), cpu_backend)\n\n# Convert to GPU (CUDA)\nx_gpu = to_backend(x_cpu, cuda_backend)   # Returns HPCVector with CuVector storage\n\n# GPU operations work transparently\ny_gpu = x_gpu + x_gpu  # Native GPU addition\nz_gpu = 2.0 * x_gpu    # Native GPU scalar multiply","category":"section"},{"location":"guide/#How-It-Works","page":"User Guide","title":"How It Works","text":"The backend's device type determines where data lives:\n\nBackend Device Storage Operations\nDeviceCPU() Vector/Matrix Native CPU\nDeviceMetal() MtlVector/MtlMatrix Native GPU for vector ops\nDeviceCUDA() CuVector/CuMatrix Native GPU for vector ops","category":"section"},{"location":"guide/#MPI-Communication","page":"User Guide","title":"MPI Communication","text":"MPI always uses CPU buffers (no GPU-aware MPI). GPU data is automatically staged through CPU:\n\nGPU vector data copied to CPU staging buffer\nMPI communication on CPU buffers\nResults copied back to GPU\n\nThis is handled transparently - you just use the same operations.","category":"section"},{"location":"guide/#Sparse-Matrix-Operations","page":"User Guide","title":"Sparse Matrix Operations","text":"Sparse matrices (HPCSparseMatrix) remain on CPU. When multiplying with GPU vectors:\n\ncuda_backend = backend_cuda_mpi(MPI.COMM_WORLD)\nA = HPCSparseMatrix(sprand(100, 100, 0.1), cuda_backend)\nx_gpu = HPCVector(rand(100), cuda_backend)\n\n# Sparse multiply: x gathered via CPU, multiply on CPU, result copied to GPU\ny_gpu = A * x_gpu  # Returns HPCVector with CuVector storage","category":"section"},{"location":"guide/#Supported-GPU-Operations","page":"User Guide","title":"Supported GPU Operations","text":"Operation Metal CUDA\nv + w, v - w Native Native\nα * v (scalar) Native Native\nA * x (sparse) CPU staging CPU staging\nA * x (dense) CPU staging CPU staging\ntranspose(A) * x CPU staging CPU staging\nBroadcasting (abs.(v)) Native Native\ncudss_lu(A), cudss_ldlt(A) N/A Multi-GPU native","category":"section"},{"location":"guide/#Element-Types","page":"User Guide","title":"Element Types","text":"Metal: Requires Float32 (no Float64 support)\nCUDA: Supports both Float32 and Float64","category":"section"},{"location":"guide/#cuDSS-Multi-GPU-Solver-(CUDA-only)","page":"User Guide","title":"cuDSS Multi-GPU Solver (CUDA only)","text":"For multi-GPU distributed sparse direct solves, HPCLinearAlgebra provides CuDSSFactorizationMPI using NVIDIA's cuDSS library with NCCL inter-GPU communication.","category":"section"},{"location":"guide/#Basic-Usage","page":"User Guide","title":"Basic Usage","text":"using CUDA, NCCL, CUDSS_jll\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\n\n# Each MPI rank should use a different GPU\nCUDA.device!(MPI.Comm_rank(MPI.COMM_WORLD) % length(CUDA.devices()))\n\n# Create CUDA backend\ncuda_backend = backend_cuda_mpi(MPI.COMM_WORLD)\n\n# Create distributed sparse matrix (symmetric positive definite)\nA = HPCSparseMatrix(make_spd_matrix(1000), cuda_backend)\nb = HPCVector(rand(1000), cuda_backend)\n\n# Multi-GPU LDLT factorization\nF = cudss_ldlt(A)\nx = F \\ b\nfinalize!(F)  # Required: clean up cuDSS resources","category":"section"},{"location":"guide/#Available-Factorizations","page":"User Guide","title":"Available Factorizations","text":"# For symmetric positive definite matrices\nF = cudss_ldlt(A)\n\n# For general (non-symmetric) matrices\nF = cudss_lu(A)","category":"section"},{"location":"guide/#Important-Notes","page":"User Guide","title":"Important Notes","text":"GPU assignment: Each MPI rank should use a different GPU. Use CUDA.device!() to assign.\nNCCL bootstrap: The NCCL communicator is created automatically from MPI - no manual setup needed.\nResource cleanup: Always call finalize!(F) when done. This prevents MPI desynchronization during garbage collection.\nRequirements: cuDSS 0.4+ with MGMN (Multi-GPU Multi-Node) support.\nNCCL P2P: On some clusters, you may need to set NCCL_P2P_DISABLE=1 to avoid hangs with cross-NUMA GPU topologies.","category":"section"},{"location":"guide/#Known-cuDSS-Bug-(as-of-January-2026)","page":"User Guide","title":"Known cuDSS Bug (as of January 2026)","text":"cuDSS MGMN mode crashes with status=5 on certain sparse matrix patterns, notably narrow-bandwidth matrices like tridiagonals. This bug has been reported to NVIDIA.\n\nSymptoms: The analysis phase fails with status code 5 for matrices that have few nonzeros per row when distributed across multiple GPUs.\n\nWorkaround: Adding explicit numerical zeros to widen the bandwidth allows cuDSS to succeed. For example, if your matrix is tridiagonal, storing it with additional zero entries in each row (making it appear as a wider-band matrix) works around the bug. The 2D Laplacian (5-point stencil) works correctly.\n\nMinimal reproducer: See bug/cudss_mgmn_tridiag_bug.cu in the repository for a C/CUDA test case demonstrating the issue.","category":"section"},{"location":"guide/#Cache-Management","page":"User Guide","title":"Cache Management","text":"HPCLinearAlgebra caches communication plans for efficiency. Clear caches when needed:\n\nclear_plan_cache!()  # Clears all plan caches including MUMPS analysis cache","category":"section"},{"location":"guide/#MPI-Collective-Operations","page":"User Guide","title":"MPI Collective Operations","text":"warning: All Operations Are Collective\nMost HPCLinearAlgebra functions are MPI collective operations. All ranks must:Call the function together\nUse the same parameters\nAvoid conditional execution based on rank\n\nCorrect:\n\n# All ranks execute this together\nx = A \\ b\n\nIncorrect (causes deadlock):\n\nif rank == 0\n    x = A \\ b  # Only rank 0 calls - DEADLOCK!\nend","category":"section"},{"location":"guide/#Next-Steps","page":"User Guide","title":"Next Steps","text":"See Examples for detailed code examples\nSee the API Reference for detailed function documentation","category":"section"},{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"This page provides detailed documentation for all exported types and functions in HPCLinearAlgebra.jl.\n\nnote: MPI Collective Operations\nUnless otherwise noted, all functions are MPI collective operations. Every MPI rank must call these functions together.","category":"section"},{"location":"api/#Distributed-Types","page":"API Reference","title":"Distributed Types","text":"","category":"section"},{"location":"api/#HPCVector","page":"API Reference","title":"HPCVector","text":"","category":"section"},{"location":"api/#HPCMatrix","page":"API Reference","title":"HPCMatrix","text":"","category":"section"},{"location":"api/#HPCSparseMatrix","page":"API Reference","title":"HPCSparseMatrix","text":"","category":"section"},{"location":"api/#SparseMatrixCSR","page":"API Reference","title":"SparseMatrixCSR","text":"","category":"section"},{"location":"api/#Local-Constructors","page":"API Reference","title":"Local Constructors","text":"These constructors create distributed types from local data without global communication.","category":"section"},{"location":"api/#Row-wise-Operations","page":"API Reference","title":"Row-wise Operations","text":"","category":"section"},{"location":"api/#Linear-System-Solvers","page":"API Reference","title":"Linear System Solvers","text":"","category":"section"},{"location":"api/#Partition-Utilities","page":"API Reference","title":"Partition Utilities","text":"","category":"section"},{"location":"api/#Cache-Management","page":"API Reference","title":"Cache Management","text":"","category":"section"},{"location":"api/#IO-Utilities","page":"API Reference","title":"IO Utilities","text":"","category":"section"},{"location":"api/#Backend-Types","page":"API Reference","title":"Backend Types","text":"","category":"section"},{"location":"api/#Type-Mappings","page":"API Reference","title":"Type Mappings","text":"","category":"section"},{"location":"api/#Native-to-Distributed-Conversions","page":"API Reference","title":"Native to Distributed Conversions","text":"Native Type Distributed Type Description\nVector{T} HPCVector{T,B} Distributed vector\nMatrix{T} HPCMatrix{T,B} Distributed dense matrix\nSparseMatrixCSC{T,Ti} HPCSparseMatrix{T,Ti,B} Distributed sparse matrix\n\nThe B<:HPCBackend type parameter specifies the backend configuration (device, communication, solver). Use pre-constructed backends like BACKEND_CPU_MPI or factory functions like backend_cuda_mpi(comm).","category":"section"},{"location":"api/#Distributed-to-Native-Conversions","page":"API Reference","title":"Distributed to Native Conversions","text":"Distributed Type Native Type Function\nHPCVector{T,B} Vector{T} Vector(v)\nHPCMatrix{T,B} Matrix{T} Matrix(A)\nHPCSparseMatrix{T,Ti,B} SparseMatrixCSC{T,Ti} SparseMatrixCSC(A)","category":"section"},{"location":"api/#Supported-Operations","page":"API Reference","title":"Supported Operations","text":"","category":"section"},{"location":"api/#HPCVector-Operations","page":"API Reference","title":"HPCVector Operations","text":"Arithmetic: +, -, * (scalar)\nLinear algebra: norm, dot, conj\nIndexing: v[i] (global index)\nConversion: Vector(v)","category":"section"},{"location":"api/#HPCMatrix-Operations","page":"API Reference","title":"HPCMatrix Operations","text":"Arithmetic: * (scalar), matrix-vector product\nTranspose: transpose(A)\nIndexing: A[i, j] (global indices)\nConversion: Matrix(A)","category":"section"},{"location":"api/#HPCSparseMatrix-Operations","page":"API Reference","title":"HPCSparseMatrix Operations","text":"Arithmetic: +, -, * (scalar, matrix-vector, matrix-matrix)\nTranspose: transpose(A)\nLinear solve: A \\ b, Symmetric(A) \\ b\nUtilities: nnz, norm, issymmetric\nConversion: SparseMatrixCSC(A)","category":"section"},{"location":"api/#Factorization-Types","page":"API Reference","title":"Factorization Types","text":"HPCLinearAlgebra uses MUMPS for sparse direct solves:\n\nlu(A): LU factorization (general matrices)\nldlt(A): LDLT factorization (symmetric matrices, faster)\n\nBoth return factorization objects that support:\n\nF \\ b: Solve with factorization\nfinalize!(F): Release MUMPS resources","category":"section"},{"location":"api/#HPCLinearAlgebra.HPCVector","page":"API Reference","title":"HPCLinearAlgebra.HPCVector","text":"HPCVector{T, B<:HPCBackend}\n\nA distributed dense vector partitioned across MPI ranks.\n\nType Parameters\n\nT: Element type (e.g., Float64, ComplexF64)\nB<:HPCBackend: Backend configuration (device, communication, solver)\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the partition\npartition::Vector{Int}: Partition boundaries, length = nranks + 1 (always on CPU)\nv::AbstractVector{T}: Local vector elements owned by this rank\nbackend::B: The HPC backend configuration\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.HPCMatrix","page":"API Reference","title":"HPCLinearAlgebra.HPCMatrix","text":"HPCMatrix{T, B<:HPCBackend}\n\nA distributed dense matrix partitioned by rows across MPI ranks.\n\nType Parameters\n\nT: Element type (e.g., Float64, ComplexF64)\nB<:HPCBackend: Backend configuration (device, communication, solver)\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1 (always on CPU)\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (always on CPU)\nA::AbstractMatrix{T}: Local rows (NOT transposed), size = (local_nrows, ncols)\nbackend::B: The HPC backend configuration\n\nInvariants\n\nrow_partition and col_partition are sorted\nrow_partition[nranks+1] = total number of rows + 1\ncol_partition[nranks+1] = total number of columns + 1\nsize(A, 1) == row_partition[rank+2] - row_partition[rank+1]\nsize(A, 2) == col_partition[end] - 1\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.HPCSparseMatrix","page":"API Reference","title":"HPCLinearAlgebra.HPCSparseMatrix","text":"HPCSparseMatrix{T,Ti,AV}\n\nA distributed sparse matrix partitioned by rows across MPI ranks.\n\nType Parameters\n\nT: Element type (e.g., Float64, ComplexF64)\nTi: Index type (e.g., Int, Int32), defaults to Int\nAV<:AbstractVector{T}: Storage type for nonzero values (Vector{T} for CPU, MtlVector{T} for GPU)\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (placeholder for transpose)\ncol_indices::Vector{Int}: Global column indices that appear in the local part (local→global mapping)\nrowptr::Vector{Ti}: Row pointers for CSR format (always CPU)\ncolval::Vector{Ti}: LOCAL column indices 1:length(col_indices) for each nonzero (always CPU)\nnzval::AV: Nonzero values (CPU or GPU)\nnrows_local::Int: Number of local rows\nncols_compressed::Int: Number of unique columns = length(col_indices)\ncached_transpose: Cached materialized transpose (bidirectionally linked)\n\nInvariants\n\ncol_indices, row_partition, and col_partition are sorted\nrow_partition[nranks+1] = total number of rows\ncol_partition[nranks+1] = total number of columns\nnrows_local == row_partition[rank+1] - row_partition[rank] (number of local rows)\nncols_compressed == length(col_indices) (compressed column dimension)\ncolval contains local indices in 1:ncols_compressed\nrowptr has length nrows_local + 1\n\nStorage Details\n\nThe local rows are stored in CSR format (Compressed Sparse Row), which enables efficient row-wise iteration - essential for a row-partitioned distributed matrix.\n\nThe CSR storage consists of:\n\nrowptr: Row pointers where row i has nonzeros at positions rowptr[i]:(rowptr[i+1]-1)\ncolval: LOCAL column indices (1:ncols_compressed), not global indices\nnzval: Nonzero values\ncol_indices[local_idx] maps local→global column indices\n\nThis compression avoids \"hypersparse\" storage where the column dimension would be the global number of columns even if only a few columns have nonzeros locally.\n\nGPU Support\n\nStructure arrays (rowptr, colval) always stay on CPU for MPI communication and indexing. Only nzval can live on GPU, with type determined by the backend device:\n\nDeviceCPU: Vector{T} storage\nDeviceMetal: MtlVector{T} storage (macOS)\nDeviceCUDA: CuVector{T} storage\n\nUse to_backend(A, target_backend) to convert between backends.\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.SparseMatrixCSR","page":"API Reference","title":"HPCLinearAlgebra.SparseMatrixCSR","text":"SparseMatrixCSR{Tv,Ti} = Transpose{Tv, SparseMatrixCSC{Tv,Ti}}\n\nType alias for CSR (Compressed Sparse Row) storage format.\n\nThe Dual Life of Transpose{SparseMatrixCSC}\n\nIn Julia, the type Transpose{Tv, SparseMatrixCSC{Tv,Ti}} has two interpretations:\n\nSemantic interpretation: A lazy transpose wrapper around a CSC matrix. When you call transpose(A) on a SparseMatrixCSC, you get this wrapper that represents A^T without copying data.\nStorage interpretation: CSR (row-major) access to sparse data. The underlying CSC stores columns contiguously, but through the transpose wrapper, we can iterate efficiently over rows instead of columns.\n\nThis alias clarifies intent: use SparseMatrixCSR when you want row-major storage semantics, and transpose(A) when you want the mathematical transpose.\n\nCSR vs CSC Storage\n\nCSC (Compressed Sparse Column): Julia's native sparse format. Efficient for column-wise operations, matrix-vector products with column access.\nCSR (Compressed Sparse Row): Efficient for row-wise operations, matrix-vector products with row access, and row-partitioned distributed matrices.\n\nFor SparseMatrixCSR, the underlying parent::SparseMatrixCSC stores the transposed matrix. If B = SparseMatrixCSR(A) represents matrix M, then B.parent is a CSC storing M^T. This means:\n\nB.parent.colptr acts as row pointers for M\nB.parent.rowval contains column indices for M\nB.parent.nzval contains values in row-major order\n\nUsage Note\n\nJulia will still display this type as Transpose{Float64, SparseMatrixCSC{...}}, not as SparseMatrixCSR. The alias improves code clarity but doesn't affect type printing.\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.HPCVector_local","page":"API Reference","title":"HPCLinearAlgebra.HPCVector_local","text":"HPCVector_local(v_local::AbstractVector{T}, backend::HPCBackend) where T\n\nCreate a HPCVector from a local vector on each rank.\n\nUnlike HPCVector(v_global, backend) which takes a global vector and partitions it, this constructor takes only the local portion of the vector that each rank owns. The partition is computed by gathering the local sizes from all ranks.\n\nArguments\n\nv_local: Local vector portion owned by this rank\nbackend: The HPCBackend configuration (determines communication)\n\nExample\n\nbackend = backend_cpu_mpi(MPI.COMM_WORLD)\n# Rank 0 has [1.0, 2.0], Rank 1 has [3.0, 4.0, 5.0]\nv = HPCVector_local([1.0, 2.0], backend)  # on rank 0\nv = HPCVector_local([3.0, 4.0, 5.0], backend)  # on rank 1\n# Result: distributed vector [1.0, 2.0, 3.0, 4.0, 5.0] with partition [1, 3, 6]\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.HPCMatrix_local","page":"API Reference","title":"HPCLinearAlgebra.HPCMatrix_local","text":"HPCMatrix_local(A_local::AbstractMatrix{T}, backend::HPCBackend; col_partition=...) where T\n\nCreate a HPCMatrix from a local matrix on each rank.\n\nUnlike HPCMatrix(M_global, backend) which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.\n\nThe type of A_local determines the storage backend (CPU Matrix, GPU MtlMatrix, etc.).\n\nAll ranks must have local matrices with the same number of columns. A collective error is raised if the column counts don't match.\n\nArguments\n\nA_local::AbstractMatrix{T}: Local rows owned by this rank\nbackend::HPCBackend: The HPC backend configuration\n\nKeyword Arguments\n\ncol_partition::Vector{Int}: Column partition boundaries (default: uniform_partition(size(A_local,2), nranks))\n\nExample\n\nbackend = backend_cpu_mpi(MPI.COMM_WORLD)\n# Rank 0 has 2×3 matrix, Rank 1 has 3×3 matrix\nA = HPCMatrix_local(randn(2, 3), backend)  # on rank 0\nA = HPCMatrix_local(randn(3, 3), backend)  # on rank 1\n# Result: 5×3 distributed matrix with row_partition [1, 3, 6]\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.HPCSparseMatrix_local","page":"API Reference","title":"HPCLinearAlgebra.HPCSparseMatrix_local","text":"HPCSparseMatrix_local(A_local::SparseMatrixCSR{T,Ti}, backend::HPCBackend; col_partition=...) where {T,Ti}\nHPCSparseMatrix_local(A_local::Adjoint{T,SparseMatrixCSC{T,Ti}}, backend::HPCBackend; col_partition=...) where {T,Ti}\n\nCreate a HPCSparseMatrix from a local sparse matrix on each rank.\n\nUnlike HPCSparseMatrix{T}(A_global, backend) which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.\n\nThe input A_local must be a SparseMatrixCSR{T,Ti} (or Adjoint of SparseMatrixCSC{T,Ti}) where:\n\nA_local.parent.n = number of local rows on this rank\nA_local.parent.m = global number of columns (must match on all ranks)\nA_local.parent.rowval = global column indices\n\nAll ranks must have local matrices with the same number of columns (block widths must match). A collective error is raised if the column counts don't match.\n\nNote: For Adjoint inputs, the values are conjugated to match the adjoint semantics.\n\nArguments\n\nA_local: Local sparse matrix in CSR format\nbackend::HPCBackend: Backend configuration (device, comm, solver)\n\nKeyword Arguments\n\ncol_partition::Vector{Int}: Column partition boundaries (default: uniform_partition(A_local.parent.m, nranks))\n\nExample\n\nbackend = backend_cpu_mpi(MPI.COMM_WORLD)\n# Create local rows in CSR format\n# Rank 0 owns rows 1-2 of a 5×3 matrix, Rank 1 owns rows 3-5\nlocal_csc = sparse([1, 1, 2], [1, 2, 3], [1.0, 2.0, 3.0], 2, 3)  # 2 local rows, 3 cols\nA = HPCSparseMatrix_local(SparseMatrixCSR(local_csc), backend)\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.map_rows","page":"API Reference","title":"HPCLinearAlgebra.map_rows","text":"map_rows(f, A...)\n\nApply function f to corresponding rows of distributed arrays, with CPU fallback.\n\nThis is the safe version that handles functions with arbitrary closures by converting GPU arrays to CPU, applying the function, and converting back. Use map_rows_gpu for performance-critical inner loops where f is isbits-compatible.\n\nArguments\n\nf: Function to apply to each row (can capture non-isbits data)\nA...: One or more distributed arrays (HPCVector or HPCMatrix)\n\nReturns\n\nHPCVector or HPCMatrix depending on the return type of f\n\nSee also: map_rows_gpu for GPU-native version (requires isbits closures)\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.map_rows_gpu","page":"API Reference","title":"HPCLinearAlgebra.map_rows_gpu","text":"map_rows_gpu(f, A...)\n\nApply function f to corresponding rows of distributed vectors/matrices (GPU-native).\n\nEach argument in A... must be either a HPCVector or HPCMatrix. All inputs are repartitioned to match the partition of the first argument before applying f.\n\nThis implementation uses GPU-friendly broadcasting: matrices are converted to Vector{SVector} via transpose+reinterpret, then f is broadcast over all arguments. This avoids GPU->CPU->GPU round-trips when the underlying arrays are on GPU.\n\nImportant: The function f must be isbits-compatible (no captured non-isbits data) for GPU execution. Use map_rows for functions with arbitrary closures.\n\nFor each row index i, f is called with:\n\nFor HPCVector: the scalar element at index i\nFor HPCMatrix: an SVector containing the i-th row\n\nResult Type\n\nThe result type depends on what f returns:\n\nf returns Result\nscalar (Number) HPCVector (one element per input row)\nSVector{K,T} HPCMatrix (K columns, one row per input row)\n\nExamples\n\n# Element-wise product of two vectors\nu = HPCVector([1.0, 2.0, 3.0])\nv = HPCVector([4.0, 5.0, 6.0])\nw = map_rows_gpu((a, b) -> a * b, u, v)  # HPCVector([4.0, 10.0, 18.0])\n\n# Row norms of a matrix\nA = HPCMatrix(randn(5, 3))\nnorms = map_rows_gpu(r -> norm(r), A)  # HPCVector of row norms\n\n# Return SVector to build a matrix\nA = HPCMatrix(randn(3, 2))\nresult = map_rows_gpu(r -> SVector(sum(r), prod(r)), A)  # 3×2 HPCMatrix\n\n# Mixed inputs: matrix rows combined with vector elements\nA = HPCMatrix(randn(4, 3))\nw = HPCVector([1.0, 2.0, 3.0, 4.0])\nresult = map_rows_gpu((row, wi) -> sum(row) * wi, A, w)  # HPCVector\n\nSee also: map_rows for CPU fallback version (handles arbitrary closures)\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.solve","page":"API Reference","title":"HPCLinearAlgebra.solve","text":"solve(F::MUMPSFactorization{Tin, Bin, Tinternal}, b::HPCVector) where {Tin, Bin, Tinternal}\n\nSolve the linear system A*x = b using the precomputed MUMPS factorization.\n\nThe input vector b can have any compatible element type and backend. The result is returned with the same element type and backend as the original matrix used for factorization.\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.solve!","page":"API Reference","title":"HPCLinearAlgebra.solve!","text":"solve!(x::HPCVector, F::MUMPSFactorization{Tin, Bin, Tinternal}, b::HPCVector) where {Tin, Bin, Tinternal}\n\nSolve A*x = b in-place using MUMPS factorization.\n\nAutomatically converts inputs to CPU Float64/ComplexF64 for MUMPS, then converts results back to the element type and backend of x.\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.finalize!","page":"API Reference","title":"HPCLinearAlgebra.finalize!","text":"finalize!(F::MUMPSFactorization)\n\nRelease MUMPS resources. Must be called on all ranks together.\n\nNote: If the MUMPS object is shared with the analysis cache (ownsmumps=false), this only removes the factorization from the registry. The MUMPS object itself is finalized when `clearmumpsanalysiscache!()` is called.\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.uniform_partition","page":"API Reference","title":"HPCLinearAlgebra.uniform_partition","text":"uniform_partition(n::Int, nranks::Int) -> Vector{Int}\n\nCompute a balanced partition of n elements across nranks ranks. Returns a vector of length nranks + 1 with 1-indexed partition boundaries.\n\nThe first mod(n, nranks) ranks get div(n, nranks) + 1 elements, the remaining ranks get div(n, nranks) elements.\n\nExample\n\npartition = uniform_partition(10, 4)  # [1, 4, 7, 9, 11]\n# Rank 0: 1:3 (3 elements)\n# Rank 1: 4:6 (3 elements)\n# Rank 2: 7:8 (2 elements)\n# Rank 3: 9:10 (2 elements)\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.repartition","page":"API Reference","title":"HPCLinearAlgebra.repartition","text":"repartition(x::HPCVector{T}, p::Vector{Int}) where T\n\nRedistribute a HPCVector to a new partition p.\n\nThe partition p must be a valid partition vector of length nranks + 1 with p[1] == 1 and p[end] == length(x) + 1.\n\nReturns a new HPCVector with the same data but partition == p.\n\nExample\n\nv = HPCVector([1.0, 2.0, 3.0, 4.0])  # uniform partition\nnew_partition = [1, 2, 5]  # rank 0 gets 1 element, rank 1 gets 3\nv_repart = repartition(v, new_partition)\n\n\n\n\n\nrepartition(A::HPCMatrix{T}, p::Vector{Int}) where T\n\nRedistribute a HPCMatrix to a new row partition p. The col_partition remains unchanged.\n\nThe partition p must be a valid partition vector of length nranks + 1 with p[1] == 1 and p[end] == size(A, 1) + 1.\n\nReturns a new HPCMatrix with the same data but row_partition == p.\n\nExample\n\nA = HPCMatrix(randn(6, 4))  # uniform partition\nnew_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank\nA_repart = repartition(A, new_partition)\n\n\n\n\n\nrepartition(A::HPCSparseMatrix{T}, p::Vector{Int}) where T\n\nRedistribute a HPCSparseMatrix to a new row partition p. The col_partition remains unchanged.\n\nThe partition p must be a valid partition vector of length nranks + 1 with p[1] == 1 and p[end] == size(A, 1) + 1.\n\nReturns a new HPCSparseMatrix with the same data but row_partition == p.\n\nExample\n\nA = HPCSparseMatrix{Float64}(sprand(6, 4, 0.5))  # uniform partition\nnew_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank\nA_repart = repartition(A, new_partition)\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.clear_plan_cache!","page":"API Reference","title":"HPCLinearAlgebra.clear_plan_cache!","text":"clear_plan_cache!()\n\nClear all memoized plan caches, including the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.clear_mumps_analysis_cache!","page":"API Reference","title":"HPCLinearAlgebra.clear_mumps_analysis_cache!","text":"clear_mumps_analysis_cache!()\n\nClear the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.io0","page":"API Reference","title":"HPCLinearAlgebra.io0","text":"io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)\n\nReturn io if the current MPI rank is in set r, otherwise return dn (default: devnull).\n\nThis is useful for printing only from specific ranks:\n\nprintln(io0(), \"Hello from rank 0!\")\nprintln(io0(r=Set([0,1])), \"Hello from ranks 0 and 1!\")\n\nWith string interpolation:\n\nprintln(io0(), \"Matrix A = $A\")\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.HPCBackend","page":"API Reference","title":"HPCLinearAlgebra.HPCBackend","text":"HPCBackend{D<:AbstractDevice, C<:AbstractComm, S<:AbstractSolver}\n\nUnified backend type that encapsulates device, communication, and solver configuration.\n\nType Parameters\n\nD: Device type (DeviceCPU, DeviceMetal, DeviceCUDA)\nC: Communication type (CommSerial, CommMPI)\nS: Solver type (SolverMUMPS, or cuDSS variants)\n\nFields\n\ndevice::D: The compute device\ncomm::C: The communication backend\nsolver::S: The linear solver\n\nExample\n\n# CPU with MPI and MUMPS\nbackend = HPCBackend(DeviceCPU(), CommMPI(MPI.COMM_WORLD), SolverMUMPS())\n\n# CPU serial (single process)\nbackend = HPCBackend(DeviceCPU(), CommSerial(), SolverMUMPS())\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.DeviceCPU","page":"API Reference","title":"HPCLinearAlgebra.DeviceCPU","text":"DeviceCPU <: AbstractDevice\n\nCPU device - uses standard Julia arrays (Vector, Matrix).\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.DeviceMetal","page":"API Reference","title":"HPCLinearAlgebra.DeviceMetal","text":"DeviceMetal <: AbstractDevice\n\nMetal GPU device (macOS) - uses Metal.jl arrays (MtlVector, MtlMatrix). Defined here as a placeholder; actual Metal support is in the Metal extension.\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.DeviceCUDA","page":"API Reference","title":"HPCLinearAlgebra.DeviceCUDA","text":"DeviceCUDA <: AbstractDevice\n\nCUDA GPU device - uses CUDA.jl arrays (CuVector, CuMatrix). Defined here as a placeholder; actual CUDA support is in the CUDA extension.\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.CommSerial","page":"API Reference","title":"HPCLinearAlgebra.CommSerial","text":"CommSerial <: AbstractComm\n\nSerial (single-process) communication - all collective operations become no-ops.\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.CommMPI","page":"API Reference","title":"HPCLinearAlgebra.CommMPI","text":"CommMPI <: AbstractComm\n\nMPI-based distributed communication.\n\nFields\n\ncomm::MPI.Comm: The MPI communicator to use for all operations.\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.SolverMUMPS","page":"API Reference","title":"HPCLinearAlgebra.SolverMUMPS","text":"SolverMUMPS <: AbstractSolver\n\nMUMPS sparse direct solver. Works on CPU with MPI or serial communication.\n\n\n\n\n\n","category":"type"},{"location":"api/#HPCLinearAlgebra.BACKEND_CPU_MPI","page":"API Reference","title":"HPCLinearAlgebra.BACKEND_CPU_MPI","text":"BACKEND_CPU_MPI\n\nPre-constructed CPU backend with MPI communication (using COMM_WORLD) and MUMPS solver. This is the default backend for distributed CPU computations.\n\nNote: While this constant is created at module load time, actual MPI operations will only work after MPI.Init() has been called.\n\n\n\n\n\n","category":"constant"},{"location":"api/#HPCLinearAlgebra.BACKEND_CPU_SERIAL","page":"API Reference","title":"HPCLinearAlgebra.BACKEND_CPU_SERIAL","text":"BACKEND_CPU_SERIAL\n\nPre-constructed CPU backend with serial (single-process) communication and MUMPS solver. Use this for non-MPI code or single-process testing.\n\n\n\n\n\n","category":"constant"},{"location":"api/#HPCLinearAlgebra.backend_metal_mpi","page":"API Reference","title":"HPCLinearAlgebra.backend_metal_mpi","text":"backend_metal_mpi(comm::MPI.Comm) -> HPCBackend\n\nCreate a Metal GPU backend with MPI communication. Requires the Metal extension to be loaded.\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.backend_cuda_mpi","page":"API Reference","title":"HPCLinearAlgebra.backend_cuda_mpi","text":"backend_cuda_mpi(comm::MPI.Comm) -> HPCBackend\n\nCreate a CUDA GPU backend with MPI communication and cuDSS solver. Requires the CUDA extension to be loaded.\n\n\n\n\n\n","category":"function"},{"location":"api/#HPCLinearAlgebra.to_backend","page":"API Reference","title":"HPCLinearAlgebra.to_backend","text":"to_backend(v::HPCVector, backend::HPCBackend) -> HPCVector\n\nConvert a HPCVector to use a different backend.\n\n\n\n\n\nto_backend(A::HPCMatrix, backend::HPCBackend) -> HPCMatrix\n\nConvert a HPCMatrix to use a different backend.\n\n\n\n\n\nto_backend(A::HPCSparseMatrix, backend::HPCBackend) -> HPCSparseMatrix\n\nConvert a HPCSparseMatrix to use a different backend. The nzval and target structure arrays are converted; CPU structure arrays remain on CPU.\n\n\n\n\n\n","category":"function"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page provides detailed examples of using HPCLinearAlgebra.jl for various distributed sparse matrix operations.","category":"section"},{"location":"examples/#Matrix-Multiplication","page":"Examples","title":"Matrix Multiplication","text":"","category":"section"},{"location":"examples/#Square-Matrices","page":"Examples","title":"Square Matrices","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# Create a tridiagonal matrix (same on all ranks)\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Create another tridiagonal matrix\nV2 = [1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]\nB = sparse(I, J, V2, n, n)\n\n# Distribute matrices\nAdist = HPCSparseMatrix(A, backend)\nBdist = HPCSparseMatrix(B, backend)\n\n# Multiply\nCdist = Adist * Bdist\n\n# Verify against reference\nC_ref = A * B\nC_ref_dist = HPCSparseMatrix(C_ref, backend)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nprintln(io0(), \"Multiplication error: $err\")\n","category":"section"},{"location":"examples/#Non-Square-Matrices","page":"Examples","title":"Non-Square Matrices","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# A is 6x8, B is 8x10, result is 6x10\nm, k, n = 6, 8, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nJ_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 2]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, k)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, k, n)\n\nAdist = HPCSparseMatrix(A, backend)\nBdist = HPCSparseMatrix(B, backend)\n\nCdist = Adist * Bdist\n\n@assert size(Cdist) == (m, n)\n\nprintln(io0(), \"Result size: $(size(Cdist))\")\n","category":"section"},{"location":"examples/#Complex-Matrices","page":"Examples","title":"Complex Matrices","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\nn = 8\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# Complex values\nV_A = ComplexF64.([2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]) .+\n      im .* ComplexF64.([0.1*ones(n); 0.2*ones(n-1); -0.2*ones(n-1)])\nA = sparse(I, J, V_A, n, n)\n\nV_B = ComplexF64.([1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]) .+\n      im .* ComplexF64.([-0.1*ones(n); 0.1*ones(n-1); 0.1*ones(n-1)])\nB = sparse(I, J, V_B, n, n)\n\nAdist = HPCSparseMatrix(A, backend)\nBdist = HPCSparseMatrix(B, backend)\n\n# Multiplication\nCdist = Adist * Bdist\n\n# Conjugate\nAconj = conj(Adist)\n\n# Adjoint (conjugate transpose) - returns lazy wrapper\nAadj = Adist'\n\n# Using adjoint in multiplication (materializes automatically)\nresult = Aadj * Bdist\n\nprintln(io0(), \"Complex matrix operations completed\")\n","category":"section"},{"location":"examples/#Addition-and-Subtraction","page":"Examples","title":"Addition and Subtraction","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\nn = 8\n\n# Matrices with different sparsity patterns\n# Matrix A: upper triangular entries\nI_A = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nJ_A = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nV_A = Float64.(1:9)\nA = sparse(I_A, J_A, V_A, n, n)\n\n# Matrix B: lower triangular entries\nI_B = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nJ_B = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nV_B = Float64.(9:-1:1)\nB = sparse(I_B, J_B, V_B, n, n)\n\nAdist = HPCSparseMatrix(A, backend)\nBdist = HPCSparseMatrix(B, backend)\n\n# Addition - handles different sparsity patterns\nCdist = Adist + Bdist\n\n# Subtraction\nDdist = Adist - Bdist\n\n# Verify\nC_ref_dist = HPCSparseMatrix(A + B, backend)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nprintln(io0(), \"Addition error: $err\")\n","category":"section"},{"location":"examples/#Transpose-Operations","page":"Examples","title":"Transpose Operations","text":"","category":"section"},{"location":"examples/#Lazy-Transpose","page":"Examples","title":"Lazy Transpose","text":"The transpose function creates a lazy wrapper without transposing the data. This is efficient because the actual transpose is only computed when needed:\n\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\nm, n = 8, 6\nI_C = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_C = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nV_C = Float64.(1:length(I_C))\nC = sparse(I_C, J_C, V_C, m, n)\n\nI_D = [1, 2, 3, 4, 5, 6, 1, 2]\nJ_D = [1, 2, 3, 4, 5, 6, 7, 8]\nV_D = Float64.(1:length(I_D))\nD = sparse(I_D, J_D, V_D, n, m)\n\nCdist = HPCSparseMatrix(C, backend)\nDdist = HPCSparseMatrix(D, backend)\n\n# transpose(C) * transpose(D) = transpose(D * C)\n# This is computed efficiently without explicitly transposing\nresult = transpose(Cdist) * transpose(Ddist)\n\nprintln(io0(), \"Lazy transpose multiplication completed\")\n","category":"section"},{"location":"examples/#Transpose-in-Multiplication","page":"Examples","title":"Transpose in Multiplication","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# A is 8x6, so A' is 6x8\n# B is 8x10, so A' * B is 6x10\nm, n, p = 8, 6, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, n)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, m, p)\n\nAdist = HPCSparseMatrix(A, backend)\nBdist = HPCSparseMatrix(B, backend)\n\n# transpose(A) * B - A is automatically materialized as transpose\nresult_dist = transpose(Adist) * Bdist\n\n# Verify\nref = sparse(A') * B\nref_dist = HPCSparseMatrix(ref, backend)\nerr = norm(result_dist - ref_dist, Inf)\n\nprintln(io0(), \"transpose(A) * B error: $err\")\n","category":"section"},{"location":"examples/#Scalar-Multiplication","page":"Examples","title":"Scalar Multiplication","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3]\nJ = [1, 2, 3, 4, 5, 6, 7, 8]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = HPCSparseMatrix(A, backend)\n\n# Scalar times matrix\na = 2.5\nresult1 = a * Adist\nresult2 = Adist * a  # Equivalent\n\n# Scalar times lazy transpose\nAt = transpose(Adist)\nresult3 = a * At  # Returns transpose(a * A)\n\n# Verify\nref_dist = HPCSparseMatrix(a * A, backend)\nerr1 = norm(result1 - ref_dist, Inf)\nerr2 = norm(result2 - ref_dist, Inf)\n\nprintln(io0(), \"Scalar multiplication errors: $err1, $err2\")\n","category":"section"},{"location":"examples/#Computing-Norms","page":"Examples","title":"Computing Norms","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3, 2, 4]\nJ = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = HPCSparseMatrix(A, backend)\n\n# Element-wise norms (treating matrix as vector)\nfrob_norm = norm(Adist)        # Frobenius (2-norm)\none_norm = norm(Adist, 1)      # Sum of absolute values\ninf_norm = norm(Adist, Inf)    # Max absolute value\np_norm = norm(Adist, 3)        # General p-norm\n\n# Operator norms\nop_1 = opnorm(Adist, 1)        # Max absolute column sum\nop_inf = opnorm(Adist, Inf)    # Max absolute row sum\n\nprintln(io0(), \"Frobenius norm: $frob_norm\")\nprintln(io0(), \"1-norm: $one_norm\")\nprintln(io0(), \"Inf-norm: $inf_norm\")\nprintln(io0(), \"3-norm: $p_norm\")\nprintln(io0(), \"Operator 1-norm: $op_1\")\nprintln(io0(), \"Operator Inf-norm: $op_inf\")\n","category":"section"},{"location":"examples/#Iterative-Methods-Example","page":"Examples","title":"Iterative Methods Example","text":"Here's an example of using HPCLinearAlgebra.jl for power iteration to find the dominant eigenvalue:\n\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# Create a symmetric positive definite matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = HPCSparseMatrix(A, backend)\n\n# For power iteration, we need matrix-vector products\n# Currently HPCLinearAlgebra focuses on matrix-matrix products\n# This example shows how to use A*A for related computations\n\n# Compute A^2\nA2dist = Adist * Adist\n\n# Compute the Frobenius norm of A^2\nnorm_A2 = norm(A2dist)\n\nprintln(io0(), \"||A^2||_F = $norm_A2\")\n# For SPD matrices, this relates to the eigenvalues\n","category":"section"},{"location":"examples/#Solving-Linear-Systems","page":"Examples","title":"Solving Linear Systems","text":"HPCLinearAlgebra provides distributed sparse direct solvers using the multifrontal method.","category":"section"},{"location":"examples/#LDLT-Factorization-(Symmetric-Matrices)","page":"Examples","title":"LDLT Factorization (Symmetric Matrices)","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# Create a symmetric positive definite tridiagonal matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Distribute the matrix\nAdist = HPCSparseMatrix(A, backend)\n\n# Compute LDLT factorization\nF = ldlt(Adist)\n\n# Create right-hand side\nb = HPCVector(ones(n), backend)\n\n# Solve Ax = b\nx = solve(F, b)\n\n# Or use backslash syntax\nx = F \\ b\n\n# Verify solution\nx_full = Vector(x)\nresidual = norm(A * x_full - ones(n), Inf)\n\nprintln(io0(), \"LDLT solve residual: $residual\")\n","category":"section"},{"location":"examples/#LU-Factorization-(General-Matrices)","page":"Examples","title":"LU Factorization (General Matrices)","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# Create a general (non-symmetric) tridiagonal matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.8*ones(n-1)]  # Non-symmetric\nA = sparse(I, J, V, n, n)\n\n# Distribute and factorize\nAdist = HPCSparseMatrix(A, backend)\nF = lu(Adist)\n\n# Solve\nb = HPCVector(ones(n), backend)\nx = solve(F, b)\n\n# Verify\nx_full = Vector(x)\nresidual = norm(A * x_full - ones(n), Inf)\n\nprintln(io0(), \"LU solve residual: $residual\")\n","category":"section"},{"location":"examples/#Symmetric-Indefinite-Matrices","page":"Examples","title":"Symmetric Indefinite Matrices","text":"LDLT uses Bunch-Kaufman pivoting to handle symmetric indefinite matrices:\n\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# Symmetric indefinite matrix (alternating signs on diagonal)\nn = 50\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\ndiag_vals = [(-1.0)^i * 2.0 for i in 1:n]  # Alternating signs\nV = [diag_vals; -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = HPCSparseMatrix(A, backend)\nF = ldlt(Adist)\n\nb = HPCVector(collect(1.0:n), backend)\nx = solve(F, b)\n\nx_full = Vector(x)\nresidual = norm(A * x_full - collect(1.0:n), Inf)\n\nprintln(io0(), \"Indefinite LDLT residual: $residual\")\n","category":"section"},{"location":"examples/#Reusing-Symbolic-Factorization","page":"Examples","title":"Reusing Symbolic Factorization","text":"For sequences of matrices with the same sparsity pattern, the symbolic factorization is cached and reused:\n\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# First matrix\nV1 = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA1 = sparse(I, J, V1, n, n)\nA1dist = HPCSparseMatrix(A1, backend)\n\n# First factorization - computes symbolic phase\nF1 = ldlt(A1dist; reuse_symbolic=true)\n\n# Second matrix - same structure, different values\nV2 = [8.0*ones(n); -2.0*ones(n-1); -2.0*ones(n-1)]\nA2 = sparse(I, J, V2, n, n)\nA2dist = HPCSparseMatrix(A2, backend)\n\n# Second factorization - reuses cached symbolic phase (faster)\nF2 = ldlt(A2dist; reuse_symbolic=true)\n\n# Both factorizations work\nb = HPCVector(ones(n), backend)\nx1 = solve(F1, b)\nx2 = solve(F2, b)\n\nx1_full = Vector(x1)\nx2_full = Vector(x2)\nprintln(io0(), \"F1 residual: \", norm(A1 * x1_full - ones(n), Inf))\nprintln(io0(), \"F2 residual: \", norm(A2 * x2_full - ones(n), Inf))\n","category":"section"},{"location":"examples/#Plan-Caching-and-Management","page":"Examples","title":"Plan Caching and Management","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\n\nbackend = BACKEND_CPU_MPI\n\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\nB = spdiagm(0 => 1.5*ones(n), 1 => 0.5*ones(n-1), -1 => 0.5*ones(n-1))\n\nAdist = HPCSparseMatrix(A, backend)\nBdist = HPCSparseMatrix(B, backend)\n\n# First multiplication - creates and caches the plan\nC1 = Adist * Bdist\n\n# Second multiplication - reuses cached plan (faster)\nC2 = Adist * Bdist\n\n# Third multiplication - still uses cached plan\nC3 = Adist * Bdist\n\n# Clear caches when done to free memory\nclear_plan_cache!()  # Clears all caches including factorization\n\n# Or clear specific caches:\n# clear_symbolic_cache!()           # Symbolic factorizations only\n# clear_factorization_plan_cache!() # Factorization plans only\n\nprintln(io0(), \"Cached multiplication completed\")\n","category":"section"},{"location":"examples/#Dense-Matrix-Operations-with-mapslices","page":"Examples","title":"Dense Matrix Operations with mapslices","text":"The mapslices function applies a function to each row or column of a distributed dense matrix. This is useful for computing row-wise or column-wise statistics.","category":"section"},{"location":"examples/#Row-wise-Operations-(dims2)","page":"Examples","title":"Row-wise Operations (dims=2)","text":"Row-wise operations are local - no MPI communication is needed since rows are already distributed:\n\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# Create a deterministic dense matrix (same on all ranks)\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\n# Distribute\nAdist = HPCMatrix(A_global, backend)\n\n# Compute row statistics: for each row, compute [norm, max, sum]\n# This transforms 100×10 matrix to 100×3 matrix\nrow_stats = mapslices(x -> [norm(x), maximum(x), sum(x)], Adist; dims=2)\n\nprintln(io0(), \"Row statistics shape: $(size(row_stats))\")  # (100, 3)\n","category":"section"},{"location":"examples/#Column-wise-Operations-(dims1)","page":"Examples","title":"Column-wise Operations (dims=1)","text":"Column-wise operations require MPI communication to gather each full column:\n\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# Create a deterministic dense matrix\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\nAdist = HPCMatrix(A_global, backend)\n\n# Compute column statistics: for each column, compute [norm, max]\n# This transforms 100×10 matrix to 2×10 matrix\ncol_stats = mapslices(x -> [norm(x), maximum(x)], Adist; dims=1)\n\nprintln(io0(), \"Column statistics shape: $(size(col_stats))\")  # (2, 10)\n","category":"section"},{"location":"examples/#Use-Case:-Replacing-vcat(f.(eachrow(A))...)","page":"Examples","title":"Use Case: Replacing vcat(f.(eachrow(A))...)","text":"The standard Julia pattern vcat(f.(eachrow(A))...) doesn't work with distributed matrices because the type information is lost after broadcasting. Use mapslices instead:\n\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing LinearAlgebra\n\nbackend = BACKEND_CPU_MPI\n\n# Standard Julia pattern (for comparison):\n# A = randn(5, 2)\n# f(x) = transpose([norm(x), maximum(x)])\n# B = vcat(f.(eachrow(A))...)\n\n# MPI-compatible equivalent:\nA_global = Float64.([i + 0.1*j for i in 1:100, j in 1:10])\nAdist = HPCMatrix(A_global, backend)\n\n# Use mapslices with dims=2 to apply function to each row\n# The function returns a vector, which becomes a row in the result\ng(x) = [norm(x), maximum(x)]\nBdist = mapslices(g, Adist; dims=2)\n\nprintln(io0(), \"Result: $(size(Bdist))\")  # (100, 2)\n","category":"section"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"","category":"section"},{"location":"installation/#Prerequisites","page":"Installation","title":"Prerequisites","text":"","category":"section"},{"location":"installation/#MPI","page":"Installation","title":"MPI","text":"HPCLinearAlgebra.jl requires an MPI implementation. When you install the package, Julia automatically provides MPI.jl with MPI_jll (bundled MPI implementation).\n\nFor HPC environments, you may want to configure MPI.jl to use your system's MPI installation. See the MPI.jl documentation for details.","category":"section"},{"location":"installation/#MUMPS","page":"Installation","title":"MUMPS","text":"The package uses MUMPS for sparse direct solves. MUMPS is typically available through your system's package manager or HPC module system.","category":"section"},{"location":"installation/#Package-Installation","page":"Installation","title":"Package Installation","text":"","category":"section"},{"location":"installation/#From-GitHub","page":"Installation","title":"From GitHub","text":"using Pkg\nPkg.add(url=\"https://github.com/sloisel/HPCLinearAlgebra.jl\")","category":"section"},{"location":"installation/#Development-Installation","page":"Installation","title":"Development Installation","text":"git clone https://github.com/sloisel/HPCLinearAlgebra.jl\ncd HPCLinearAlgebra.jl\njulia --project -e 'using Pkg; Pkg.instantiate()'","category":"section"},{"location":"installation/#Verification","page":"Installation","title":"Verification","text":"Test your installation:\n\ncd HPCLinearAlgebra.jl\njulia --project -e 'using Pkg; Pkg.test()'\n\nThe test harness automatically spawns MPI processes for each test file.","category":"section"},{"location":"installation/#Initialization-Pattern","page":"Installation","title":"Initialization Pattern","text":"tip: Initialization Pattern\nInitialize MPI before using HPCLinearAlgebra:\n\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\n# Now you can use the package with BACKEND_CPU_MPI","category":"section"},{"location":"installation/#Running-MPI-Programs","page":"Installation","title":"Running MPI Programs","text":"Create a script file (e.g., my_program.jl):\n\nusing MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\n\n# Use the default MPI backend\nbackend = BACKEND_CPU_MPI\n\n# Create distributed matrix\nA = HPCSparseMatrix(sprandn(100, 100, 0.1) + 10I, backend)\nb = HPCVector(randn(100), backend)\n\n# Solve\nx = A \\ b\n\nprintln(io0(), \"Solution computed!\")\n\nRun with MPI:\n\nmpiexec -n 4 julia --project my_program.jl","category":"section"},{"location":"installation/#Troubleshooting","page":"Installation","title":"Troubleshooting","text":"","category":"section"},{"location":"installation/#MPI-Issues","page":"Installation","title":"MPI Issues","text":"If you see MPI-related errors, try rebuilding MPI.jl:\n\nusing Pkg; Pkg.build(\"MPI\")","category":"section"},{"location":"installation/#MUMPS-Issues","page":"Installation","title":"MUMPS Issues","text":"If MUMPS fails to load, ensure it's properly installed on your system.","category":"section"},{"location":"installation/#Next-Steps","page":"Installation","title":"Next Steps","text":"Once installed, proceed to the User Guide to learn how to use the package.","category":"section"},{"location":"","page":"Home","title":"Home","text":"using Markdown\nusing Pkg\nusing HPCLinearAlgebra\nv = string(pkgversion(HPCLinearAlgebra))\nmd\"# HPCLinearAlgebra.jl $v\"\n\nPure Julia distributed linear algebra with MPI.","category":"section"},{"location":"#Overview","page":"Home","title":"Overview","text":"HPCLinearAlgebra.jl provides distributed matrix and vector types for parallel computing with MPI. It offers a pure Julia implementation of distributed linear algebra, with MUMPS for sparse direct solves.","category":"section"},{"location":"#Key-Features","page":"Home","title":"Key Features","text":"Distributed Types: HPCVector, HPCMatrix, and HPCSparseMatrix for row-partitioned distributed storage\nMUMPS Solver: Direct solves using MUMPS for sparse linear systems\nRow-wise Operations: map_rows for efficient distributed row operations\nSeamless Integration: Works with standard Julia linear algebra operations\nPlan Caching: Efficient repeated operations through memoized communication plans","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"using MPI\nMPI.Init()\nusing HPCLinearAlgebra\nusing SparseArrays\n\n# Use the default MPI backend\nbackend = BACKEND_CPU_MPI\n\n# Create distributed sparse matrix\nA = HPCSparseMatrix(sprandn(100, 100, 0.1) + 10I, backend)\nb = HPCVector(randn(100), backend)\n\n# Solve linear system\nx = A \\ b\n\n# Row-wise operations\nnorms = map_rows(row -> norm(row), HPCMatrix(randn(50, 10), backend))\n\nprintln(io0(), \"Solution computed!\")\n\nRun with MPI:\n\nmpiexec -n 4 julia --project example.jl","category":"section"},{"location":"#Documentation-Contents","page":"Home","title":"Documentation Contents","text":"Pages = [\"installation.md\", \"guide.md\", \"examples.md\", \"api.md\"]\nDepth = 2","category":"section"},{"location":"#Related-Packages","page":"Home","title":"Related Packages","text":"MultiGridBarrierMPI.jl: Multigrid barrier methods using HPCLinearAlgebra\nMultiGridBarrier.jl: Core multigrid barrier method implementation\nMPI.jl: Julia MPI bindings","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.11 or later\nMPI installation (OpenMPI, MPICH, or Intel MPI)\nMUMPS for sparse direct solves (bundled via MUMPS.jl)","category":"section"},{"location":"#License","page":"Home","title":"License","text":"This package is licensed under the MIT License.","category":"section"}]
}
