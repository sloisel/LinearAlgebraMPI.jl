var documenterSearchIndex = {"docs":
[{"location":"api/#API-Reference","page":"API Reference","title":"API Reference","text":"This page documents the public API of LinearAlgebraMPI.jl.","category":"section"},{"location":"api/#Types","page":"API Reference","title":"Types","text":"","category":"section"},{"location":"api/#SparseMatrixMPI","page":"API Reference","title":"SparseMatrixMPI","text":"","category":"section"},{"location":"api/#MatrixMPI","page":"API Reference","title":"MatrixMPI","text":"","category":"section"},{"location":"api/#VectorMPI","page":"API Reference","title":"VectorMPI","text":"","category":"section"},{"location":"api/#Sparse-Matrix-Operations","page":"API Reference","title":"Sparse Matrix Operations","text":"","category":"section"},{"location":"api/#Arithmetic","page":"API Reference","title":"Arithmetic","text":"A * B          # Matrix multiplication\nA + B          # Addition\nA - B          # Subtraction\na * A          # Scalar multiplication\nA * a          # Scalar multiplication","category":"section"},{"location":"api/#Transpose-and-Adjoint","page":"API Reference","title":"Transpose and Adjoint","text":"transpose(A)   # Lazy transpose\nconj(A)        # Conjugate (new matrix)\nA'             # Adjoint (conjugate transpose, lazy)","category":"section"},{"location":"api/#Matrix-Vector-Multiplication","page":"API Reference","title":"Matrix-Vector Multiplication","text":"y = A * x      # Returns VectorMPI\nmul!(y, A, x)  # In-place version","category":"section"},{"location":"api/#Vector-Matrix-Multiplication","page":"API Reference","title":"Vector-Matrix Multiplication","text":"transpose(v) * A   # Row vector times matrix\nv' * A             # Conjugate row vector times matrix","category":"section"},{"location":"api/#Norms","page":"API Reference","title":"Norms","text":"norm(A)        # Frobenius norm (default)\nnorm(A, 1)     # Sum of absolute values\nnorm(A, Inf)   # Maximum absolute value\nnorm(A, p)     # General p-norm\n\nopnorm(A, 1)   # Maximum absolute column sum\nopnorm(A, Inf) # Maximum absolute row sum","category":"section"},{"location":"api/#Properties","page":"API Reference","title":"Properties","text":"size(A)        # Global dimensions (m, n)\nsize(A, d)     # Size along dimension d\neltype(A)      # Element type\nnnz(A)         # Number of nonzeros\nissparse(A)    # Returns true","category":"section"},{"location":"api/#Reductions","page":"API Reference","title":"Reductions","text":"sum(A)         # Sum of all stored elements\nsum(A; dims=1) # Column sums (returns VectorMPI) - SparseMatrixMPI only\nsum(A; dims=2) # Row sums (returns VectorMPI) - SparseMatrixMPI only\nmaximum(A)     # Maximum of stored values\nminimum(A)     # Minimum of stored values\ntr(A)          # Trace (sum of diagonal) - SparseMatrixMPI only","category":"section"},{"location":"api/#Element-wise-Operations","page":"API Reference","title":"Element-wise Operations","text":"abs(A)         # Absolute value\nabs2(A)        # Squared absolute value\nreal(A)        # Real part\nimag(A)        # Imaginary part\nfloor(A)       # Floor\nceil(A)        # Ceiling\nround(A)       # Round","category":"section"},{"location":"api/#Utilities","page":"API Reference","title":"Utilities","text":"copy(A)        # Deep copy\ndropzeros(A)   # Remove stored zeros\ndiag(A)        # Main diagonal (returns VectorMPI)\ndiag(A, k)     # k-th diagonal\ntriu(A)        # Upper triangular\ntriu(A, k)     # Upper triangular from k-th diagonal\ntril(A)        # Lower triangular\ntril(A, k)     # Lower triangular from k-th diagonal","category":"section"},{"location":"api/#Block-Operations","page":"API Reference","title":"Block Operations","text":"cat(A, B, C; dims=1)       # Vertical concatenation\ncat(A, B, C; dims=2)       # Horizontal concatenation\ncat(A, B, C, D; dims=(2,2)) # 2x2 block matrix [A B; C D]\nvcat(A, B, C)              # Vertical concatenation\nhcat(A, B, C)              # Horizontal concatenation\nblockdiag(A, B, C)         # Block diagonal matrix","category":"section"},{"location":"api/#Diagonal-Matrix-Construction","page":"API Reference","title":"Diagonal Matrix Construction","text":"spdiagm(v)                 # Diagonal matrix from VectorMPI\nspdiagm(m, n, v)           # m x n diagonal matrix\nspdiagm(k => v)            # k-th diagonal\nspdiagm(0 => v, 1 => w)    # Multiple diagonals","category":"section"},{"location":"api/#Dense-Matrix-Operations","page":"API Reference","title":"Dense Matrix Operations","text":"","category":"section"},{"location":"api/#Arithmetic-2","page":"API Reference","title":"Arithmetic","text":"A * x          # Matrix-vector multiplication (returns VectorMPI)\ntranspose(A)   # Lazy transpose\nconj(A)        # Conjugate\nA'             # Adjoint\na * A          # Scalar multiplication","category":"section"},{"location":"api/#mapslices","page":"API Reference","title":"mapslices","text":"Apply a function to rows or columns of a distributed dense matrix.\n\nmapslices(f, A; dims=2)   # Apply f to each row (local, no MPI)\nmapslices(f, A; dims=1)   # Apply f to each column (requires MPI)\n\nExample:\n\nusing LinearAlgebra\n\n# Create deterministic test matrix (same on all ranks)\nA_global = Float64.([i + 0.1*j for i in 1:100, j in 1:10])\nA = MatrixMPI(A_global)\n\n# Compute row statistics: norm, max, sum for each row\n# Transforms 100x10 to 100x3\nB = mapslices(x -> [norm(x), maximum(x), sum(x)], A; dims=2)","category":"section"},{"location":"api/#Block-Operations-2","page":"API Reference","title":"Block Operations","text":"cat(A, B; dims=1)          # Vertical concatenation\ncat(A, B; dims=2)          # Horizontal concatenation\nvcat(A, B)                 # Vertical concatenation\nhcat(A, B)                 # Horizontal concatenation","category":"section"},{"location":"api/#Norms-2","page":"API Reference","title":"Norms","text":"norm(A)        # Frobenius norm\nnorm(A, p)     # General p-norm\nopnorm(A, 1)   # Maximum absolute column sum\nopnorm(A, Inf) # Maximum absolute row sum","category":"section"},{"location":"api/#Vector-Operations","page":"API Reference","title":"Vector Operations","text":"","category":"section"},{"location":"api/#Arithmetic-3","page":"API Reference","title":"Arithmetic","text":"u + v          # Addition (auto-aligns partitions)\nu - v          # Subtraction\n-v             # Negation\na * v          # Scalar multiplication\nv * a          # Scalar multiplication\nv / a          # Scalar division","category":"section"},{"location":"api/#Transpose-and-Adjoint-2","page":"API Reference","title":"Transpose and Adjoint","text":"transpose(v)   # Lazy transpose (row vector)\nconj(v)        # Conjugate\nv'             # Adjoint","category":"section"},{"location":"api/#Norms-3","page":"API Reference","title":"Norms","text":"norm(v)        # 2-norm (default)\nnorm(v, 1)     # 1-norm\nnorm(v, Inf)   # Infinity norm\nnorm(v, p)     # General p-norm","category":"section"},{"location":"api/#Reductions-2","page":"API Reference","title":"Reductions","text":"sum(v)         # Sum of elements\nprod(v)        # Product of elements\nmaximum(v)     # Maximum element\nminimum(v)     # Minimum element\nmean(v)        # Mean of elements","category":"section"},{"location":"api/#Element-wise-Operations-2","page":"API Reference","title":"Element-wise Operations","text":"abs(v)         # Absolute value\nabs2(v)        # Squared absolute value\nreal(v)        # Real part\nimag(v)        # Imaginary part\ncopy(v)        # Deep copy","category":"section"},{"location":"api/#Broadcasting","page":"API Reference","title":"Broadcasting","text":"VectorMPI supports broadcasting for element-wise operations:\n\nv .+ w         # Element-wise addition\nv .* w         # Element-wise multiplication\nsin.(v)        # Apply function element-wise\nv .* 2.0 .+ w  # Compound expressions","category":"section"},{"location":"api/#Block-Operations-3","page":"API Reference","title":"Block Operations","text":"vcat(u, v, w)  # Concatenate vectors (returns VectorMPI)\nhcat(u, v, w)  # Stack as columns (returns MatrixMPI)","category":"section"},{"location":"api/#Properties-2","page":"API Reference","title":"Properties","text":"length(v)      # Global length\nsize(v)        # Returns (length,)\neltype(v)      # Element type","category":"section"},{"location":"api/#Indexing","page":"API Reference","title":"Indexing","text":"All distributed types support element access and assignment. These are collective operations - all MPI ranks must call them with the same arguments.","category":"section"},{"location":"api/#VectorMPI-Indexing","page":"API Reference","title":"VectorMPI Indexing","text":"v[i]           # Get element (collective)\nv[i] = x       # Set element (collective)\nv[1:10]        # Range indexing (returns VectorMPI)\nv[1:10] = x    # Range assignment (scalar or vector)\nv[idx]         # VectorMPI{Int} indexing (returns VectorMPI)\nv[idx] = src   # VectorMPI{Int} assignment (src::VectorMPI)","category":"section"},{"location":"api/#MatrixMPI-Indexing","page":"API Reference","title":"MatrixMPI Indexing","text":"# Single element\nA[i, j]        # Get element\nA[i, j] = x    # Set element\n\n# Range indexing (returns MatrixMPI)\nA[1:3, 2:5]    # Submatrix by ranges\nA[1:3, :]      # Row range, all columns\nA[:, 2:5]      # All rows, column range\n\n# VectorMPI indices (returns MatrixMPI)\nA[row_idx, col_idx]  # Both indices are VectorMPI{Int}\n\n# Mixed indexing (returns MatrixMPI or VectorMPI)\nA[row_idx, 1:5]      # VectorMPI rows, range columns\nA[row_idx, :]        # VectorMPI rows, all columns\nA[1:5, col_idx]      # Range rows, VectorMPI columns\nA[:, col_idx]        # All rows, VectorMPI columns\nA[row_idx, j]        # VectorMPI rows, single column (returns VectorMPI)\nA[i, col_idx]        # Single row, VectorMPI columns (returns VectorMPI)","category":"section"},{"location":"api/#SparseMatrixMPI-Indexing","page":"API Reference","title":"SparseMatrixMPI Indexing","text":"# Single element\nA[i, j]        # Get element (returns 0 for structural zeros)\nA[i, j] = x    # Set element (modifies structure if needed)\n\n# Range indexing (returns SparseMatrixMPI)\nA[1:3, 2:5]    # Submatrix by ranges\nA[1:3, :]      # Row range, all columns\nA[:, 2:5]      # All rows, column range\n\n# VectorMPI indices (returns SparseMatrixMPI)\nA[row_idx, col_idx]  # Both indices are VectorMPI{Int}\n\n# Mixed indexing (returns SparseMatrixMPI or VectorMPI)\nA[row_idx, 1:5]      # VectorMPI rows, range columns\nA[row_idx, :]        # VectorMPI rows, all columns\nA[1:5, col_idx]      # Range rows, VectorMPI columns\nA[:, col_idx]        # All rows, VectorMPI columns\nA[row_idx, j]        # VectorMPI rows, single column (returns VectorMPI)\nA[i, col_idx]        # Single row, VectorMPI columns (returns VectorMPI)","category":"section"},{"location":"api/#setindex!-Source-Types","page":"API Reference","title":"setindex! Source Types","text":"For setindex! operations, the source type depends on the indexing pattern:\n\nIndex Pattern Source Type\nA[i, j] = x Scalar\nA[range, range] = x Scalar, Matrix, or distributed matrix\nA[VectorMPI, VectorMPI] = src MatrixMPI (matching partitions)\nA[VectorMPI, range] = src MatrixMPI\nA[range, VectorMPI] = src MatrixMPI\nA[VectorMPI, j] = src VectorMPI\nA[i, VectorMPI] = src VectorMPI","category":"section"},{"location":"api/#Utility-Functions","page":"API Reference","title":"Utility Functions","text":"","category":"section"},{"location":"api/#Partition-Computation","page":"API Reference","title":"Partition Computation","text":"","category":"section"},{"location":"api/#Rank-Selective-Output","page":"API Reference","title":"Rank-Selective Output","text":"","category":"section"},{"location":"api/#Gathering-Distributed-Data","page":"API Reference","title":"Gathering Distributed Data","text":"Convert distributed MPI types to standard Julia types (gathers data to all ranks):\n\nVector(v::VectorMPI)              # Gather to Vector\nMatrix(A::MatrixMPI)              # Gather to Matrix\nSparseMatrixCSC(A::SparseMatrixMPI) # Gather to SparseMatrixCSC\n\nThese conversions enable show and string interpolation:\n\nprintln(io0(), \"Result: $v\")      # Works with VectorMPI\nprintln(io0(), \"Matrix: $A\")      # Works with MatrixMPI/SparseMatrixMPI","category":"section"},{"location":"api/#Local-Constructors","page":"API Reference","title":"Local Constructors","text":"Create distributed types from local data (each rank provides only its portion):","category":"section"},{"location":"api/#Factorization","page":"API Reference","title":"Factorization","text":"LinearAlgebraMPI provides distributed sparse direct solvers using the multifrontal method.","category":"section"},{"location":"api/#LU-Factorization","page":"API Reference","title":"LU Factorization","text":"F = lu(A::SparseMatrixMPI{T}; reuse_symbolic=true) -> LUFactorizationMPI{T}\n\nCompute LU factorization of a distributed sparse matrix using the multifrontal method with:\n\nAMD fill-reducing ordering\nPartial pivoting for numerical stability\nMUMPS-style subtree-to-rank mapping\n\nIf reuse_symbolic=true (default), caches and reuses symbolic factorization for matrices with the same sparsity pattern.","category":"section"},{"location":"api/#LDLT-Factorization","page":"API Reference","title":"LDLT Factorization","text":"F = ldlt(A::SparseMatrixMPI{T}; reuse_symbolic=true) -> LDLTFactorizationMPI{T}\n\nCompute LDLT factorization of a distributed symmetric sparse matrix using the multifrontal method with:\n\nAMD fill-reducing ordering\nBunch-Kaufman pivoting for numerical stability with indefinite matrices\nMUMPS-style subtree-to-rank mapping\n\nThe factorization computes P' * L * D * L^T * P which equals the symmetrically permuted matrix (A reordered by the fill-reducing permutation perm).\n\nNote: Uses transpose (L^T), not adjoint (L*). Correct for real symmetric and complex symmetric matrices, but NOT for complex Hermitian matrices.","category":"section"},{"location":"api/#Factorization-Types","page":"API Reference","title":"Factorization Types","text":"","category":"section"},{"location":"api/#Solving-Linear-Systems","page":"API Reference","title":"Solving Linear Systems","text":"","category":"section"},{"location":"api/#Usage-Examples","page":"API Reference","title":"Usage Examples","text":"using LinearAlgebraMPI\nusing LinearAlgebra\nusing SparseArrays\n\n# Create a distributed sparse matrix\nA_local = sprand(1000, 1000, 0.01) + 10I\nA_local = A_local + A_local'  # Make symmetric\nA = SparseMatrixMPI{Float64}(A_local)\n\n# LDLT factorization (for symmetric matrices)\nF = ldlt(A)\n\n# Solve Ax = b\nb = VectorMPI(ones(1000))\nx = solve(F, b)\n\n# Or use backslash\nx = F \\ b\n\n# For non-symmetric matrices, use LU\nA_nonsym = SparseMatrixMPI{Float64}(sprand(1000, 1000, 0.01) + 10I)\nF_lu = lu(A_nonsym)\nx = solve(F_lu, b)","category":"section"},{"location":"api/#Direct-Solve-Syntax","page":"API Reference","title":"Direct Solve Syntax","text":"Both left division (\\) and right division (/) are supported:\n\n# Left division: solve A*x = b\nx = A \\ b\nx = transpose(A) \\ b    # solve transpose(A)*x = b\nx = A' \\ b              # solve A'*x = b\n\n# Right division: solve x*A = b (for row vectors)\nx = transpose(b) / A           # solve x*A = transpose(b)\nx = transpose(b) / transpose(A)  # solve x*transpose(A) = transpose(b)\nx = b' / A                     # solve x*A = b'\nx = b' / A'                    # solve x*A' = b'","category":"section"},{"location":"api/#Plan-Reuse","page":"API Reference","title":"Plan Reuse","text":"The symbolic factorization (fill-reducing ordering, elimination tree, supernode detection) is cached and reused for matrices with the same sparsity pattern:\n\n# First factorization computes symbolic phase\nF1 = ldlt(A1; reuse_symbolic=true)\n\n# Second factorization with same structure reuses symbolic phase\nA2 = SparseMatrixMPI{Float64}(A2_local)  # Same structure, different values\nF2 = ldlt(A2; reuse_symbolic=true)  # Faster - reuses cached symbolic","category":"section"},{"location":"api/#Cache-Management","page":"API Reference","title":"Cache Management","text":"","category":"section"},{"location":"api/#Distributed-Solve-(Advanced)","page":"API Reference","title":"Distributed Solve (Advanced)","text":"These functions provide direct access to the distributed solve implementation. Most users should use solve or \\ instead.","category":"section"},{"location":"api/#Full-API-Index","page":"API Reference","title":"Full API Index","text":"","category":"section"},{"location":"api/#LinearAlgebraMPI.SparseMatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.SparseMatrixMPI","text":"SparseMatrixMPI{T}\n\nA distributed sparse matrix partitioned by rows across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (placeholder for transpose)\ncol_indices::Vector{Int}: Global column indices that appear in the local part (local→global mapping)\nA::Transpose{T,SparseMatrixCSC{T,Int}}: Local rows as a lazy transpose wrapper around compressed CSC storage\n\nInvariants\n\ncol_indices, row_partition, and col_partition are sorted\nrow_partition[nranks+1] = total number of rows\ncol_partition[nranks+1] = total number of columns\nsize(A, 1) == row_partition[rank+1] - row_partition[rank] (number of local rows)\nsize(A.parent, 1) == length(col_indices) (compressed column dimension)\nA.parent.rowval contains local indices in 1:length(col_indices)\n\nStorage Details\n\nThe local rows are stored in compressed form as A = transpose(AT) where AT::SparseMatrixCSC has:\n\nAT.m = length(col_indices) (compressed, not global ncols)\nAT.n = number of local rows\nAT.rowval contains LOCAL column indices (1:length(col_indices))\ncol_indices[local_idx] maps local→global column indices\n\nThis compression avoids \"hypersparse\" storage where AT.m >> length(unique(AT.rowval)), which would cause excessive allocations in matrix operations.\n\nAccess the underlying CSC via A.parent when needed for low-level operations.\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.MatrixMPI","page":"API Reference","title":"LinearAlgebraMPI.MatrixMPI","text":"MatrixMPI{T}\n\nA distributed dense matrix partitioned by rows across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the structural pattern\nrow_partition::Vector{Int}: Row partition boundaries, length = nranks + 1\ncol_partition::Vector{Int}: Column partition boundaries, length = nranks + 1 (for transpose)\nA::Matrix{T}: Local rows (NOT transposed), size = (local_nrows, ncols)\n\nInvariants\n\nrow_partition and col_partition are sorted\nrow_partition[nranks+1] = total number of rows + 1\ncol_partition[nranks+1] = total number of columns + 1\nsize(A, 1) == row_partition[rank+2] - row_partition[rank+1]\nsize(A, 2) == col_partition[end] - 1\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.VectorMPI","page":"API Reference","title":"LinearAlgebraMPI.VectorMPI","text":"VectorMPI{T}\n\nA distributed dense vector partitioned across MPI ranks.\n\nFields\n\nstructural_hash::Blake3Hash: 256-bit Blake3 hash of the partition\npartition::Vector{Int}: Partition boundaries, length = nranks + 1\nv::Vector{T}: Local vector elements owned by this rank\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.mean","page":"API Reference","title":"LinearAlgebraMPI.mean","text":"mean(v::VectorMPI{T}) where T\n\nCompute the mean of all elements in the distributed vector.\n\n\n\n\n\nmean(A::SparseMatrixMPI{T}) where T\n\nCompute the mean of all elements (including implicit zeros) in the distributed sparse matrix.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.uniform_partition","page":"API Reference","title":"LinearAlgebraMPI.uniform_partition","text":"uniform_partition(n::Int, nranks::Int) -> Vector{Int}\n\nCompute a balanced partition of n elements across nranks ranks. Returns a vector of length nranks + 1 with 1-indexed partition boundaries.\n\nThe first mod(n, nranks) ranks get div(n, nranks) + 1 elements, the remaining ranks get div(n, nranks) elements.\n\nExample\n\npartition = uniform_partition(10, 4)  # [1, 4, 7, 9, 11]\n# Rank 0: 1:3 (3 elements)\n# Rank 1: 4:6 (3 elements)\n# Rank 2: 7:8 (2 elements)\n# Rank 3: 9:10 (2 elements)\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.io0","page":"API Reference","title":"LinearAlgebraMPI.io0","text":"io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)\n\nReturn io if the current MPI rank is in set r, otherwise return dn (default: devnull).\n\nThis is useful for printing only from specific ranks:\n\nprintln(io0(), \"Hello from rank 0!\")\nprintln(io0(r=Set([0,1])), \"Hello from ranks 0 and 1!\")\n\nWith string interpolation:\n\nprintln(io0(), \"Matrix A = $A\")\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.VectorMPI_local","page":"API Reference","title":"LinearAlgebraMPI.VectorMPI_local","text":"VectorMPI_local(v_local::Vector{T}, comm::MPI.Comm=MPI.COMM_WORLD) where T\n\nCreate a VectorMPI from a local vector on each rank.\n\nUnlike VectorMPI(v_global) which takes a global vector and partitions it, this constructor takes only the local portion of the vector that each rank owns. The partition is computed by gathering the local sizes from all ranks.\n\nExample\n\n# Rank 0 has [1.0, 2.0], Rank 1 has [3.0, 4.0, 5.0]\nv = VectorMPI_local([1.0, 2.0])  # on rank 0\nv = VectorMPI_local([3.0, 4.0, 5.0])  # on rank 1\n# Result: distributed vector [1.0, 2.0, 3.0, 4.0, 5.0] with partition [1, 3, 6]\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.MatrixMPI_local","page":"API Reference","title":"LinearAlgebraMPI.MatrixMPI_local","text":"MatrixMPI_local(A_local::Matrix{T}; comm=MPI.COMM_WORLD, col_partition=...) where T\n\nCreate a MatrixMPI from a local matrix on each rank.\n\nUnlike MatrixMPI(M_global) which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.\n\nAll ranks must have local matrices with the same number of columns. A collective error is raised if the column counts don't match.\n\nKeyword Arguments\n\ncomm::MPI.Comm: MPI communicator (default: MPI.COMM_WORLD)\ncol_partition::Vector{Int}: Column partition boundaries (default: uniform_partition(size(A_local,2), nranks))\n\nExample\n\n# Rank 0 has 2×3 matrix, Rank 1 has 3×3 matrix\nA = MatrixMPI_local(randn(2, 3))  # on rank 0\nA = MatrixMPI_local(randn(3, 3))  # on rank 1\n# Result: 5×3 distributed matrix with row_partition [1, 3, 6]\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.SparseMatrixMPI_local","page":"API Reference","title":"LinearAlgebraMPI.SparseMatrixMPI_local","text":"SparseMatrixMPI_local(A_local::Transpose{T,SparseMatrixCSC{T,Int}}; comm=MPI.COMM_WORLD, col_partition=...) where T\nSparseMatrixMPI_local(A_local::Adjoint{T,SparseMatrixCSC{T,Int}}; comm=MPI.COMM_WORLD, col_partition=...) where T\n\nCreate a SparseMatrixMPI from a local sparse matrix on each rank.\n\nUnlike SparseMatrixMPI{T}(A_global) which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.\n\nThe input A_local must be a Transpose (or Adjoint) of a SparseMatrixCSC{T,Int} where:\n\nA_local.parent.n = number of local rows on this rank\nA_local.parent.m = global number of columns (must match on all ranks)\nA_local.parent.rowval = global column indices\n\nAll ranks must have local matrices with the same number of columns (block widths must match). A collective error is raised if the column counts don't match.\n\nNote: For Adjoint inputs, the values are conjugated to match the adjoint semantics.\n\nKeyword Arguments\n\ncomm::MPI.Comm: MPI communicator (default: MPI.COMM_WORLD)\ncol_partition::Vector{Int}: Column partition boundaries (default: uniform_partition(A_local.parent.m, nranks))\n\nExample\n\n# Create local rows as transpose of CSC storage\n# Rank 0 owns rows 1-2 of a 5×3 matrix, Rank 1 owns rows 3-5\nlocal_AT = sparse([1, 2, 3], [1, 1, 2], [1.0, 2.0, 3.0], 3, 2)  # 3 cols, 2 local rows\nA = SparseMatrixMPI_local(transpose(local_AT))\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.LUFactorizationMPI","page":"API Reference","title":"LinearAlgebraMPI.LUFactorizationMPI","text":"LUFactorizationMPI{T}\n\nDistributed LU factorization result.\n\nThe factorization satisfies: P_row * Ap = L * U where:\n\nAp is A symmetrically permuted by perm (the fill-reducing permutation)\nP_row is the row permutation from partial pivoting\nL is unit lower triangular (in elimination order)\nU is upper triangular (in elimination order)\n\nEach MPI rank stores the L and U columns/rows for supernodes it owns. L and U are stored with GLOBAL indices (1 to n referencing Ap rows/cols).\n\nSolve sequence\n\nApply fill-reducing perm: work = b[perm]\nApply row pivot perm: work2[k] = work[row_perm[k]]\nForward solve (in elim order): process columns in elimtoglobal order\nBackward solve (in reverse elim order): process columns in reverse elimtoglobal order\nApply inverse row pivot perm\nApply inverse fill-reducing perm\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.LDLTFactorizationMPI","page":"API Reference","title":"LinearAlgebraMPI.LDLTFactorizationMPI","text":"LDLTFactorizationMPI{T}\n\nDistributed LDLT factorization result for symmetric matrices.\n\nThe factorization satisfies: Ap = P' * L * D * Lᵀ * P where:\n\nAp is A symmetrically permuted by perm (the fill-reducing permutation)\nP is the symmetric permutation from Bunch-Kaufman pivoting\nL is unit lower triangular (in elimination order)\nD is block diagonal (1×1 and 2×2 blocks for indefinite matrices)\nLᵀ is the transpose (NOT adjoint) of L\n\nComplex number handling\n\nUses transpose (Lᵀ), not adjoint (L*). This is correct for:\n\nReal symmetric matrices (A = Aᵀ)\nComplex symmetric matrices (A = Aᵀ, rare but used in some physics applications)\n\nNOT correct for complex Hermitian matrices (A = A*). Use LU factorization instead.\n\nPivoting information\n\npivots[k] > 0: 1×1 pivot at elimination step k\npivots[k] < 0: 2×2 pivot starting at elimination step k (paired with k+1)\n\nSolve sequence\n\nApply fill-reducing perm: work = b[perm]\nApply symmetric pivot perm: work2 = work[sym_perm]\nForward solve with L\nDiagonal solve with D (handling 2×2 blocks)\nBackward solve with Lᵀ\nApply inverse symmetric perm\nApply inverse fill-reducing perm\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.SymbolicFactorization","page":"API Reference","title":"LinearAlgebraMPI.SymbolicFactorization","text":"SymbolicFactorization\n\nResult of symbolic factorization phase. Computed once per sparsity pattern and cached for reuse. Contains the elimination tree structure and MPI rank assignments.\n\nThe symbolic factorization is identical across all MPI ranks.\n\n\n\n\n\n","category":"type"},{"location":"api/#LinearAlgebraMPI.solve","page":"API Reference","title":"LinearAlgebraMPI.solve","text":"solve(F::LUFactorizationMPI{T}, b::VectorMPI{T}) -> VectorMPI{T}\n\nSolve the linear system A*x = b using the precomputed LU factorization.\n\n\n\n\n\nsolve(F::LDLTFactorizationMPI{T}, b::VectorMPI{T}) -> VectorMPI{T}\n\nSolve the linear system A*x = b using the precomputed LDLT factorization.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.solve!","page":"API Reference","title":"LinearAlgebraMPI.solve!","text":"solve!(x::VectorMPI{T}, F::LUFactorizationMPI{T}, b::VectorMPI{T}; distributed::Bool=true)\n\nSolve A*x = b in-place using LU factorization.\n\nBy default uses distributed solve (MUMPS-style) that keeps factors distributed and only communicates at subtree boundaries. Set distributed=false to use the gathered solve which collects L/U to all ranks (useful for debugging).\n\n\n\n\n\nsolve!(x::VectorMPI{T}, F::LDLTFactorizationMPI{T}, b::VectorMPI{T}; distributed::Bool=true)\n\nSolve A*x = b in-place using LDLT factorization.\n\nBy default uses distributed solve (MUMPS-style) that keeps factors distributed and only communicates at subtree boundaries. Set distributed=false to use the gathered solve which collects L/D to all ranks (useful for debugging).\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.solve_transpose","page":"API Reference","title":"LinearAlgebraMPI.solve_transpose","text":"solve_transpose(F::LUFactorizationMPI{T}, b::VectorMPI{T}) -> VectorMPI{T}\n\nSolve transpose(A)*x = b using the precomputed LU factorization of A.\n\n\n\n\n\nsolve_transpose(F::LDLTFactorizationMPI{T}, b::VectorMPI{T}) -> VectorMPI{T}\n\nSolve transpose(A)*x = b. For symmetric matrices, transpose(A) = A, so this is equivalent to solve(F, b).\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.solve_adjoint","page":"API Reference","title":"LinearAlgebraMPI.solve_adjoint","text":"solve_adjoint(F::LUFactorizationMPI{T}, b::VectorMPI{T}) -> VectorMPI{T}\n\nSolve A'*x = b (adjoint/conjugate transpose) using the precomputed LU factorization of A.\n\n\n\n\n\nsolve_adjoint(F::LDLTFactorizationMPI{T}, b::VectorMPI{T}) -> VectorMPI{T}\n\nSolve A'*x = b (adjoint/conjugate transpose). For real symmetric matrices, A' = A, so this is equivalent to solve(F, b). For complex symmetric matrices, A' = conj(A), so we conjugate during the solve.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.clear_plan_cache!","page":"API Reference","title":"LinearAlgebraMPI.clear_plan_cache!","text":"clear_plan_cache!()\n\nClear all memoized plan caches (including factorization caches).\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.clear_symbolic_cache!","page":"API Reference","title":"LinearAlgebraMPI.clear_symbolic_cache!","text":"clear_symbolic_cache!()\n\nClear the symbolic factorization cache.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.clear_solve_plan_cache!","page":"API Reference","title":"LinearAlgebraMPI.clear_solve_plan_cache!","text":"clear_solve_plan_cache!()\n\nClear the solve plan cache.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.clear_input_plan_cache!","page":"API Reference","title":"LinearAlgebraMPI.clear_input_plan_cache!","text":"clear_input_plan_cache!()\n\nClear the factorization input plan cache.\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.distributed_solve_lu!","page":"API Reference","title":"LinearAlgebraMPI.distributed_solve_lu!","text":"distributed_solve_lu!(x::VectorMPI{T}, F::LUFactorizationMPI{T}, b::VectorMPI{T}) where T\n\nSolve A*x = b using distributed LU factorization without gathering factors.\n\nSteps:\n\nGather b to get components needed for this rank's supernodes\nApply permutations to get b in elimination order with pivoting\nDistributed forward solve: L y = permuted_b\nDistributed backward solve: U z = y\nApply inverse permutations\nDistribute result to x\n\n\n\n\n\n","category":"function"},{"location":"api/#LinearAlgebraMPI.distributed_solve_ldlt!","page":"API Reference","title":"LinearAlgebraMPI.distributed_solve_ldlt!","text":"distributed_solve_ldlt!(x::VectorMPI{T}, F::LDLTFactorizationMPI{T}, b::VectorMPI{T}) where T\n\nSolve A*x = b using distributed LDLT factorization without gathering factors.\n\nSteps:\n\nGather b components needed for this rank's supernodes\nApply fill-reducing and symmetric pivot permutations\nDistributed forward solve: L y = permuted_b\nDistributed diagonal solve: D z = y\nDistributed backward solve: L^T w = z\nApply inverse permutations\nDistribute result to x\n\n\n\n\n\n","category":"function"},{"location":"getting-started/#Getting-Started","page":"Getting Started","title":"Getting Started","text":"This guide will walk you through the basics of using LinearAlgebraMPI.jl for distributed sparse matrix computations.","category":"section"},{"location":"getting-started/#Prerequisites","page":"Getting Started","title":"Prerequisites","text":"Before using LinearAlgebraMPI.jl, ensure you have:\n\nA working MPI installation (OpenMPI, MPICH, or Intel MPI)\nMPI.jl configured to use your MPI installation\n\nYou can verify your MPI setup with:\n\nusing MPI\nMPI.Init()\nprintln(\"Rank $(MPI.Comm_rank(MPI.COMM_WORLD)) of $(MPI.Comm_size(MPI.COMM_WORLD))\")\nMPI.Finalize()\n\nRun with:\n\nmpiexec -n 4 julia --project=. your_script.jl","category":"section"},{"location":"getting-started/#Creating-Distributed-Matrices","page":"Getting Started","title":"Creating Distributed Matrices","text":"","category":"section"},{"location":"getting-started/#From-a-Global-Sparse-Matrix","page":"Getting Started","title":"From a Global Sparse Matrix","text":"The most common way to create a distributed matrix is from an existing SparseMatrixCSC:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create a sparse matrix - MUST be identical on all ranks\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\n\n# Distribute across MPI ranks\nAdist = SparseMatrixMPI{Float64}(A)\n\nImportant: All MPI ranks must have the same matrix size when constructing distributed types. However, each rank only extracts its own local rows, so the actual data only needs to be correct for each rank's portion.","category":"section"},{"location":"getting-started/#Understanding-Row-Partitioning","page":"Getting Started","title":"Understanding Row Partitioning","text":"The matrix is partitioned roughly equally by rows. For example, with 4 ranks and a 100x100 matrix:\n\nRank 0: rows 1-25\nRank 1: rows 26-50\nRank 2: rows 51-75\nRank 3: rows 76-100","category":"section"},{"location":"getting-started/#Efficient-Local-Only-Construction","page":"Getting Started","title":"Efficient Local-Only Construction","text":"For large matrices, you can avoid replicating data across all ranks by only populating each rank's local portion:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Global dimensions\nm, n = 1000, 1000\n\n# Compute which rows this rank owns\nrows_per_rank = div(m, nranks)\nremainder = mod(m, nranks)\nmy_row_start = 1 + rank * rows_per_rank + min(rank, remainder)\nmy_row_end = my_row_start + rows_per_rank - 1 + (rank < remainder ? 1 : 0)\n\n# Create a sparse matrix with correct size, but only populate local rows\nI, J, V = Int[], Int[], Float64[]\nfor i in my_row_start:my_row_end\n    # Example: tridiagonal matrix\n    if i > 1\n        push!(I, i); push!(J, i-1); push!(V, -1.0)\n    end\n    push!(I, i); push!(J, i); push!(V, 2.0)\n    if i < m\n        push!(I, i); push!(J, i+1); push!(V, -1.0)\n    end\nend\nA = sparse(I, J, V, m, n)\n\n# The constructor extracts only local rows - other rows are ignored\nAdist = SparseMatrixMPI{Float64}(A)\n\nThis pattern is useful when:\n\nThe global matrix is too large to fit in memory on each rank\nYou're generating matrix entries programmatically\nYou want to minimize memory usage during construction","category":"section"},{"location":"getting-started/#Basic-Operations","page":"Getting Started","title":"Basic Operations","text":"","category":"section"},{"location":"getting-started/#Matrix-Multiplication","page":"Getting Started","title":"Matrix Multiplication","text":"# Both matrices must be distributed\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Multiply\nCdist = Adist * Bdist\n\nThe multiplication automatically handles the necessary communication between ranks.","category":"section"},{"location":"getting-started/#Addition-and-Subtraction","page":"Getting Started","title":"Addition and Subtraction","text":"Cdist = Adist + Bdist\nDdist = Adist - Bdist\n\nIf A and B have different row partitions, B's rows are redistributed to match A's partition.","category":"section"},{"location":"getting-started/#Scalar-Multiplication","page":"Getting Started","title":"Scalar Multiplication","text":"Cdist = 2.5 * Adist\nCdist = Adist * 2.5  # Equivalent","category":"section"},{"location":"getting-started/#Transpose","page":"Getting Started","title":"Transpose","text":"# Transpose is lazy (no communication until needed)\nAt = transpose(Adist)\n\n# Use in multiplication - automatically materializes when needed\nCdist = At * Bdist","category":"section"},{"location":"getting-started/#Adjoint-(Conjugate-Transpose)","page":"Getting Started","title":"Adjoint (Conjugate Transpose)","text":"For complex matrices:\n\nAdist = SparseMatrixMPI{ComplexF64}(A)\nAadj = Adist'  # Conjugate transpose (lazy)","category":"section"},{"location":"getting-started/#Computing-Norms","page":"Getting Started","title":"Computing Norms","text":"# Frobenius norm (default)\nf_norm = norm(Adist)\n\n# 1-norm (sum of absolute values)\none_norm = norm(Adist, 1)\n\n# Infinity norm (maximum absolute value)\ninf_norm = norm(Adist, Inf)\n\n# General p-norm\np_norm = norm(Adist, 3)\n\n# Operator norms\ncol_norm = opnorm(Adist, 1)   # Max column sum\nrow_norm = opnorm(Adist, Inf) # Max row sum","category":"section"},{"location":"getting-started/#Running-MPI-Programs","page":"Getting Started","title":"Running MPI Programs","text":"","category":"section"},{"location":"getting-started/#Command-Line","page":"Getting Started","title":"Command Line","text":"mpiexec -n 4 julia --project=. my_program.jl","category":"section"},{"location":"getting-started/#Program-Structure","page":"Getting Started","title":"Program Structure","text":"A typical LinearAlgebraMPI.jl program follows this pattern:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\nnranks = MPI.Comm_size(comm)\n\n# Create matrices (identical on all ranks)\nA = create_my_matrix()  # Your matrix creation function\nB = create_my_matrix()\n\n# Distribute\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Compute\nCdist = Adist * Bdist\n\n# Get results (e.g., norm is computed globally)\nresult_norm = norm(Cdist)\n\nif rank == 0\n    println(\"Result norm: $result_norm\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"getting-started/#Performance-Tips","page":"Getting Started","title":"Performance Tips","text":"","category":"section"},{"location":"getting-started/#Reuse-Communication-Plans","page":"Getting Started","title":"Reuse Communication Plans","text":"For repeated operations with the same sparsity pattern, LinearAlgebraMPI.jl automatically caches communication plans:\n\n# First multiplication creates and caches the plan\nC1 = Adist * Bdist\n\n# Subsequent multiplications with same A, B reuse the cached plan\nC2 = Adist * Bdist  # Uses cached plan - much faster","category":"section"},{"location":"getting-started/#Clear-Cache-When-Done","page":"Getting Started","title":"Clear Cache When Done","text":"If you're done with a set of matrices and want to free memory:\n\nclear_plan_cache!()","category":"section"},{"location":"getting-started/#Use-Deterministic-Test-Data","page":"Getting Started","title":"Use Deterministic Test Data","text":"For testing with the simple \"replicate everywhere\" pattern, avoid random matrices since they'll differ across ranks:\n\n# Bad - different random values on each rank\nA = sprand(100, 100, 0.01)\n\n# Good - deterministic formula, same on all ranks\nI = [1:100; 1:99; 2:100]\nJ = [1:100; 2:100; 1:99]\nV = [2.0*ones(100); -0.5*ones(99); -0.5*ones(99)]\nA = sparse(I, J, V, 100, 100)\n\nAlternatively, use the local-only construction pattern where each rank generates only its own rows.","category":"section"},{"location":"getting-started/#Next-Steps","page":"Getting Started","title":"Next Steps","text":"See Examples for more detailed usage examples\nRead the API Reference for complete function documentation","category":"section"},{"location":"examples/#Examples","page":"Examples","title":"Examples","text":"This page provides detailed examples of using LinearAlgebraMPI.jl for various distributed sparse matrix operations.","category":"section"},{"location":"examples/#Matrix-Multiplication","page":"Examples","title":"Matrix Multiplication","text":"","category":"section"},{"location":"examples/#Square-Matrices","page":"Examples","title":"Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\n\n# Create a tridiagonal matrix (same on all ranks)\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Create another tridiagonal matrix\nV2 = [1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]\nB = sparse(I, J, V2, n, n)\n\n# Distribute matrices\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Multiply\nCdist = Adist * Bdist\n\n# Verify against reference\nC_ref = A * B\nC_ref_dist = SparseMatrixMPI{Float64}(C_ref)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nif rank == 0\n    println(\"Multiplication error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Non-Square-Matrices","page":"Examples","title":"Non-Square Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# A is 6x8, B is 8x10, result is 6x10\nm, k, n = 6, 8, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nJ_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 2]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, k)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, k, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\nCdist = Adist * Bdist\n\n@assert size(Cdist) == (m, n)\n\nif rank == 0\n    println(\"Result size: $(size(Cdist))\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Complex-Matrices","page":"Examples","title":"Complex Matrices","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 8\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# Complex values\nV_A = ComplexF64.([2.0*ones(n); -0.5*ones(n-1); -0.5*ones(n-1)]) .+\n      im .* ComplexF64.([0.1*ones(n); 0.2*ones(n-1); -0.2*ones(n-1)])\nA = sparse(I, J, V_A, n, n)\n\nV_B = ComplexF64.([1.5*ones(n); 0.25*ones(n-1); 0.25*ones(n-1)]) .+\n      im .* ComplexF64.([-0.1*ones(n); 0.1*ones(n-1); 0.1*ones(n-1)])\nB = sparse(I, J, V_B, n, n)\n\nAdist = SparseMatrixMPI{ComplexF64}(A)\nBdist = SparseMatrixMPI{ComplexF64}(B)\n\n# Multiplication\nCdist = Adist * Bdist\n\n# Conjugate\nAconj = conj(Adist)\n\n# Adjoint (conjugate transpose) - returns lazy wrapper\nAadj = Adist'\n\n# Using adjoint in multiplication (materializes automatically)\nresult = Aadj * Bdist\n\nif rank == 0\n    println(\"Complex matrix operations completed\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Addition-and-Subtraction","page":"Examples","title":"Addition and Subtraction","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 8\n\n# Matrices with different sparsity patterns\n# Matrix A: upper triangular entries\nI_A = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nJ_A = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nV_A = Float64.(1:9)\nA = sparse(I_A, J_A, V_A, n, n)\n\n# Matrix B: lower triangular entries\nI_B = [1, 2, 2, 3, 4, 5, 6, 7, 8]\nJ_B = [1, 1, 2, 3, 4, 5, 6, 7, 8]\nV_B = Float64.(9:-1:1)\nB = sparse(I_B, J_B, V_B, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Addition - handles different sparsity patterns\nCdist = Adist + Bdist\n\n# Subtraction\nDdist = Adist - Bdist\n\n# Verify\nC_ref_dist = SparseMatrixMPI{Float64}(A + B)\nerr = norm(Cdist - C_ref_dist, Inf)\n\nif rank == 0\n    println(\"Addition error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Transpose-Operations","page":"Examples","title":"Transpose Operations","text":"","category":"section"},{"location":"examples/#Lazy-Transpose","page":"Examples","title":"Lazy Transpose","text":"The transpose function creates a lazy wrapper without transposing the data. This is efficient because the actual transpose is only computed when needed:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 8, 6\nI_C = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nJ_C = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\nV_C = Float64.(1:length(I_C))\nC = sparse(I_C, J_C, V_C, m, n)\n\nI_D = [1, 2, 3, 4, 5, 6, 1, 2]\nJ_D = [1, 2, 3, 4, 5, 6, 7, 8]\nV_D = Float64.(1:length(I_D))\nD = sparse(I_D, J_D, V_D, n, m)\n\nCdist = SparseMatrixMPI{Float64}(C)\nDdist = SparseMatrixMPI{Float64}(D)\n\n# transpose(C) * transpose(D) = transpose(D * C)\n# This is computed efficiently without explicitly transposing\nresult = transpose(Cdist) * transpose(Ddist)\n\nif rank == 0\n    println(\"Lazy transpose multiplication completed\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Transpose-in-Multiplication","page":"Examples","title":"Transpose in Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# A is 8x6, so A' is 6x8\n# B is 8x10, so A' * B is 6x10\nm, n, p = 8, 6, 10\n\nI_A = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_A = [1, 2, 3, 4, 5, 6, 1, 2, 3, 4, 5, 6]\nV_A = Float64.(1:length(I_A))\nA = sparse(I_A, J_A, V_A, m, n)\n\nI_B = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3, 5, 7]\nJ_B = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2]\nV_B = Float64.(1:length(I_B))\nB = sparse(I_B, J_B, V_B, m, p)\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# transpose(A) * B - A is automatically materialized as transpose\nresult_dist = transpose(Adist) * Bdist\n\n# Verify\nref = sparse(A') * B\nref_dist = SparseMatrixMPI{Float64}(ref)\nerr = norm(result_dist - ref_dist, Inf)\n\nif rank == 0\n    println(\"transpose(A) * B error: $err\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Scalar-Multiplication","page":"Examples","title":"Scalar Multiplication","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3]\nJ = [1, 2, 3, 4, 5, 6, 7, 8]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Scalar times matrix\na = 2.5\nresult1 = a * Adist\nresult2 = Adist * a  # Equivalent\n\n# Scalar times lazy transpose\nAt = transpose(Adist)\nresult3 = a * At  # Returns transpose(a * A)\n\n# Verify\nref_dist = SparseMatrixMPI{Float64}(a * A)\nerr1 = norm(result1 - ref_dist, Inf)\nerr2 = norm(result2 - ref_dist, Inf)\n\nif rank == 0\n    println(\"Scalar multiplication errors: $err1, $err2\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Computing-Norms","page":"Examples","title":"Computing Norms","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nm, n = 6, 8\nI = [1, 2, 3, 4, 5, 6, 1, 3, 2, 4]\nJ = [1, 2, 3, 4, 5, 6, 7, 8, 1, 3]\nV = Float64.(1:length(I))\nA = sparse(I, J, V, m, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Element-wise norms (treating matrix as vector)\nfrob_norm = norm(Adist)        # Frobenius (2-norm)\none_norm = norm(Adist, 1)      # Sum of absolute values\ninf_norm = norm(Adist, Inf)    # Max absolute value\np_norm = norm(Adist, 3)        # General p-norm\n\n# Operator norms\nop_1 = opnorm(Adist, 1)        # Max absolute column sum\nop_inf = opnorm(Adist, Inf)    # Max absolute row sum\n\nif rank == 0\n    println(\"Frobenius norm: $frob_norm\")\n    println(\"1-norm: $one_norm\")\n    println(\"Inf-norm: $inf_norm\")\n    println(\"3-norm: $p_norm\")\n    println(\"Operator 1-norm: $op_1\")\n    println(\"Operator Inf-norm: $op_inf\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Iterative-Methods-Example","page":"Examples","title":"Iterative Methods Example","text":"Here's an example of using LinearAlgebraMPI.jl for power iteration to find the dominant eigenvalue:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\n\n# Create a symmetric positive definite matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\n\n# For power iteration, we need matrix-vector products\n# Currently LinearAlgebraMPI focuses on matrix-matrix products\n# This example shows how to use A*A for related computations\n\n# Compute A^2\nA2dist = Adist * Adist\n\n# Compute the Frobenius norm of A^2\nnorm_A2 = norm(A2dist)\n\nif rank == 0\n    println(\"||A^2||_F = $norm_A2\")\n    # For SPD matrices, this relates to the eigenvalues\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Solving-Linear-Systems","page":"Examples","title":"Solving Linear Systems","text":"LinearAlgebraMPI provides distributed sparse direct solvers using the multifrontal method.","category":"section"},{"location":"examples/#LDLT-Factorization-(Symmetric-Matrices)","page":"Examples","title":"LDLT Factorization (Symmetric Matrices)","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\n\n# Create a symmetric positive definite tridiagonal matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\n# Distribute the matrix\nAdist = SparseMatrixMPI{Float64}(A)\n\n# Compute LDLT factorization\nF = ldlt(Adist)\n\n# Create right-hand side\nb = VectorMPI(ones(n))\n\n# Solve Ax = b\nx = solve(F, b)\n\n# Or use backslash syntax\nx = F \\ b\n\n# Verify solution\nx_full = Vector(x)\nresidual = norm(A * x_full - ones(n), Inf)\n\nif rank == 0\n    println(\"LDLT solve residual: $residual\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#LU-Factorization-(General-Matrices)","page":"Examples","title":"LU Factorization (General Matrices)","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\ncomm = MPI.COMM_WORLD\nrank = MPI.Comm_rank(comm)\n\n# Create a general (non-symmetric) tridiagonal matrix\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\nV = [2.0*ones(n); -0.5*ones(n-1); -0.8*ones(n-1)]  # Non-symmetric\nA = sparse(I, J, V, n, n)\n\n# Distribute and factorize\nAdist = SparseMatrixMPI{Float64}(A)\nF = lu(Adist)\n\n# Solve\nb = VectorMPI(ones(n))\nx = solve(F, b)\n\n# Verify\nx_full = Vector(x)\nresidual = norm(A * x_full - ones(n), Inf)\n\nif rank == 0\n    println(\"LU solve residual: $residual\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Symmetric-Indefinite-Matrices","page":"Examples","title":"Symmetric Indefinite Matrices","text":"LDLT uses Bunch-Kaufman pivoting to handle symmetric indefinite matrices:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# Symmetric indefinite matrix (alternating signs on diagonal)\nn = 50\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\ndiag_vals = [(-1.0)^i * 2.0 for i in 1:n]  # Alternating signs\nV = [diag_vals; -ones(n-1); -ones(n-1)]\nA = sparse(I, J, V, n, n)\n\nAdist = SparseMatrixMPI{Float64}(A)\nF = ldlt(Adist)\n\nb = VectorMPI(collect(1.0:n))\nx = solve(F, b)\n\nx_full = Vector(x)\nresidual = norm(A * x_full - collect(1.0:n), Inf)\n\nif rank == 0\n    println(\"Indefinite LDLT residual: $residual\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Reusing-Symbolic-Factorization","page":"Examples","title":"Reusing Symbolic Factorization","text":"For sequences of matrices with the same sparsity pattern, the symbolic factorization is cached and reused:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 100\nI = [1:n; 1:n-1; 2:n]\nJ = [1:n; 2:n; 1:n-1]\n\n# First matrix\nV1 = [4.0*ones(n); -ones(n-1); -ones(n-1)]\nA1 = sparse(I, J, V1, n, n)\nA1dist = SparseMatrixMPI{Float64}(A1)\n\n# First factorization - computes symbolic phase\nF1 = ldlt(A1dist; reuse_symbolic=true)\n\n# Second matrix - same structure, different values\nV2 = [8.0*ones(n); -2.0*ones(n-1); -2.0*ones(n-1)]\nA2 = sparse(I, J, V2, n, n)\nA2dist = SparseMatrixMPI{Float64}(A2)\n\n# Second factorization - reuses cached symbolic phase (faster)\nF2 = ldlt(A2dist; reuse_symbolic=true)\n\n# Both factorizations work\nb = VectorMPI(ones(n))\nx1 = solve(F1, b)\nx2 = solve(F2, b)\n\nif rank == 0\n    x1_full = Vector(x1)\n    x2_full = Vector(x2)\n    println(\"F1 residual: \", norm(A1 * x1_full - ones(n), Inf))\n    println(\"F2 residual: \", norm(A2 * x2_full - ones(n), Inf))\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Plan-Caching-and-Management","page":"Examples","title":"Plan Caching and Management","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\nn = 100\nA = spdiagm(0 => 2.0*ones(n), 1 => -ones(n-1), -1 => -ones(n-1))\nB = spdiagm(0 => 1.5*ones(n), 1 => 0.5*ones(n-1), -1 => 0.5*ones(n-1))\n\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# First multiplication - creates and caches the plan\nC1 = Adist * Bdist\n\n# Second multiplication - reuses cached plan (faster)\nC2 = Adist * Bdist\n\n# Third multiplication - still uses cached plan\nC3 = Adist * Bdist\n\n# Clear caches when done to free memory\nclear_plan_cache!()  # Clears all caches including factorization\n\n# Or clear specific caches:\n# clear_symbolic_cache!()           # Symbolic factorizations only\n# clear_factorization_plan_cache!() # Factorization plans only\n\nif rank == 0\n    println(\"Cached multiplication completed\")\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Dense-Matrix-Operations-with-mapslices","page":"Examples","title":"Dense Matrix Operations with mapslices","text":"The mapslices function applies a function to each row or column of a distributed dense matrix. This is useful for computing row-wise or column-wise statistics.","category":"section"},{"location":"examples/#Row-wise-Operations-(dims2)","page":"Examples","title":"Row-wise Operations (dims=2)","text":"Row-wise operations are local - no MPI communication is needed since rows are already distributed:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# Create a deterministic dense matrix (same on all ranks)\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\n# Distribute\nAdist = MatrixMPI(A_global)\n\n# Compute row statistics: for each row, compute [norm, max, sum]\n# This transforms 100×10 matrix to 100×3 matrix\nrow_stats = mapslices(x -> [norm(x), maximum(x), sum(x)], Adist; dims=2)\n\nif rank == 0\n    println(\"Row statistics shape: $(size(row_stats))\")  # (100, 3)\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Column-wise-Operations-(dims1)","page":"Examples","title":"Column-wise Operations (dims=1)","text":"Column-wise operations require MPI communication to gather each full column:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# Create a deterministic dense matrix\nm, n = 100, 10\nA_global = Float64.([i + 0.1*j for i in 1:m, j in 1:n])\n\nAdist = MatrixMPI(A_global)\n\n# Compute column statistics: for each column, compute [norm, max]\n# This transforms 100×10 matrix to 2×10 matrix\ncol_stats = mapslices(x -> [norm(x), maximum(x)], Adist; dims=1)\n\nif rank == 0\n    println(\"Column statistics shape: $(size(col_stats))\")  # (2, 10)\nend\n\nMPI.Finalize()","category":"section"},{"location":"examples/#Use-Case:-Replacing-vcat(f.(eachrow(A))...)","page":"Examples","title":"Use Case: Replacing vcat(f.(eachrow(A))...)","text":"The standard Julia pattern vcat(f.(eachrow(A))...) doesn't work with distributed matrices because the type information is lost after broadcasting. Use mapslices instead:\n\nusing MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing LinearAlgebra\n\nrank = MPI.Comm_rank(MPI.COMM_WORLD)\n\n# Standard Julia pattern (for comparison):\n# A = randn(5, 2)\n# f(x) = transpose([norm(x), maximum(x)])\n# B = vcat(f.(eachrow(A))...)\n\n# MPI-compatible equivalent:\nA_global = Float64.([i + 0.1*j for i in 1:100, j in 1:10])\nAdist = MatrixMPI(A_global)\n\n# Use mapslices with dims=2 to apply function to each row\n# The function returns a vector, which becomes a row in the result\ng(x) = [norm(x), maximum(x)]\nBdist = mapslices(g, Adist; dims=2)\n\nif rank == 0\n    println(\"Result: $(size(Bdist))\")  # (100, 2)\nend\n\nMPI.Finalize()","category":"section"},{"location":"#LinearAlgebraMPI.jl","page":"Home","title":"LinearAlgebraMPI.jl","text":"Distributed sparse matrix operations using MPI for parallel computing across multiple ranks.\n\nLinearAlgebraMPI.jl provides a high-performance implementation of distributed sparse matrices in Julia, enabling parallel sparse linear algebra operations across multiple MPI processes. The package is designed for large-scale computations where matrices are too large to fit on a single node or where parallel speedup is desired.","category":"section"},{"location":"#Features","page":"Home","title":"Features","text":"Row-partitioned sparse matrices: Matrices are distributed by rows across MPI ranks\nMatrix multiplication: Efficient sparse matrix-matrix product with memoized communication plans\nAddition and subtraction: Element-wise operations with automatic data redistribution\nTranspose operations: Both eager and lazy transpose support\nConjugate and adjoint: Full support for complex matrices\nScalar multiplication: Efficient scalar-matrix products\nNorms: Frobenius norm, 1-norm, infinity norm, and general p-norms\nOperator norms: 1-norm and infinity-norm of operators\nType stability: Generic implementation supporting Float64, ComplexF64, and other numeric types\nPlan caching: Communication plans are memoized for repeated operations with the same sparsity pattern","category":"section"},{"location":"#Quick-Example","page":"Home","title":"Quick Example","text":"using MPI\nMPI.Init()\n\nusing LinearAlgebraMPI\nusing SparseArrays\n\n# Create a sparse matrix (must be identical on all ranks)\nA = sprand(1000, 1000, 0.01)\nB = sprand(1000, 1000, 0.01)\n\n# Distribute matrices across MPI ranks\nAdist = SparseMatrixMPI{Float64}(A)\nBdist = SparseMatrixMPI{Float64}(B)\n\n# Perform distributed operations\nC = Adist * Bdist    # Matrix multiplication\nD = Adist + Bdist    # Addition\nE = Adist - Bdist    # Subtraction\nF = 2.0 * Adist      # Scalar multiplication\n\n# Compute norms\nfrobenius_norm = norm(Adist)\nmax_col_sum = opnorm(Adist, 1)\n\nMPI.Finalize()","category":"section"},{"location":"#Package-Overview","page":"Home","title":"Package Overview","text":"Pages = [\"getting-started.md\", \"examples.md\", \"api.md\"]\nDepth = 2","category":"section"},{"location":"#Installation","page":"Home","title":"Installation","text":"Add LinearAlgebraMPI.jl to your project:\n\nusing Pkg\nPkg.add(\"LinearAlgebraMPI\")\n\nOr for development:\n\nPkg.develop(path=\"/path/to/LinearAlgebraMPI.jl\")","category":"section"},{"location":"#Requirements","page":"Home","title":"Requirements","text":"Julia 1.10+\nMPI.jl with a working MPI implementation\nSparseArrays.jl\nLinearAlgebra.jl\nBlake3Hash.jl","category":"section"},{"location":"#License","page":"Home","title":"License","text":"MIT License","category":"section"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"algorithms/#Algorithms","page":"Algorithms","title":"Algorithms","text":"This document describes the algorithms used in LinearAlgebraMPI's distributed sparse direct solvers. It is intended for experts familiar with sparse matrix computations who want to understand our implementation choices.","category":"section"},{"location":"algorithms/#Overview","page":"Algorithms","title":"Overview","text":"LinearAlgebraMPI implements distributed sparse LU and LDLT factorizations using the multifrontal method. The implementation follows a three-phase approach:\n\nSymbolic Phase: Compute fill-reducing ordering, elimination tree, supernodes, and rank assignments\nNumerical Phase: Distributed multifrontal factorization with communication\nSolve Phase: Distributed triangular solves","category":"section"},{"location":"algorithms/#The-Multifrontal-Method","page":"Algorithms","title":"The Multifrontal Method","text":"The multifrontal method was introduced by Duff and Reid [1] for symmetric indefinite systems and later extended to unsymmetric systems [2]. Liu provides an excellent survey of the method [3].\n\nThe key insight is that sparse Gaussian elimination can be organized as a sequence of partial factorizations of dense submatrices called frontal matrices. Each frontal matrix corresponds to eliminating a set of pivot columns and produces:\n\nCompleted rows/columns of the factors L and U (or L and D for LDLT)\nA dense update matrix (Schur complement) that is passed to a parent frontal\n\nThis organization enables:\n\nHigh performance through dense BLAS operations on frontal matrices\nNatural parallelism from the tree structure of dependencies\nReduced memory traffic compared to left-looking or right-looking methods","category":"section"},{"location":"algorithms/#Frontal-Matrix-Structure","page":"Algorithms","title":"Frontal Matrix Structure","text":"For a supernode with nfs fully-summed (pivot) columns and nrs rows in total, the frontal matrix F has the block structure:\n\nF = [ F11  F12 ]    (nfs × nfs)  (nfs × (nrs-nfs))\n    [ F21  F22 ]    ((nrs-nfs) × nfs)  ((nrs-nfs) × (nrs-nfs))\n\nAfter partial factorization:\n\nF11 contains the diagonal block of the factors\nF21 contains the subdiagonal of L (LU) or is symmetric with F12 (LDLT)\nF12 contains the superdiagonal of U (LU only)\nF22 becomes the update matrix (Schur complement) passed to the parent","category":"section"},{"location":"algorithms/#Assembly-(Extend-Add)","page":"Algorithms","title":"Assembly (Extend-Add)","text":"When a frontal matrix receives update matrices from its children in the elimination tree, these are assembled using the extend-add operation. The child's update matrix is scattered into the parent's frontal matrix at positions determined by the index mapping, and values are summed.","category":"section"},{"location":"algorithms/#Fill-Reducing-Ordering","page":"Algorithms","title":"Fill-Reducing Ordering","text":"We use the Approximate Minimum Degree (AMD) ordering algorithm [4, 5] to reduce fill-in during factorization. AMD computes a permutation P such that factoring P'AP produces fewer nonzeros in L and U than factoring A directly.\n\nThe minimum degree algorithm is a greedy heuristic that, at each elimination step, selects the variable whose elimination creates the least fill-in. AMD uses quotient graph techniques to efficiently approximate the true minimum degree, achieving O(n²) worst-case complexity while producing orderings comparable to exact minimum degree.\n\nOur implementation uses Julia's AMD.jl package, which provides a native Julia implementation of the AMD algorithm.","category":"section"},{"location":"algorithms/#Elimination-Tree","page":"Algorithms","title":"Elimination Tree","text":"The elimination tree captures the dependencies between columns during factorization [6]. For a matrix A with Cholesky factor L, the elimination tree T has:\n\nn nodes (one per column)\nAn edge from j to parent(j) where parent(j) is the row index of the first subdiagonal nonzero in column j of L\n\nKey properties:\n\nColumn j of L depends only on columns in the subtree rooted at j\nDisjoint subtrees can be factored independently (parallelism)\nThe tree structure determines the flow of update matrices in the multifrontal method\n\nFor unsymmetric LU factorization, we use the elimination tree of the symmetrized structure A + A'.","category":"section"},{"location":"algorithms/#Supernodes","page":"Algorithms","title":"Supernodes","text":"A supernode is a set of contiguous columns with nearly identical sparsity structure [7, 8]. Grouping columns into supernodes enables:\n\nDense matrix operations (BLAS-3) instead of sparse column operations\nReduced indexing overhead\nBetter cache utilization\n\nWe detect fundamental supernodes: maximal sets of contiguous columns where each column's structure is contained in the next column's structure (plus the new diagonal). This can be characterized in terms of the elimination tree: columns j and j+1 are in the same fundamental supernode if and only if parent(j) = j+1 in the elimination tree [7].\n\nAfter detecting fundamental supernodes, we construct a supernodal elimination tree where each node represents a supernode rather than a single column.","category":"section"},{"location":"algorithms/#Distributed-Execution","page":"Algorithms","title":"Distributed Execution","text":"","category":"section"},{"location":"algorithms/#Supernode-to-Rank-Assignment","page":"Algorithms","title":"Supernode-to-Rank Assignment","text":"We use a subtree-to-rank mapping strategy inspired by MUMPS [9, 10]. The key idea is to assign complete subtrees of the supernodal elimination tree to single MPI ranks, minimizing communication.\n\nThe assignment algorithm:\n\nCompute the \"work\" for each supernode: work[s] = nfs * nrows² + Σ work[children]\nIdentify subtree roots (supernodes whose parent is assigned to a different rank or is the root)\nUse bin-packing to assign subtrees to ranks, balancing the total work per rank\n\nThis approach ensures that:\n\nCommunication only occurs at subtree boundaries\nLoad is approximately balanced across ranks\nSmall problems use fewer ranks efficiently\n\nFor very large frontal matrices near the root of the tree, more sophisticated 2D distribution schemes (as in MUMPS [9]) could be employed, but our current implementation assigns each supernode to a single rank.","category":"section"},{"location":"algorithms/#Communication-Pattern","page":"Algorithms","title":"Communication Pattern","text":"During numerical factorization, communication occurs when:\n\nA child supernode on rank r₁ sends its update matrix to a parent supernode on rank r₂ ≠ r₁\nThe update matrix and its row indices are sent via MPI point-to-point communication\n\nWe use non-blocking sends (MPI.Isend) to overlap communication with computation where possible. Synchronization barriers between supernodes ensure correct ordering.","category":"section"},{"location":"algorithms/#Numerical-Pivoting","page":"Algorithms","title":"Numerical Pivoting","text":"","category":"section"},{"location":"algorithms/#LU-Factorization:-Partial-Pivoting","page":"Algorithms","title":"LU Factorization: Partial Pivoting","text":"For unsymmetric matrices, we use partial pivoting within each frontal matrix. At each elimination step k within the frontal:\n\nFind the largest magnitude element in column k below the diagonal\nSwap rows if necessary\nProceed with elimination\n\nThe row permutation is tracked and applied during the solve phase. Partial pivoting ensures numerical stability with element growth bounded by 2^(n-1) in the worst case, though growth is typically much smaller in practice.","category":"section"},{"location":"algorithms/#LDLT-Factorization:-Bunch-Kaufman-Pivoting","page":"Algorithms","title":"LDLT Factorization: Bunch-Kaufman Pivoting","text":"For symmetric matrices (definite or indefinite), we use the Bunch-Kaufman pivoting strategy [11]. This produces a factorization:\n\nP' * A * P = L * D * L'\n\nwhere:\n\nP is a permutation matrix\nL is unit lower triangular\nD is block diagonal with 1×1 and 2×2 blocks\n\nThe algorithm maintains symmetry while providing numerical stability for indefinite matrices. At each step, it chooses between:\n\nA 1×1 pivot if the diagonal element is sufficiently large\nA 2×2 pivot using an off-diagonal element if better conditioning is achieved\n\nThe threshold parameter α = (1 + √17)/8 ≈ 0.6404 balances stability and sparsity [11].\n\nFor 2×2 pivots at positions (k, k+1), the block:\n\nD_block = [ d_kk    d_k,k+1  ]\n          [ d_k+1,k  d_k+1,k+1 ]\n\nis stored, and the solve phase handles these blocks specially by solving 2×2 systems.\n\nImportant: Our LDLT uses transpose (L'), not conjugate transpose (L*). This is correct for real symmetric and complex symmetric matrices, but NOT for complex Hermitian matrices.","category":"section"},{"location":"algorithms/#Solve-Phase","page":"Algorithms","title":"Solve Phase","text":"","category":"section"},{"location":"algorithms/#LU-Solve","page":"Algorithms","title":"LU Solve","text":"Given the factorization Pr' * L * U * Pc = A (with row permutation Pr from pivoting and column permutation Pc from AMD ordering), we solve Ax = b as:\n\nApply AMD permutation: y = P_c' * b\nApply row pivot permutation: z = P_r * y\nForward solve: L * w = z (in elimination order)\nBackward solve: U * v = w (in reverse elimination order)\nApply inverse row permutation: u = P_r' * v\nApply inverse AMD permutation: x = P_c * u","category":"section"},{"location":"algorithms/#LDLT-Solve","page":"Algorithms","title":"LDLT Solve","text":"Given P' * A * P = L * D * L' with symmetric pivot permutation P_s, we solve Ax = b as:\n\nApply AMD permutation: y = P_perm * b\nApply symmetric pivot permutation: z = P_s * y\nForward solve: L * w = z\nDiagonal solve: D * v = w (handling 2×2 blocks)\nBackward solve: L' * u = v\nApply inverse symmetric permutation: t = P_s' * u\nApply inverse AMD permutation: x = P_perm' * t\n\nThe diagonal solve for 2×2 blocks requires solving:\n\n[ d11  d12 ] [ v_k   ]   [ w_k   ]\n[ d21  d22 ] [ v_k+1 ] = [ w_k+1 ]\n\nwhich is done by direct 2×2 matrix inversion.","category":"section"},{"location":"algorithms/#Plan-Caching","page":"Algorithms","title":"Plan Caching","text":"Following the pattern used throughout LinearAlgebraMPI, we cache computed structures for reuse:\n\nSymbolic factorization cache: Keyed by the structural hash of the matrix. Stores the AMD ordering, elimination tree, supernodes, and rank assignments.\nFactorization plan cache: Keyed by (structural hash, element type). Stores pre-allocated communication buffers.\n\nWhen factoring a sequence of matrices with the same sparsity pattern, only the first factorization incurs the cost of symbolic analysis. Subsequent factorizations reuse the cached symbolic structure and communication plans, performing only the numerical computation.","category":"section"},{"location":"algorithms/#Complexity","page":"Algorithms","title":"Complexity","text":"For a sparse matrix of dimension n with nnz nonzeros:\n\nAMD ordering: O(nnz) average case, O(n²) worst case\nSymbolic factorization: O(nnzL) where nnzL is nonzeros in L\nNumerical factorization: Dominated by dense operations on frontal matrices. For 2D problems from finite elements, typically O(n^1.5); for 3D problems, O(n²)\nSolve: O(nnzL + nnzU)\n\nThe distributed implementation adds communication overhead proportional to the number of cross-rank edges in the supernodal elimination tree.","category":"section"},{"location":"algorithms/#References","page":"Algorithms","title":"References","text":"[1] I. S. Duff and J. K. Reid, \"The multifrontal solution of indefinite sparse symmetric linear equations,\" ACM Transactions on Mathematical Software, vol. 9, no. 3, pp. 302–325, 1983. https://doi.org/10.1145/356044.356047\n\n[2] I. S. Duff and J. K. Reid, \"The multifrontal solution of unsymmetric sets of linear equations,\" SIAM Journal on Scientific and Statistical Computing, vol. 5, no. 3, pp. 633–641, 1984.\n\n[3] J. W. H. Liu, \"The multifrontal method for sparse matrix solution: Theory and practice,\" SIAM Review, vol. 34, no. 1, pp. 82–109, 1992. https://doi.org/10.1137/1034004\n\n[4] P. R. Amestoy, T. A. Davis, and I. S. Duff, \"An approximate minimum degree ordering algorithm,\" SIAM Journal on Matrix Analysis and Applications, vol. 17, no. 4, pp. 886–905, 1996. https://doi.org/10.1137/S0895479894278952\n\n[5] P. R. Amestoy, T. A. Davis, and I. S. Duff, \"Algorithm 837: AMD, an approximate minimum degree ordering algorithm,\" ACM Transactions on Mathematical Software, vol. 30, no. 3, pp. 381–388, 2004. https://doi.org/10.1145/1024074.1024081\n\n[6] J. W. H. Liu, \"The role of elimination trees in sparse factorization,\" SIAM Journal on Matrix Analysis and Applications, vol. 11, no. 1, pp. 134–172, 1990. https://doi.org/10.1137/0611010\n\n[7] J. W. H. Liu, E. G. Ng, and B. W. Peyton, \"On finding supernodes for sparse matrix computations,\" SIAM Journal on Matrix Analysis and Applications, vol. 14, no. 1, pp. 242–252, 1993. https://doi.org/10.1137/0614019\n\n[8] C. Ashcraft and R. Grimes, \"The influence of relaxed supernode partitions on the multifrontal method,\" ACM Transactions on Mathematical Software, vol. 15, no. 4, pp. 291–309, 1989. https://doi.org/10.1145/76909.76912\n\n[9] P. R. Amestoy, I. S. Duff, J. Koster, and J.-Y. L'Excellent, \"A fully asynchronous multifrontal solver using distributed dynamic scheduling,\" SIAM Journal on Matrix Analysis and Applications, vol. 23, no. 1, pp. 15–41, 2001. https://doi.org/10.1137/S0895479899358194\n\n[10] P. R. Amestoy, I. S. Duff, and J.-Y. L'Excellent, \"Multifrontal parallel distributed symmetric and unsymmetric solvers,\" Computer Methods in Applied Mechanics and Engineering, vol. 184, no. 2–4, pp. 501–520, 2000.\n\n[11] J. R. Bunch and L. Kaufman, \"Some stable methods for calculating inertia and solving symmetric linear systems,\" Mathematics of Computation, vol. 31, no. 137, pp. 163–179, 1977. https://doi.org/10.1090/S0025-5718-1977-0428694-0","category":"section"}]
}
