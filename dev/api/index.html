<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · LinearAlgebraMPI.jl</title><meta name="title" content="API Reference · LinearAlgebraMPI.jl"/><meta property="og:title" content="API Reference · LinearAlgebraMPI.jl"/><meta property="twitter:title" content="API Reference · LinearAlgebraMPI.jl"/><meta name="description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="og:description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="twitter:description" content="Documentation for LinearAlgebraMPI.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">LinearAlgebraMPI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting-started/">Getting Started</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Types"><span>Types</span></a></li><li><a class="tocitem" href="#Sparse-Matrix-Operations"><span>Sparse Matrix Operations</span></a></li><li><a class="tocitem" href="#Dense-Matrix-Operations"><span>Dense Matrix Operations</span></a></li><li><a class="tocitem" href="#Vector-Operations"><span>Vector Operations</span></a></li><li><a class="tocitem" href="#Indexing"><span>Indexing</span></a></li><li><a class="tocitem" href="#Utility-Functions"><span>Utility Functions</span></a></li><li><a class="tocitem" href="#Factorization"><span>Factorization</span></a></li><li><a class="tocitem" href="#Cache-Management"><span>Cache Management</span></a></li><li><a class="tocitem" href="#Full-API-Index"><span>Full API Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><p>This page documents the public API of LinearAlgebraMPI.jl.</p><h2 id="Types"><a class="docs-heading-anchor" href="#Types">Types</a><a id="Types-1"></a><a class="docs-heading-anchor-permalink" href="#Types" title="Permalink"></a></h2><h3 id="SparseMatrixMPI"><a class="docs-heading-anchor" href="#SparseMatrixMPI">SparseMatrixMPI</a><a id="SparseMatrixMPI-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixMPI"><code>LinearAlgebraMPI.SparseMatrixMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixMPI{T}</code></pre><p>A distributed sparse matrix partitioned by rows across MPI ranks.</p><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (placeholder for transpose)</li><li><code>col_indices::Vector{Int}</code>: Global column indices that appear in the local part (local→global mapping)</li><li><code>A::Transpose{T,SparseMatrixCSC{T,Int}}</code>: Local rows as a lazy transpose wrapper around compressed CSC storage</li></ul><p><strong>Invariants</strong></p><ul><li><code>col_indices</code>, <code>row_partition</code>, and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows</li><li><code>col_partition[nranks+1]</code> = total number of columns</li><li><code>size(A, 1) == row_partition[rank+1] - row_partition[rank]</code> (number of local rows)</li><li><code>size(A.parent, 1) == length(col_indices)</code> (compressed column dimension)</li><li><code>A.parent.rowval</code> contains local indices in <code>1:length(col_indices)</code></li></ul><p><strong>Storage Details</strong></p><p>The local rows are stored in compressed form as <code>A = transpose(AT)</code> where <code>AT::SparseMatrixCSC</code> has:</p><ul><li><code>AT.m = length(col_indices)</code> (compressed, not global ncols)</li><li><code>AT.n</code> = number of local rows</li><li><code>AT.rowval</code> contains LOCAL column indices (1:length(col_indices))</li><li><code>col_indices[local_idx]</code> maps local→global column indices</li></ul><p>This compression avoids &quot;hypersparse&quot; storage where <code>AT.m &gt;&gt; length(unique(AT.rowval))</code>, which would cause excessive allocations in matrix operations.</p><p>Access the underlying CSC via <code>A.parent</code> when needed for low-level operations.</p></div></section></details></article><h3 id="MatrixMPI"><a class="docs-heading-anchor" href="#MatrixMPI">MatrixMPI</a><a id="MatrixMPI-1"></a><a class="docs-heading-anchor-permalink" href="#MatrixMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.MatrixMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.MatrixMPI"><code>LinearAlgebraMPI.MatrixMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MatrixMPI{T}</code></pre><p>A distributed dense matrix partitioned by rows across MPI ranks.</p><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (for transpose)</li><li><code>A::Matrix{T}</code>: Local rows (NOT transposed), size = (local_nrows, ncols)</li></ul><p><strong>Invariants</strong></p><ul><li><code>row_partition</code> and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows + 1</li><li><code>col_partition[nranks+1]</code> = total number of columns + 1</li><li><code>size(A, 1) == row_partition[rank+2] - row_partition[rank+1]</code></li><li><code>size(A, 2) == col_partition[end] - 1</code></li></ul></div></section></details></article><h3 id="VectorMPI"><a class="docs-heading-anchor" href="#VectorMPI">VectorMPI</a><a id="VectorMPI-1"></a><a class="docs-heading-anchor-permalink" href="#VectorMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.VectorMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.VectorMPI"><code>LinearAlgebraMPI.VectorMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">VectorMPI{T}</code></pre><p>A distributed dense vector partitioned across MPI ranks.</p><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the partition</li><li><code>partition::Vector{Int}</code>: Partition boundaries, length = nranks + 1</li><li><code>v::Vector{T}</code>: Local vector elements owned by this rank</li></ul></div></section></details></article><h2 id="Sparse-Matrix-Operations"><a class="docs-heading-anchor" href="#Sparse-Matrix-Operations">Sparse Matrix Operations</a><a id="Sparse-Matrix-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-Matrix-Operations" title="Permalink"></a></h2><h3 id="Arithmetic"><a class="docs-heading-anchor" href="#Arithmetic">Arithmetic</a><a id="Arithmetic-1"></a><a class="docs-heading-anchor-permalink" href="#Arithmetic" title="Permalink"></a></h3><pre><code class="language-julia hljs">A * B          # Matrix multiplication
A + B          # Addition
A - B          # Subtraction
a * A          # Scalar multiplication
A * a          # Scalar multiplication</code></pre><h3 id="Threaded-Sparse-Multiplication"><a class="docs-heading-anchor" href="#Threaded-Sparse-Multiplication">Threaded Sparse Multiplication</a><a id="Threaded-Sparse-Multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Threaded-Sparse-Multiplication" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.:⊛"><a class="docstring-binding" href="#LinearAlgebraMPI.:⊛"><code>LinearAlgebraMPI.:⊛</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">⊛(A::SparseMatrixCSC{Tv,Ti}, B::SparseMatrixCSC{Tv,Ti}; max_threads=Threads.nthreads()) where {Tv,Ti}</code></pre><p>Multithreaded sparse matrix multiplication. Splits B into column blocks and computes <code>A * B_block</code> in parallel using Julia&#39;s optimized builtin <code>*</code>.</p><p><strong>Threading behavior</strong></p><ul><li>Uses at most <code>n ÷ 100</code> threads, where <code>n = size(B, 2)</code>, ensuring at least 100 columns per thread</li><li>Falls back to single-threaded <code>A * B</code> when <code>n &lt; 100</code> or when threading overhead would dominate</li><li>The <code>max_threads</code> keyword limits the maximum number of threads used</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using SparseArrays
A = sprand(1000, 1000, 0.01)
B = sprand(1000, 500, 0.01)
C = A ⊛ B                    # Use all available threads (up to n÷100)
C = ⊛(A, B; max_threads=2)   # Limit to 2 threads</code></pre></div></section></details></article><p>The <code>⊛</code> operator (typed as <code>\circledast&lt;tab&gt;</code>) provides multithreaded sparse matrix multiplication for <code>SparseMatrixCSC</code>. It is used internally by <code>SparseMatrixMPI</code> multiplication and can also be used directly on local sparse matrices:</p><pre><code class="language-julia hljs">using SparseArrays
A = sprand(10000, 10000, 0.001)
B = sprand(10000, 5000, 0.001)
C = A ⊛ B   # Parallel multiplication using available threads</code></pre><h3 id="Transpose-and-Adjoint"><a class="docs-heading-anchor" href="#Transpose-and-Adjoint">Transpose and Adjoint</a><a id="Transpose-and-Adjoint-1"></a><a class="docs-heading-anchor-permalink" href="#Transpose-and-Adjoint" title="Permalink"></a></h3><pre><code class="language-julia hljs">transpose(A)   # Lazy transpose
conj(A)        # Conjugate (new matrix)
A&#39;             # Adjoint (conjugate transpose, lazy)</code></pre><h3 id="Matrix-Vector-Multiplication"><a class="docs-heading-anchor" href="#Matrix-Vector-Multiplication">Matrix-Vector Multiplication</a><a id="Matrix-Vector-Multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-Vector-Multiplication" title="Permalink"></a></h3><pre><code class="language-julia hljs">y = A * x      # Returns VectorMPI
mul!(y, A, x)  # In-place version</code></pre><h3 id="Vector-Matrix-Multiplication"><a class="docs-heading-anchor" href="#Vector-Matrix-Multiplication">Vector-Matrix Multiplication</a><a id="Vector-Matrix-Multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-Matrix-Multiplication" title="Permalink"></a></h3><pre><code class="language-julia hljs">transpose(v) * A   # Row vector times matrix
v&#39; * A             # Conjugate row vector times matrix</code></pre><h3 id="Norms"><a class="docs-heading-anchor" href="#Norms">Norms</a><a id="Norms-1"></a><a class="docs-heading-anchor-permalink" href="#Norms" title="Permalink"></a></h3><pre><code class="language-julia hljs">norm(A)        # Frobenius norm (default)
norm(A, 1)     # Sum of absolute values
norm(A, Inf)   # Maximum absolute value
norm(A, p)     # General p-norm

opnorm(A, 1)   # Maximum absolute column sum
opnorm(A, Inf) # Maximum absolute row sum</code></pre><h3 id="Properties"><a class="docs-heading-anchor" href="#Properties">Properties</a><a id="Properties-1"></a><a class="docs-heading-anchor-permalink" href="#Properties" title="Permalink"></a></h3><pre><code class="language-julia hljs">size(A)        # Global dimensions (m, n)
size(A, d)     # Size along dimension d
eltype(A)      # Element type
nnz(A)         # Number of nonzeros
issparse(A)    # Returns true</code></pre><h3 id="Reductions"><a class="docs-heading-anchor" href="#Reductions">Reductions</a><a id="Reductions-1"></a><a class="docs-heading-anchor-permalink" href="#Reductions" title="Permalink"></a></h3><pre><code class="language-julia hljs">sum(A)         # Sum of all stored elements
sum(A; dims=1) # Column sums (returns VectorMPI) - SparseMatrixMPI only
sum(A; dims=2) # Row sums (returns VectorMPI) - SparseMatrixMPI only
maximum(A)     # Maximum of stored values
minimum(A)     # Minimum of stored values
tr(A)          # Trace (sum of diagonal) - SparseMatrixMPI only</code></pre><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.mean"><a class="docstring-binding" href="#LinearAlgebraMPI.mean"><code>LinearAlgebraMPI.mean</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mean(v::VectorMPI{T}) where T</code></pre><p>Compute the mean of all elements in the distributed vector.</p></div></section><section><div><pre><code class="language-julia hljs">mean(A::SparseMatrixMPI{T}) where T</code></pre><p>Compute the mean of all elements (including implicit zeros) in the distributed sparse matrix.</p></div></section></details></article><h3 id="Element-wise-Operations"><a class="docs-heading-anchor" href="#Element-wise-Operations">Element-wise Operations</a><a id="Element-wise-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Element-wise-Operations" title="Permalink"></a></h3><pre><code class="language-julia hljs">abs(A)         # Absolute value
abs2(A)        # Squared absolute value
real(A)        # Real part
imag(A)        # Imaginary part
floor(A)       # Floor
ceil(A)        # Ceiling
round(A)       # Round</code></pre><h3 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h3><pre><code class="language-julia hljs">copy(A)        # Deep copy
dropzeros(A)   # Remove stored zeros
diag(A)        # Main diagonal (returns VectorMPI)
diag(A, k)     # k-th diagonal
triu(A)        # Upper triangular
triu(A, k)     # Upper triangular from k-th diagonal
tril(A)        # Lower triangular
tril(A, k)     # Lower triangular from k-th diagonal</code></pre><h3 id="Block-Operations"><a class="docs-heading-anchor" href="#Block-Operations">Block Operations</a><a id="Block-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Block-Operations" title="Permalink"></a></h3><pre><code class="language-julia hljs">cat(A, B, C; dims=1)       # Vertical concatenation
cat(A, B, C; dims=2)       # Horizontal concatenation
cat(A, B, C, D; dims=(2,2)) # 2x2 block matrix [A B; C D]
vcat(A, B, C)              # Vertical concatenation
hcat(A, B, C)              # Horizontal concatenation
blockdiag(A, B, C)         # Block diagonal matrix</code></pre><h3 id="Diagonal-Matrix-Construction"><a class="docs-heading-anchor" href="#Diagonal-Matrix-Construction">Diagonal Matrix Construction</a><a id="Diagonal-Matrix-Construction-1"></a><a class="docs-heading-anchor-permalink" href="#Diagonal-Matrix-Construction" title="Permalink"></a></h3><pre><code class="language-julia hljs">spdiagm(v)                 # Diagonal matrix from VectorMPI
spdiagm(m, n, v)           # m x n diagonal matrix
spdiagm(k =&gt; v)            # k-th diagonal
spdiagm(0 =&gt; v, 1 =&gt; w)    # Multiple diagonals</code></pre><h2 id="Dense-Matrix-Operations"><a class="docs-heading-anchor" href="#Dense-Matrix-Operations">Dense Matrix Operations</a><a id="Dense-Matrix-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Dense-Matrix-Operations" title="Permalink"></a></h2><h3 id="Arithmetic-2"><a class="docs-heading-anchor" href="#Arithmetic-2">Arithmetic</a><a class="docs-heading-anchor-permalink" href="#Arithmetic-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">A * x          # Matrix-vector multiplication (returns VectorMPI)
transpose(A)   # Lazy transpose
conj(A)        # Conjugate
A&#39;             # Adjoint
a * A          # Scalar multiplication</code></pre><h3 id="mapslices"><a class="docs-heading-anchor" href="#mapslices">mapslices</a><a id="mapslices-1"></a><a class="docs-heading-anchor-permalink" href="#mapslices" title="Permalink"></a></h3><p>Apply a function to rows or columns of a distributed dense matrix.</p><pre><code class="language-julia hljs">mapslices(f, A; dims=2)   # Apply f to each row (local, no MPI)
mapslices(f, A; dims=1)   # Apply f to each column (requires MPI)</code></pre><p><strong>Example:</strong></p><pre><code class="language-julia hljs">using LinearAlgebra

# Create deterministic test matrix (same on all ranks)
A_global = Float64.([i + 0.1*j for i in 1:100, j in 1:10])
A = MatrixMPI(A_global)

# Compute row statistics: norm, max, sum for each row
# Transforms 100x10 to 100x3
B = mapslices(x -&gt; [norm(x), maximum(x), sum(x)], A; dims=2)</code></pre><h3 id="Block-Operations-2"><a class="docs-heading-anchor" href="#Block-Operations-2">Block Operations</a><a class="docs-heading-anchor-permalink" href="#Block-Operations-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">cat(A, B; dims=1)          # Vertical concatenation
cat(A, B; dims=2)          # Horizontal concatenation
vcat(A, B)                 # Vertical concatenation
hcat(A, B)                 # Horizontal concatenation</code></pre><h3 id="Norms-2"><a class="docs-heading-anchor" href="#Norms-2">Norms</a><a class="docs-heading-anchor-permalink" href="#Norms-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">norm(A)        # Frobenius norm
norm(A, p)     # General p-norm
opnorm(A, 1)   # Maximum absolute column sum
opnorm(A, Inf) # Maximum absolute row sum</code></pre><h2 id="Vector-Operations"><a class="docs-heading-anchor" href="#Vector-Operations">Vector Operations</a><a id="Vector-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-Operations" title="Permalink"></a></h2><h3 id="Arithmetic-3"><a class="docs-heading-anchor" href="#Arithmetic-3">Arithmetic</a><a class="docs-heading-anchor-permalink" href="#Arithmetic-3" title="Permalink"></a></h3><pre><code class="language-julia hljs">u + v          # Addition (auto-aligns partitions)
u - v          # Subtraction
-v             # Negation
a * v          # Scalar multiplication
v * a          # Scalar multiplication
v / a          # Scalar division</code></pre><h3 id="Transpose-and-Adjoint-2"><a class="docs-heading-anchor" href="#Transpose-and-Adjoint-2">Transpose and Adjoint</a><a class="docs-heading-anchor-permalink" href="#Transpose-and-Adjoint-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">transpose(v)   # Lazy transpose (row vector)
conj(v)        # Conjugate
v&#39;             # Adjoint</code></pre><h3 id="Norms-3"><a class="docs-heading-anchor" href="#Norms-3">Norms</a><a class="docs-heading-anchor-permalink" href="#Norms-3" title="Permalink"></a></h3><pre><code class="language-julia hljs">norm(v)        # 2-norm (default)
norm(v, 1)     # 1-norm
norm(v, Inf)   # Infinity norm
norm(v, p)     # General p-norm</code></pre><h3 id="Reductions-2"><a class="docs-heading-anchor" href="#Reductions-2">Reductions</a><a class="docs-heading-anchor-permalink" href="#Reductions-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">sum(v)         # Sum of elements
prod(v)        # Product of elements
maximum(v)     # Maximum element
minimum(v)     # Minimum element
mean(v)        # Mean of elements</code></pre><h3 id="Element-wise-Operations-2"><a class="docs-heading-anchor" href="#Element-wise-Operations-2">Element-wise Operations</a><a class="docs-heading-anchor-permalink" href="#Element-wise-Operations-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">abs(v)         # Absolute value
abs2(v)        # Squared absolute value
real(v)        # Real part
imag(v)        # Imaginary part
copy(v)        # Deep copy</code></pre><h3 id="Broadcasting"><a class="docs-heading-anchor" href="#Broadcasting">Broadcasting</a><a id="Broadcasting-1"></a><a class="docs-heading-anchor-permalink" href="#Broadcasting" title="Permalink"></a></h3><p>VectorMPI supports broadcasting for element-wise operations:</p><pre><code class="language-julia hljs">v .+ w         # Element-wise addition
v .* w         # Element-wise multiplication
sin.(v)        # Apply function element-wise
v .* 2.0 .+ w  # Compound expressions</code></pre><h3 id="Block-Operations-3"><a class="docs-heading-anchor" href="#Block-Operations-3">Block Operations</a><a class="docs-heading-anchor-permalink" href="#Block-Operations-3" title="Permalink"></a></h3><pre><code class="language-julia hljs">vcat(u, v, w)  # Concatenate vectors (returns VectorMPI)
hcat(u, v, w)  # Stack as columns (returns MatrixMPI)</code></pre><h3 id="Properties-2"><a class="docs-heading-anchor" href="#Properties-2">Properties</a><a class="docs-heading-anchor-permalink" href="#Properties-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">length(v)      # Global length
size(v)        # Returns (length,)
eltype(v)      # Element type</code></pre><h2 id="Indexing"><a class="docs-heading-anchor" href="#Indexing">Indexing</a><a id="Indexing-1"></a><a class="docs-heading-anchor-permalink" href="#Indexing" title="Permalink"></a></h2><p>All distributed types support element access and assignment. These are collective operations - all MPI ranks must call them with the same arguments.</p><h3 id="VectorMPI-Indexing"><a class="docs-heading-anchor" href="#VectorMPI-Indexing">VectorMPI Indexing</a><a id="VectorMPI-Indexing-1"></a><a class="docs-heading-anchor-permalink" href="#VectorMPI-Indexing" title="Permalink"></a></h3><pre><code class="language-julia hljs">v[i]           # Get element (collective)
v[i] = x       # Set element (collective)
v[1:10]        # Range indexing (returns VectorMPI)
v[1:10] = x    # Range assignment (scalar or vector)
v[idx]         # VectorMPI{Int} indexing (returns VectorMPI)
v[idx] = src   # VectorMPI{Int} assignment (src::VectorMPI)</code></pre><h3 id="MatrixMPI-Indexing"><a class="docs-heading-anchor" href="#MatrixMPI-Indexing">MatrixMPI Indexing</a><a id="MatrixMPI-Indexing-1"></a><a class="docs-heading-anchor-permalink" href="#MatrixMPI-Indexing" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Single element
A[i, j]        # Get element
A[i, j] = x    # Set element

# Range indexing (returns MatrixMPI)
A[1:3, 2:5]    # Submatrix by ranges
A[1:3, :]      # Row range, all columns
A[:, 2:5]      # All rows, column range

# VectorMPI indices (returns MatrixMPI)
A[row_idx, col_idx]  # Both indices are VectorMPI{Int}

# Mixed indexing (returns MatrixMPI or VectorMPI)
A[row_idx, 1:5]      # VectorMPI rows, range columns
A[row_idx, :]        # VectorMPI rows, all columns
A[1:5, col_idx]      # Range rows, VectorMPI columns
A[:, col_idx]        # All rows, VectorMPI columns
A[row_idx, j]        # VectorMPI rows, single column (returns VectorMPI)
A[i, col_idx]        # Single row, VectorMPI columns (returns VectorMPI)</code></pre><h3 id="SparseMatrixMPI-Indexing"><a class="docs-heading-anchor" href="#SparseMatrixMPI-Indexing">SparseMatrixMPI Indexing</a><a id="SparseMatrixMPI-Indexing-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixMPI-Indexing" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Single element
A[i, j]        # Get element (returns 0 for structural zeros)
A[i, j] = x    # Set element (modifies structure if needed)

# Range indexing (returns SparseMatrixMPI)
A[1:3, 2:5]    # Submatrix by ranges
A[1:3, :]      # Row range, all columns
A[:, 2:5]      # All rows, column range

# VectorMPI indices (returns SparseMatrixMPI)
A[row_idx, col_idx]  # Both indices are VectorMPI{Int}

# Mixed indexing (returns SparseMatrixMPI or VectorMPI)
A[row_idx, 1:5]      # VectorMPI rows, range columns
A[row_idx, :]        # VectorMPI rows, all columns
A[1:5, col_idx]      # Range rows, VectorMPI columns
A[:, col_idx]        # All rows, VectorMPI columns
A[row_idx, j]        # VectorMPI rows, single column (returns VectorMPI)
A[i, col_idx]        # Single row, VectorMPI columns (returns VectorMPI)</code></pre><h3 id="setindex!-Source-Types"><a class="docs-heading-anchor" href="#setindex!-Source-Types">setindex! Source Types</a><a id="setindex!-Source-Types-1"></a><a class="docs-heading-anchor-permalink" href="#setindex!-Source-Types" title="Permalink"></a></h3><p>For <code>setindex!</code> operations, the source type depends on the indexing pattern:</p><table><tr><th style="text-align: right">Index Pattern</th><th style="text-align: right">Source Type</th></tr><tr><td style="text-align: right"><code>A[i, j] = x</code></td><td style="text-align: right">Scalar</td></tr><tr><td style="text-align: right"><code>A[range, range] = x</code></td><td style="text-align: right">Scalar, Matrix, or distributed matrix</td></tr><tr><td style="text-align: right"><code>A[VectorMPI, VectorMPI] = src</code></td><td style="text-align: right">MatrixMPI (matching partitions)</td></tr><tr><td style="text-align: right"><code>A[VectorMPI, range] = src</code></td><td style="text-align: right">MatrixMPI</td></tr><tr><td style="text-align: right"><code>A[range, VectorMPI] = src</code></td><td style="text-align: right">MatrixMPI</td></tr><tr><td style="text-align: right"><code>A[VectorMPI, j] = src</code></td><td style="text-align: right">VectorMPI</td></tr><tr><td style="text-align: right"><code>A[i, VectorMPI] = src</code></td><td style="text-align: right">VectorMPI</td></tr></table><h2 id="Utility-Functions"><a class="docs-heading-anchor" href="#Utility-Functions">Utility Functions</a><a id="Utility-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-Functions" title="Permalink"></a></h2><h3 id="Partition-Computation"><a class="docs-heading-anchor" href="#Partition-Computation">Partition Computation</a><a id="Partition-Computation-1"></a><a class="docs-heading-anchor-permalink" href="#Partition-Computation" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.uniform_partition"><a class="docstring-binding" href="#LinearAlgebraMPI.uniform_partition"><code>LinearAlgebraMPI.uniform_partition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">uniform_partition(n::Int, nranks::Int) -&gt; Vector{Int}</code></pre><p>Compute a balanced partition of <code>n</code> elements across <code>nranks</code> ranks. Returns a vector of length <code>nranks + 1</code> with 1-indexed partition boundaries.</p><p>The first <code>mod(n, nranks)</code> ranks get <code>div(n, nranks) + 1</code> elements, the remaining ranks get <code>div(n, nranks)</code> elements.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">partition = uniform_partition(10, 4)  # [1, 4, 7, 9, 11]
# Rank 0: 1:3 (3 elements)
# Rank 1: 4:6 (3 elements)
# Rank 2: 7:8 (2 elements)
# Rank 3: 9:10 (2 elements)</code></pre></div></section></details></article><h3 id="Rank-Selective-Output"><a class="docs-heading-anchor" href="#Rank-Selective-Output">Rank-Selective Output</a><a id="Rank-Selective-Output-1"></a><a class="docs-heading-anchor-permalink" href="#Rank-Selective-Output" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.io0"><a class="docstring-binding" href="#LinearAlgebraMPI.io0"><code>LinearAlgebraMPI.io0</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)</code></pre><p>Return <code>io</code> if the current MPI rank is in set <code>r</code>, otherwise return <code>dn</code> (default: <code>devnull</code>).</p><p>This is useful for printing only from specific ranks:</p><pre><code class="language-julia hljs">println(io0(), &quot;Hello from rank 0!&quot;)
println(io0(r=Set([0,1])), &quot;Hello from ranks 0 and 1!&quot;)</code></pre><p>With string interpolation:</p><pre><code class="language-julia hljs">println(io0(), &quot;Matrix A = $A&quot;)</code></pre></div></section></details></article><h3 id="Gathering-Distributed-Data"><a class="docs-heading-anchor" href="#Gathering-Distributed-Data">Gathering Distributed Data</a><a id="Gathering-Distributed-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Gathering-Distributed-Data" title="Permalink"></a></h3><p>Convert distributed MPI types to standard Julia types (gathers data to all ranks):</p><pre><code class="language-julia hljs">Vector(v::VectorMPI)              # Gather to Vector
Matrix(A::MatrixMPI)              # Gather to Matrix
SparseMatrixCSC(A::SparseMatrixMPI) # Gather to SparseMatrixCSC</code></pre><p>These conversions enable <code>show</code> and string interpolation:</p><pre><code class="language-julia hljs">println(io0(), &quot;Result: $v&quot;)      # Works with VectorMPI
println(io0(), &quot;Matrix: $A&quot;)      # Works with MatrixMPI/SparseMatrixMPI</code></pre><h3 id="Local-Constructors"><a class="docs-heading-anchor" href="#Local-Constructors">Local Constructors</a><a id="Local-Constructors-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Constructors" title="Permalink"></a></h3><p>Create distributed types from local data (each rank provides only its portion):</p><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.VectorMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.VectorMPI_local"><code>LinearAlgebraMPI.VectorMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">VectorMPI_local(v_local::Vector{T}, comm::MPI.Comm=MPI.COMM_WORLD) where T</code></pre><p>Create a VectorMPI from a local vector on each rank.</p><p>Unlike <code>VectorMPI(v_global)</code> which takes a global vector and partitions it, this constructor takes only the local portion of the vector that each rank owns. The partition is computed by gathering the local sizes from all ranks.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Rank 0 has [1.0, 2.0], Rank 1 has [3.0, 4.0, 5.0]
v = VectorMPI_local([1.0, 2.0])  # on rank 0
v = VectorMPI_local([3.0, 4.0, 5.0])  # on rank 1
# Result: distributed vector [1.0, 2.0, 3.0, 4.0, 5.0] with partition [1, 3, 6]</code></pre></div></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.MatrixMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.MatrixMPI_local"><code>LinearAlgebraMPI.MatrixMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">MatrixMPI_local(A_local::Matrix{T}; comm=MPI.COMM_WORLD, col_partition=...) where T</code></pre><p>Create a MatrixMPI from a local matrix on each rank.</p><p>Unlike <code>MatrixMPI(M_global)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>All ranks must have local matrices with the same number of columns. A collective error is raised if the column counts don&#39;t match.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>comm::MPI.Comm</code>: MPI communicator (default: <code>MPI.COMM_WORLD</code>)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(size(A_local,2), nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Rank 0 has 2×3 matrix, Rank 1 has 3×3 matrix
A = MatrixMPI_local(randn(2, 3))  # on rank 0
A = MatrixMPI_local(randn(3, 3))  # on rank 1
# Result: 5×3 distributed matrix with row_partition [1, 3, 6]</code></pre></div></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixMPI_local"><code>LinearAlgebraMPI.SparseMatrixMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixMPI_local(A_local::Transpose{T,SparseMatrixCSC{T,Int}}; comm=MPI.COMM_WORLD, col_partition=...) where T
SparseMatrixMPI_local(A_local::Adjoint{T,SparseMatrixCSC{T,Int}}; comm=MPI.COMM_WORLD, col_partition=...) where T</code></pre><p>Create a SparseMatrixMPI from a local sparse matrix on each rank.</p><p>Unlike <code>SparseMatrixMPI{T}(A_global)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>The input <code>A_local</code> must be a <code>Transpose</code> (or <code>Adjoint</code>) of a <code>SparseMatrixCSC{T,Int}</code> where:</p><ul><li><code>A_local.parent.n</code> = number of local rows on this rank</li><li><code>A_local.parent.m</code> = global number of columns (must match on all ranks)</li><li><code>A_local.parent.rowval</code> = global column indices</li></ul><p>All ranks must have local matrices with the same number of columns (block widths must match). A collective error is raised if the column counts don&#39;t match.</p><p>Note: For <code>Adjoint</code> inputs, the values are conjugated to match the adjoint semantics.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>comm::MPI.Comm</code>: MPI communicator (default: <code>MPI.COMM_WORLD</code>)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(A_local.parent.m, nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create local rows as transpose of CSC storage
# Rank 0 owns rows 1-2 of a 5×3 matrix, Rank 1 owns rows 3-5
local_AT = sparse([1, 2, 3], [1, 1, 2], [1.0, 2.0, 3.0], 3, 2)  # 3 cols, 2 local rows
A = SparseMatrixMPI_local(transpose(local_AT))</code></pre></div></section></details></article><h2 id="Factorization"><a class="docs-heading-anchor" href="#Factorization">Factorization</a><a id="Factorization-1"></a><a class="docs-heading-anchor-permalink" href="#Factorization" title="Permalink"></a></h2><p>LinearAlgebraMPI provides distributed sparse direct solvers using MUMPS (MUltifrontal Massively Parallel Solver).</p><h3 id="LU-Factorization"><a class="docs-heading-anchor" href="#LU-Factorization">LU Factorization</a><a id="LU-Factorization-1"></a><a class="docs-heading-anchor-permalink" href="#LU-Factorization" title="Permalink"></a></h3><pre><code class="language-julia hljs">F = lu(A::SparseMatrixMPI{T})</code></pre><p>Compute LU factorization of a distributed sparse matrix using MUMPS. Suitable for general (non-symmetric) matrices.</p><h3 id="LDLT-Factorization"><a class="docs-heading-anchor" href="#LDLT-Factorization">LDLT Factorization</a><a id="LDLT-Factorization-1"></a><a class="docs-heading-anchor-permalink" href="#LDLT-Factorization" title="Permalink"></a></h3><pre><code class="language-julia hljs">F = ldlt(A::SparseMatrixMPI{T})</code></pre><p>Compute LDLT factorization of a distributed symmetric sparse matrix using MUMPS. More efficient than LU for symmetric matrices.</p><p>Note: Uses transpose (<code>L^T</code>), not adjoint (<code>L*</code>). Correct for real symmetric and complex symmetric matrices, but NOT for complex Hermitian matrices.</p><h3 id="Solving-Linear-Systems"><a class="docs-heading-anchor" href="#Solving-Linear-Systems">Solving Linear Systems</a><a id="Solving-Linear-Systems-1"></a><a class="docs-heading-anchor-permalink" href="#Solving-Linear-Systems" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.solve"><a class="docstring-binding" href="#LinearAlgebraMPI.solve"><code>LinearAlgebraMPI.solve</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve(F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T</code></pre><p>Solve the linear system A*x = b using the precomputed MUMPS factorization.</p></div></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.solve!"><a class="docstring-binding" href="#LinearAlgebraMPI.solve!"><code>LinearAlgebraMPI.solve!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve!(x::VectorMPI{T}, F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T</code></pre><p>Solve A*x = b in-place using MUMPS factorization.</p></div></section></details></article><h3 id="Releasing-Factorization-Resources"><a class="docs-heading-anchor" href="#Releasing-Factorization-Resources">Releasing Factorization Resources</a><a id="Releasing-Factorization-Resources-1"></a><a class="docs-heading-anchor-permalink" href="#Releasing-Factorization-Resources" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.finalize!"><a class="docstring-binding" href="#LinearAlgebraMPI.finalize!"><code>LinearAlgebraMPI.finalize!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">finalize!(F::MUMPSFactorizationMPI)</code></pre><p>Release MUMPS resources. Must be called when done with the factorization.</p></div></section></details></article><h3 id="Usage-Examples"><a class="docs-heading-anchor" href="#Usage-Examples">Usage Examples</a><a id="Usage-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Usage-Examples" title="Permalink"></a></h3><pre><code class="language-julia hljs">using LinearAlgebraMPI
using LinearAlgebra
using SparseArrays

# Create a distributed sparse matrix
A_local = sprand(1000, 1000, 0.01) + 10I
A_local = A_local + A_local&#39;  # Make symmetric
A = SparseMatrixMPI{Float64}(A_local)

# LDLT factorization (for symmetric matrices)
F = ldlt(A)

# Solve Ax = b
b = VectorMPI(ones(1000))
x = solve(F, b)

# Or use backslash
x = F \ b

# Release factorization resources when done
finalize!(F)

# For non-symmetric matrices, use LU
A_nonsym = SparseMatrixMPI{Float64}(sprand(1000, 1000, 0.01) + 10I)
F_lu = lu(A_nonsym)
x = F_lu \ b
finalize!(F_lu)</code></pre><h3 id="Direct-Solve-Syntax"><a class="docs-heading-anchor" href="#Direct-Solve-Syntax">Direct Solve Syntax</a><a id="Direct-Solve-Syntax-1"></a><a class="docs-heading-anchor-permalink" href="#Direct-Solve-Syntax" title="Permalink"></a></h3><p>Both left division (<code>\</code>) and right division (<code>/</code>) are supported:</p><pre><code class="language-julia hljs"># Left division: solve A*x = b
x = A \ b
x = transpose(A) \ b    # solve transpose(A)*x = b
x = A&#39; \ b              # solve A&#39;*x = b

# Right division: solve x*A = b (for row vectors)
x = transpose(b) / A           # solve x*A = transpose(b)
x = transpose(b) / transpose(A)  # solve x*transpose(A) = transpose(b)</code></pre><p>Note: One-shot solves like <code>A \ b</code> automatically clean up the factorization. For repeated solves with the same matrix, compute the factorization once with <code>lu()</code> or <code>ldlt()</code>, reuse it, then call <code>finalize!()</code> when done.</p><h2 id="Cache-Management"><a class="docs-heading-anchor" href="#Cache-Management">Cache Management</a><a id="Cache-Management-1"></a><a class="docs-heading-anchor-permalink" href="#Cache-Management" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.clear_plan_cache!"><a class="docstring-binding" href="#LinearAlgebraMPI.clear_plan_cache!"><code>LinearAlgebraMPI.clear_plan_cache!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">clear_plan_cache!()</code></pre><p>Clear all memoized plan caches.</p></div></section></details></article><h2 id="Full-API-Index"><a class="docs-heading-anchor" href="#Full-API-Index">Full API Index</a><a id="Full-API-Index-1"></a><a class="docs-heading-anchor-permalink" href="#Full-API-Index" title="Permalink"></a></h2><ul><li><a href="#LinearAlgebraMPI.MatrixMPI"><code>LinearAlgebraMPI.MatrixMPI</code></a></li><li><a href="#LinearAlgebraMPI.SparseMatrixMPI"><code>LinearAlgebraMPI.SparseMatrixMPI</code></a></li><li><a href="#LinearAlgebraMPI.VectorMPI"><code>LinearAlgebraMPI.VectorMPI</code></a></li><li><a href="#LinearAlgebraMPI.:⊛"><code>LinearAlgebraMPI.:⊛</code></a></li><li><a href="#LinearAlgebraMPI.MatrixMPI_local"><code>LinearAlgebraMPI.MatrixMPI_local</code></a></li><li><a href="#LinearAlgebraMPI.SparseMatrixMPI_local"><code>LinearAlgebraMPI.SparseMatrixMPI_local</code></a></li><li><a href="#LinearAlgebraMPI.VectorMPI_local"><code>LinearAlgebraMPI.VectorMPI_local</code></a></li><li><a href="#LinearAlgebraMPI.clear_plan_cache!"><code>LinearAlgebraMPI.clear_plan_cache!</code></a></li><li><a href="#LinearAlgebraMPI.finalize!"><code>LinearAlgebraMPI.finalize!</code></a></li><li><a href="#LinearAlgebraMPI.io0"><code>LinearAlgebraMPI.io0</code></a></li><li><a href="#LinearAlgebraMPI.mean"><code>LinearAlgebraMPI.mean</code></a></li><li><a href="#LinearAlgebraMPI.solve"><code>LinearAlgebraMPI.solve</code></a></li><li><a href="#LinearAlgebraMPI.solve!"><code>LinearAlgebraMPI.solve!</code></a></li><li><a href="#LinearAlgebraMPI.uniform_partition"><code>LinearAlgebraMPI.uniform_partition</code></a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Monday 15 December 2025 04:07">Monday 15 December 2025</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
