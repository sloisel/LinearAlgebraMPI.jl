<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · LinearAlgebraMPI.jl</title><meta name="title" content="API Reference · LinearAlgebraMPI.jl"/><meta property="og:title" content="API Reference · LinearAlgebraMPI.jl"/><meta property="twitter:title" content="API Reference · LinearAlgebraMPI.jl"/><meta name="description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="og:description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="twitter:description" content="Documentation for LinearAlgebraMPI.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">LinearAlgebraMPI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../getting-started/">Getting Started</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Types"><span>Types</span></a></li><li><a class="tocitem" href="#CSR-Storage-Format"><span>CSR Storage Format</span></a></li><li><a class="tocitem" href="#Sparse-Matrix-Operations"><span>Sparse Matrix Operations</span></a></li><li><a class="tocitem" href="#Dense-Matrix-Operations"><span>Dense Matrix Operations</span></a></li><li><a class="tocitem" href="#Vector-Operations"><span>Vector Operations</span></a></li><li><a class="tocitem" href="#Indexing"><span>Indexing</span></a></li><li><a class="tocitem" href="#Utility-Functions"><span>Utility Functions</span></a></li><li><a class="tocitem" href="#Factorization"><span>Factorization</span></a></li><li><a class="tocitem" href="#Cache-Management"><span>Cache Management</span></a></li><li><a class="tocitem" href="#Full-API-Index"><span>Full API Index</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><p>This page documents the public API of LinearAlgebraMPI.jl.</p><h2 id="Types"><a class="docs-heading-anchor" href="#Types">Types</a><a id="Types-1"></a><a class="docs-heading-anchor-permalink" href="#Types" title="Permalink"></a></h2><h3 id="SparseMatrixMPI"><a class="docs-heading-anchor" href="#SparseMatrixMPI">SparseMatrixMPI</a><a id="SparseMatrixMPI-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixMPI"><code>LinearAlgebraMPI.SparseMatrixMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixMPI{T}</code></pre><p>A distributed sparse matrix partitioned by rows across MPI ranks.</p><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (placeholder for transpose)</li><li><code>col_indices::Vector{Int}</code>: Global column indices that appear in the local part (local→global mapping)</li><li><code>A::SparseMatrixCSR{T,Int}</code>: Local rows in CSR format for efficient row-wise iteration</li><li><code>cached_transpose</code>: Cached materialized transpose (bidirectionally linked)</li></ul><p><strong>Invariants</strong></p><ul><li><code>col_indices</code>, <code>row_partition</code>, and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows</li><li><code>col_partition[nranks+1]</code> = total number of columns</li><li><code>size(A, 1) == row_partition[rank+1] - row_partition[rank]</code> (number of local rows)</li><li><code>size(A.parent, 1) == length(col_indices)</code> (compressed column dimension)</li><li><code>A.parent.rowval</code> contains local indices in <code>1:length(col_indices)</code></li></ul><p><strong>Storage Details</strong></p><p>The local rows are stored in CSR format (Compressed Sparse Row), which enables efficient row-wise iteration - essential for a row-partitioned distributed matrix.</p><p>In Julia, <code>SparseMatrixCSR{T,Ti}</code> is a type alias for <code>Transpose{T, SparseMatrixCSC{T,Ti}}</code>. This type has a dual interpretation:</p><ul><li><strong>Semantic view</strong>: A lazy transpose of a CSC matrix</li><li><strong>Storage view</strong>: Row-major (CSR) access to the data</li></ul><p>The underlying <code>A.parent::SparseMatrixCSC</code> stores the transposed data with:</p><ul><li><code>A.parent.m = length(col_indices)</code> (compressed, not global ncols)</li><li><code>A.parent.n</code> = number of local rows (columns in the parent = rows in CSR)</li><li><code>A.parent.colptr</code> = row pointers for the CSR format</li><li><code>A.parent.rowval</code> = LOCAL column indices (1:length(col_indices))</li><li><code>col_indices[local_idx]</code> maps local→global column indices</li></ul><p>This compression avoids &quot;hypersparse&quot; storage where the column dimension would be the global number of columns even if only a few columns have nonzeros locally.</p><p>Access the underlying CSC storage via <code>A.parent</code> when needed for low-level operations.</p></div></section></details></article><h3 id="MatrixMPI"><a class="docs-heading-anchor" href="#MatrixMPI">MatrixMPI</a><a id="MatrixMPI-1"></a><a class="docs-heading-anchor-permalink" href="#MatrixMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.MatrixMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.MatrixMPI"><code>LinearAlgebraMPI.MatrixMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MatrixMPI{T}</code></pre><p>A distributed dense matrix partitioned by rows across MPI ranks.</p><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (for transpose)</li><li><code>A::Matrix{T}</code>: Local rows (NOT transposed), size = (local_nrows, ncols)</li></ul><p><strong>Invariants</strong></p><ul><li><code>row_partition</code> and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows + 1</li><li><code>col_partition[nranks+1]</code> = total number of columns + 1</li><li><code>size(A, 1) == row_partition[rank+2] - row_partition[rank+1]</code></li><li><code>size(A, 2) == col_partition[end] - 1</code></li></ul></div></section></details></article><h3 id="VectorMPI"><a class="docs-heading-anchor" href="#VectorMPI">VectorMPI</a><a id="VectorMPI-1"></a><a class="docs-heading-anchor-permalink" href="#VectorMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.VectorMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.VectorMPI"><code>LinearAlgebraMPI.VectorMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">VectorMPI{T}</code></pre><p>A distributed dense vector partitioned across MPI ranks.</p><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the partition</li><li><code>partition::Vector{Int}</code>: Partition boundaries, length = nranks + 1</li><li><code>v::Vector{T}</code>: Local vector elements owned by this rank</li></ul></div></section></details></article><h3 id="SparseMatrixCSR"><a class="docs-heading-anchor" href="#SparseMatrixCSR">SparseMatrixCSR</a><a id="SparseMatrixCSR-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixCSR" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixCSR"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixCSR"><code>LinearAlgebraMPI.SparseMatrixCSR</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixCSR{Tv,Ti} = Transpose{Tv, SparseMatrixCSC{Tv,Ti}}</code></pre><p>Type alias for CSR (Compressed Sparse Row) storage format.</p><p><strong>The Dual Life of Transpose{SparseMatrixCSC}</strong></p><p>In Julia, the type <code>Transpose{Tv, SparseMatrixCSC{Tv,Ti}}</code> has two interpretations:</p><ol><li><p><strong>Semantic interpretation</strong>: A lazy transpose wrapper around a CSC matrix. When you call <code>transpose(A)</code> on a SparseMatrixCSC, you get this wrapper that represents A^T without copying data.</p></li><li><p><strong>Storage interpretation</strong>: CSR (row-major) access to sparse data. The underlying CSC stores columns contiguously, but through the transpose wrapper, we can iterate efficiently over rows instead of columns.</p></li></ol><p>This alias clarifies intent: use <code>SparseMatrixCSR</code> when you want row-major storage semantics, and <code>transpose(A)</code> when you want the mathematical transpose.</p><p><strong>CSR vs CSC Storage</strong></p><ul><li><strong>CSC (Compressed Sparse Column)</strong>: Julia&#39;s native sparse format. Efficient for column-wise operations, matrix-vector products with column access.</li><li><strong>CSR (Compressed Sparse Row)</strong>: Efficient for row-wise operations, matrix-vector products with row access, and row-partitioned distributed matrices.</li></ul><p>For <code>SparseMatrixCSR</code>, the underlying <code>parent::SparseMatrixCSC</code> stores the <em>transposed</em> matrix. If <code>B = SparseMatrixCSR(A)</code> represents matrix M, then <code>B.parent</code> is a CSC storing M^T. This means:</p><ul><li><code>B.parent.colptr</code> acts as row pointers for M</li><li><code>B.parent.rowval</code> contains column indices for M</li><li><code>B.parent.nzval</code> contains values in row-major order</li></ul><p><strong>Usage Note</strong></p><p>Julia will still display this type as <code>Transpose{Float64, SparseMatrixCSC{...}}</code>, not as <code>SparseMatrixCSR</code>. The alias improves code clarity but doesn&#39;t affect type printing.</p></div></section></details></article><h2 id="CSR-Storage-Format"><a class="docs-heading-anchor" href="#CSR-Storage-Format">CSR Storage Format</a><a id="CSR-Storage-Format-1"></a><a class="docs-heading-anchor-permalink" href="#CSR-Storage-Format" title="Permalink"></a></h2><p>LinearAlgebraMPI uses CSR (Compressed Sparse Row) format internally for <code>SparseMatrixMPI</code> because row-partitioned distributed matrices need efficient row-wise access.</p><h3 id="The-Dual-Life-of-Transpose{SparseMatrixCSC}"><a class="docs-heading-anchor" href="#The-Dual-Life-of-Transpose{SparseMatrixCSC}">The Dual Life of Transpose{SparseMatrixCSC}</a><a id="The-Dual-Life-of-Transpose{SparseMatrixCSC}-1"></a><a class="docs-heading-anchor-permalink" href="#The-Dual-Life-of-Transpose{SparseMatrixCSC}" title="Permalink"></a></h3><p>In Julia, the type <code>Transpose{T, SparseMatrixCSC{T,Int}}</code> has two interpretations:</p><ol><li><strong>Semantic</strong>: A lazy transpose of a CSC matrix (what you get from <code>transpose(A)</code>)</li><li><strong>Storage</strong>: Row-major (CSR) access to sparse data</li></ol><p>This duality can be confusing. When you call <code>transpose(A)</code> on a SparseMatrixCSC, you get a wrapper that represents A^T. But the same wrapper type, when used for storage, provides efficient row iteration.</p><h3 id="The-SparseMatrixCSR-Type-Alias"><a class="docs-heading-anchor" href="#The-SparseMatrixCSR-Type-Alias">The SparseMatrixCSR Type Alias</a><a id="The-SparseMatrixCSR-Type-Alias-1"></a><a class="docs-heading-anchor-permalink" href="#The-SparseMatrixCSR-Type-Alias" title="Permalink"></a></h3><p>To clarify intent, LinearAlgebraMPI exports:</p><pre><code class="language-julia hljs">const SparseMatrixCSR{Tv,Ti} = Transpose{Tv, SparseMatrixCSC{Tv,Ti}}</code></pre><p>Use <code>SparseMatrixCSR</code> when you want row-major storage, and <code>transpose(A)</code> when you want the mathematical transpose.</p><h3 id="Converting-Between-CSC-and-CSR"><a class="docs-heading-anchor" href="#Converting-Between-CSC-and-CSR">Converting Between CSC and CSR</a><a id="Converting-Between-CSC-and-CSR-1"></a><a class="docs-heading-anchor-permalink" href="#Converting-Between-CSC-and-CSR" title="Permalink"></a></h3><pre><code class="language-julia hljs"># CSC to CSR (same matrix, different storage)
A_csc = sparse([1,2,2], [1,1,2], [1.0, 2.0, 3.0], 2, 2)
A_csr = SparseMatrixCSR(A_csc)
A_csr[1,1] == A_csc[1,1]  # true

# CSR to CSC
A_back = SparseMatrixCSC(A_csr)
A_back == A_csc  # true</code></pre><h3 id="Why-CSR-for-Distributed-Matrices?"><a class="docs-heading-anchor" href="#Why-CSR-for-Distributed-Matrices?">Why CSR for Distributed Matrices?</a><a id="Why-CSR-for-Distributed-Matrices?-1"></a><a class="docs-heading-anchor-permalink" href="#Why-CSR-for-Distributed-Matrices?" title="Permalink"></a></h3><p><code>SparseMatrixMPI</code> partitions matrices by rows across MPI ranks. Each rank needs to efficiently iterate over its local rows for operations like matrix-vector multiplication. CSR format provides O(1) access to each row&#39;s nonzeros, while CSC would require scanning the entire column pointer array.</p><h2 id="Sparse-Matrix-Operations"><a class="docs-heading-anchor" href="#Sparse-Matrix-Operations">Sparse Matrix Operations</a><a id="Sparse-Matrix-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Sparse-Matrix-Operations" title="Permalink"></a></h2><h3 id="Arithmetic"><a class="docs-heading-anchor" href="#Arithmetic">Arithmetic</a><a id="Arithmetic-1"></a><a class="docs-heading-anchor-permalink" href="#Arithmetic" title="Permalink"></a></h3><pre><code class="language-julia hljs">A * B          # Matrix multiplication
A + B          # Addition
A - B          # Subtraction
a * A          # Scalar multiplication
A * a          # Scalar multiplication</code></pre><h3 id="Threaded-Sparse-Multiplication"><a class="docs-heading-anchor" href="#Threaded-Sparse-Multiplication">Threaded Sparse Multiplication</a><a id="Threaded-Sparse-Multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Threaded-Sparse-Multiplication" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.:⊛"><a class="docstring-binding" href="#LinearAlgebraMPI.:⊛"><code>LinearAlgebraMPI.:⊛</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">⊛(A::SparseMatrixCSC{Tv,Ti}, B::SparseMatrixCSC{Tv,Ti}; max_threads=Threads.nthreads()) where {Tv,Ti}</code></pre><p>Multithreaded sparse matrix multiplication. Splits B into column blocks and computes <code>A * B_block</code> in parallel using Julia&#39;s optimized builtin <code>*</code>.</p><p><strong>Threading behavior</strong></p><ul><li>Uses at most <code>n ÷ 100</code> threads, where <code>n = size(B, 2)</code>, ensuring at least 100 columns per thread</li><li>Falls back to single-threaded <code>A * B</code> when <code>n &lt; 100</code> or when threading overhead would dominate</li><li>The <code>max_threads</code> keyword limits the maximum number of threads used</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using SparseArrays
A = sprand(1000, 1000, 0.01)
B = sprand(1000, 500, 0.01)
C = A ⊛ B                    # Use all available threads (up to n÷100)
C = ⊛(A, B; max_threads=2)   # Limit to 2 threads</code></pre></div></section></details></article><p>The <code>⊛</code> operator (typed as <code>\circledast&lt;tab&gt;</code>) provides multithreaded sparse matrix multiplication for <code>SparseMatrixCSC</code>. It is used internally by <code>SparseMatrixMPI</code> multiplication and can also be used directly on local sparse matrices:</p><pre><code class="language-julia hljs">using SparseArrays
A = sprand(10000, 10000, 0.001)
B = sprand(10000, 5000, 0.001)
C = A ⊛ B   # Parallel multiplication using available threads</code></pre><h3 id="Transpose-and-Adjoint"><a class="docs-heading-anchor" href="#Transpose-and-Adjoint">Transpose and Adjoint</a><a id="Transpose-and-Adjoint-1"></a><a class="docs-heading-anchor-permalink" href="#Transpose-and-Adjoint" title="Permalink"></a></h3><pre><code class="language-julia hljs">transpose(A)              # Lazy transpose
conj(A)                   # Conjugate (new matrix)
A&#39;                        # Adjoint (conjugate transpose, lazy)
SparseMatrixMPI(transpose(A))  # Materialize lazy transpose (cached)</code></pre><h3 id="Matrix-Vector-Multiplication"><a class="docs-heading-anchor" href="#Matrix-Vector-Multiplication">Matrix-Vector Multiplication</a><a id="Matrix-Vector-Multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Matrix-Vector-Multiplication" title="Permalink"></a></h3><pre><code class="language-julia hljs">y = A * x      # Returns VectorMPI
mul!(y, A, x)  # In-place version</code></pre><h3 id="Vector-Matrix-Multiplication"><a class="docs-heading-anchor" href="#Vector-Matrix-Multiplication">Vector-Matrix Multiplication</a><a id="Vector-Matrix-Multiplication-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-Matrix-Multiplication" title="Permalink"></a></h3><pre><code class="language-julia hljs">transpose(v) * A   # Row vector times matrix
v&#39; * A             # Conjugate row vector times matrix</code></pre><h3 id="Norms"><a class="docs-heading-anchor" href="#Norms">Norms</a><a id="Norms-1"></a><a class="docs-heading-anchor-permalink" href="#Norms" title="Permalink"></a></h3><pre><code class="language-julia hljs">norm(A)        # Frobenius norm (default)
norm(A, 1)     # Sum of absolute values
norm(A, Inf)   # Maximum absolute value
norm(A, p)     # General p-norm

opnorm(A, 1)   # Maximum absolute column sum
opnorm(A, Inf) # Maximum absolute row sum</code></pre><h3 id="Properties"><a class="docs-heading-anchor" href="#Properties">Properties</a><a id="Properties-1"></a><a class="docs-heading-anchor-permalink" href="#Properties" title="Permalink"></a></h3><pre><code class="language-julia hljs">size(A)        # Global dimensions (m, n)
size(A, d)     # Size along dimension d
eltype(A)      # Element type
nnz(A)         # Number of nonzeros
issparse(A)    # Returns true</code></pre><h3 id="Reductions"><a class="docs-heading-anchor" href="#Reductions">Reductions</a><a id="Reductions-1"></a><a class="docs-heading-anchor-permalink" href="#Reductions" title="Permalink"></a></h3><pre><code class="language-julia hljs">sum(A)         # Sum of all stored elements
sum(A; dims=1) # Column sums (returns VectorMPI) - SparseMatrixMPI only
sum(A; dims=2) # Row sums (returns VectorMPI) - SparseMatrixMPI only
maximum(A)     # Maximum of stored values
minimum(A)     # Minimum of stored values
tr(A)          # Trace (sum of diagonal) - SparseMatrixMPI only</code></pre><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.mean"><a class="docstring-binding" href="#LinearAlgebraMPI.mean"><code>LinearAlgebraMPI.mean</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">mean(v::VectorMPI{T}) where T</code></pre><p>Compute the mean of all elements in the distributed vector.</p></div></section><section><div><pre><code class="language-julia hljs">mean(A::SparseMatrixMPI{T}) where T</code></pre><p>Compute the mean of all elements (including implicit zeros) in the distributed sparse matrix.</p></div></section></details></article><h3 id="Element-wise-Operations"><a class="docs-heading-anchor" href="#Element-wise-Operations">Element-wise Operations</a><a id="Element-wise-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Element-wise-Operations" title="Permalink"></a></h3><pre><code class="language-julia hljs">abs(A)         # Absolute value
abs2(A)        # Squared absolute value
real(A)        # Real part
imag(A)        # Imaginary part
floor(A)       # Floor
ceil(A)        # Ceiling
round(A)       # Round</code></pre><h3 id="Utilities"><a class="docs-heading-anchor" href="#Utilities">Utilities</a><a id="Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Utilities" title="Permalink"></a></h3><pre><code class="language-julia hljs">copy(A)        # Deep copy
dropzeros(A)   # Remove stored zeros
diag(A)        # Main diagonal (returns VectorMPI)
diag(A, k)     # k-th diagonal
triu(A)        # Upper triangular
triu(A, k)     # Upper triangular from k-th diagonal
tril(A)        # Lower triangular
tril(A, k)     # Lower triangular from k-th diagonal</code></pre><h3 id="Block-Operations"><a class="docs-heading-anchor" href="#Block-Operations">Block Operations</a><a id="Block-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Block-Operations" title="Permalink"></a></h3><pre><code class="language-julia hljs">cat(A, B, C; dims=1)       # Vertical concatenation
cat(A, B, C; dims=2)       # Horizontal concatenation
cat(A, B, C, D; dims=(2,2)) # 2x2 block matrix [A B; C D]
vcat(A, B, C)              # Vertical concatenation
hcat(A, B, C)              # Horizontal concatenation
blockdiag(A, B, C)         # Block diagonal matrix</code></pre><h3 id="Diagonal-Matrix-Construction"><a class="docs-heading-anchor" href="#Diagonal-Matrix-Construction">Diagonal Matrix Construction</a><a id="Diagonal-Matrix-Construction-1"></a><a class="docs-heading-anchor-permalink" href="#Diagonal-Matrix-Construction" title="Permalink"></a></h3><pre><code class="language-julia hljs">spdiagm(v)                 # Diagonal matrix from VectorMPI
spdiagm(m, n, v)           # m x n diagonal matrix
spdiagm(k =&gt; v)            # k-th diagonal
spdiagm(0 =&gt; v, 1 =&gt; w)    # Multiple diagonals</code></pre><h2 id="Dense-Matrix-Operations"><a class="docs-heading-anchor" href="#Dense-Matrix-Operations">Dense Matrix Operations</a><a id="Dense-Matrix-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Dense-Matrix-Operations" title="Permalink"></a></h2><h3 id="Arithmetic-2"><a class="docs-heading-anchor" href="#Arithmetic-2">Arithmetic</a><a class="docs-heading-anchor-permalink" href="#Arithmetic-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">A * x          # Matrix-vector multiplication (returns VectorMPI)
transpose(A)   # Lazy transpose
conj(A)        # Conjugate
A&#39;             # Adjoint
a * A          # Scalar multiplication</code></pre><h3 id="mapslices"><a class="docs-heading-anchor" href="#mapslices">mapslices</a><a id="mapslices-1"></a><a class="docs-heading-anchor-permalink" href="#mapslices" title="Permalink"></a></h3><p>Apply a function to rows or columns of a distributed dense matrix.</p><pre><code class="language-julia hljs">mapslices(f, A; dims=2)   # Apply f to each row (local, no MPI)
mapslices(f, A; dims=1)   # Apply f to each column (requires MPI)</code></pre><p><strong>Example:</strong></p><pre><code class="language-julia hljs">using LinearAlgebra

# Create deterministic test matrix (same on all ranks)
A_global = Float64.([i + 0.1*j for i in 1:100, j in 1:10])
A = MatrixMPI(A_global)

# Compute row statistics: norm, max, sum for each row
# Transforms 100x10 to 100x3
B = mapslices(x -&gt; [norm(x), maximum(x), sum(x)], A; dims=2)</code></pre><h3 id="map_rows"><a class="docs-heading-anchor" href="#map_rows">map_rows</a><a id="map_rows-1"></a><a class="docs-heading-anchor-permalink" href="#map_rows" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.map_rows"><a class="docstring-binding" href="#LinearAlgebraMPI.map_rows"><code>LinearAlgebraMPI.map_rows</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">map_rows(f, A...)</code></pre><p>Apply function <code>f</code> to corresponding rows of distributed vectors/matrices.</p><p>Each argument in <code>A...</code> must be either a <code>VectorMPI</code> or <code>MatrixMPI</code>. All inputs are repartitioned to match the partition of the first argument before applying <code>f</code>.</p><p>For each row index i, <code>f</code> is called with the i-th row from each input:</p><ul><li>For <code>VectorMPI</code>, the i-th &quot;row&quot; is a length-1 view of element i</li><li>For <code>MatrixMPI</code>, the i-th row is a row vector (a view into the local matrix)</li></ul><p><strong>Result Type (vcat semantics)</strong></p><p>The result type depends on what <code>f</code> returns, matching the behavior of <code>vcat</code>:</p><table><tr><th style="text-align: right"><code>f</code> returns</th><th style="text-align: right">Julia type</th><th style="text-align: right">Result</th></tr><tr><td style="text-align: right">scalar</td><td style="text-align: right"><code>Number</code></td><td style="text-align: right"><code>VectorMPI</code> (one element per input row)</td></tr><tr><td style="text-align: right">column vector</td><td style="text-align: right"><code>AbstractVector</code></td><td style="text-align: right"><code>VectorMPI</code> (vcat concatenates all vectors)</td></tr><tr><td style="text-align: right">row vector</td><td style="text-align: right"><code>Transpose</code>, <code>Adjoint</code></td><td style="text-align: right"><code>MatrixMPI</code> (vcat stacks as rows)</td></tr><tr><td style="text-align: right">matrix</td><td style="text-align: right"><code>AbstractMatrix</code></td><td style="text-align: right"><code>MatrixMPI</code> (vcat stacks rows)</td></tr></table><p><strong>Lazy Wrappers</strong></p><p>Julia&#39;s <code>transpose(v)</code> and <code>v&#39;</code> (adjoint) return lazy wrappers that are subtypes of <code>AbstractMatrix</code>, so they produce <code>MatrixMPI</code> results:</p><pre><code class="language-julia hljs">map_rows(r -&gt; [1,2,3], A)           # Vector → VectorMPI (length 3n)
map_rows(r -&gt; [1,2,3]&#39;, A)          # Adjoint → MatrixMPI (n×3)
map_rows(r -&gt; transpose([1,2,3]), A) # Transpose → MatrixMPI (n×3)
map_rows(r -&gt; conj([1,2,3]), A)     # Vector → VectorMPI (length 3n)
map_rows(r -&gt; [1 2 3], A)           # Matrix literal → MatrixMPI (n×3)</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Element-wise product of two vectors
u = VectorMPI([1.0, 2.0, 3.0])
v = VectorMPI([4.0, 5.0, 6.0])
w = map_rows((a, b) -&gt; a[1] * b[1], u, v)  # VectorMPI([4.0, 10.0, 18.0])

# Row norms of a matrix
A = MatrixMPI(randn(5, 3))
norms = map_rows(r -&gt; norm(r), A)  # VectorMPI of row norms

# Expand each row to multiple elements (vcat behavior)
A = MatrixMPI(randn(3, 2))
result = map_rows(r -&gt; [1, 2, 3], A)  # VectorMPI of length 9

# Return row vectors to build a matrix
A = MatrixMPI(randn(3, 2))
result = map_rows(r -&gt; [1, 2, 3]&#39;, A)  # 3×3 MatrixMPI

# Variable-length output per row
v = VectorMPI([1.0, 2.0, 3.0])
result = map_rows(r -&gt; ones(Int(r[1])), v)  # VectorMPI of length 6 (1+2+3)

# Mixed inputs: matrix rows weighted by vector elements
A = MatrixMPI(randn(4, 3))
w = VectorMPI([1.0, 2.0, 3.0, 4.0])
result = map_rows((row, wi) -&gt; sum(row) * wi[1], A, w)  # VectorMPI</code></pre><p>This is the MPI-distributed version of:</p><pre><code class="language-julia hljs">map_rows(f, A...) = vcat((f.((eachrow.(A))...))...)</code></pre></div></section></details></article><p>Apply a function to corresponding rows of multiple distributed vectors/matrices. This is the MPI-distributed version of <code>vcat((f.((eachrow.(A))...))...)</code>.</p><p><strong>Result type follows vcat semantics:</strong></p><table><tr><th style="text-align: right"><code>f</code> returns</th><th style="text-align: right">Result</th></tr><tr><td style="text-align: right">scalar</td><td style="text-align: right"><code>VectorMPI</code> (one element per row)</td></tr><tr><td style="text-align: right">column vector <code>[1,2,3]</code></td><td style="text-align: right"><code>VectorMPI</code> (concatenated)</td></tr><tr><td style="text-align: right">row vector <code>[1,2,3]&#39;</code> or <code>transpose([1,2,3])</code></td><td style="text-align: right"><code>MatrixMPI</code> (stacked rows)</td></tr><tr><td style="text-align: right">matrix <code>[1 2 3]</code></td><td style="text-align: right"><code>MatrixMPI</code> (stacked rows)</td></tr></table><p><strong>Examples:</strong></p><pre><code class="language-julia hljs"># Row norms
A = MatrixMPI(randn(100, 10))
norms = map_rows(r -&gt; norm(r), A)  # VectorMPI of length 100

# Expand rows to vectors (vcat behavior)
result = map_rows(r -&gt; [1, 2, 3], A)  # VectorMPI of length 300

# Build matrix from row vectors
result = map_rows(r -&gt; [1, 2, 3]&#39;, A)  # 100×3 MatrixMPI

# Multiple inputs with automatic partition alignment
A = MatrixMPI(randn(100, 10))
w = VectorMPI(randn(100))
weighted = map_rows((row, wi) -&gt; sum(row) * wi[1], A, w)</code></pre><h3 id="Block-Operations-2"><a class="docs-heading-anchor" href="#Block-Operations-2">Block Operations</a><a class="docs-heading-anchor-permalink" href="#Block-Operations-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">cat(A, B; dims=1)          # Vertical concatenation
cat(A, B; dims=2)          # Horizontal concatenation
vcat(A, B)                 # Vertical concatenation
hcat(A, B)                 # Horizontal concatenation</code></pre><h3 id="Norms-2"><a class="docs-heading-anchor" href="#Norms-2">Norms</a><a class="docs-heading-anchor-permalink" href="#Norms-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">norm(A)        # Frobenius norm
norm(A, p)     # General p-norm
opnorm(A, 1)   # Maximum absolute column sum
opnorm(A, Inf) # Maximum absolute row sum</code></pre><h2 id="Vector-Operations"><a class="docs-heading-anchor" href="#Vector-Operations">Vector Operations</a><a id="Vector-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Vector-Operations" title="Permalink"></a></h2><h3 id="Arithmetic-3"><a class="docs-heading-anchor" href="#Arithmetic-3">Arithmetic</a><a class="docs-heading-anchor-permalink" href="#Arithmetic-3" title="Permalink"></a></h3><pre><code class="language-julia hljs">u + v          # Addition (auto-aligns partitions)
u - v          # Subtraction
-v             # Negation
a * v          # Scalar multiplication
v * a          # Scalar multiplication
v / a          # Scalar division</code></pre><h3 id="Transpose-and-Adjoint-2"><a class="docs-heading-anchor" href="#Transpose-and-Adjoint-2">Transpose and Adjoint</a><a class="docs-heading-anchor-permalink" href="#Transpose-and-Adjoint-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">transpose(v)   # Lazy transpose (row vector)
conj(v)        # Conjugate
v&#39;             # Adjoint</code></pre><h3 id="Norms-3"><a class="docs-heading-anchor" href="#Norms-3">Norms</a><a class="docs-heading-anchor-permalink" href="#Norms-3" title="Permalink"></a></h3><pre><code class="language-julia hljs">norm(v)        # 2-norm (default)
norm(v, 1)     # 1-norm
norm(v, Inf)   # Infinity norm
norm(v, p)     # General p-norm</code></pre><h3 id="Reductions-2"><a class="docs-heading-anchor" href="#Reductions-2">Reductions</a><a class="docs-heading-anchor-permalink" href="#Reductions-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">sum(v)         # Sum of elements
prod(v)        # Product of elements
maximum(v)     # Maximum element
minimum(v)     # Minimum element
mean(v)        # Mean of elements</code></pre><h3 id="Element-wise-Operations-2"><a class="docs-heading-anchor" href="#Element-wise-Operations-2">Element-wise Operations</a><a class="docs-heading-anchor-permalink" href="#Element-wise-Operations-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">abs(v)         # Absolute value
abs2(v)        # Squared absolute value
real(v)        # Real part
imag(v)        # Imaginary part
copy(v)        # Deep copy</code></pre><h3 id="Broadcasting"><a class="docs-heading-anchor" href="#Broadcasting">Broadcasting</a><a id="Broadcasting-1"></a><a class="docs-heading-anchor-permalink" href="#Broadcasting" title="Permalink"></a></h3><p>VectorMPI supports broadcasting for element-wise operations:</p><pre><code class="language-julia hljs">v .+ w         # Element-wise addition
v .* w         # Element-wise multiplication
sin.(v)        # Apply function element-wise
v .* 2.0 .+ w  # Compound expressions</code></pre><h3 id="Block-Operations-3"><a class="docs-heading-anchor" href="#Block-Operations-3">Block Operations</a><a class="docs-heading-anchor-permalink" href="#Block-Operations-3" title="Permalink"></a></h3><pre><code class="language-julia hljs">vcat(u, v, w)  # Concatenate vectors (returns VectorMPI)
hcat(u, v, w)  # Stack as columns (returns MatrixMPI)</code></pre><h3 id="Properties-2"><a class="docs-heading-anchor" href="#Properties-2">Properties</a><a class="docs-heading-anchor-permalink" href="#Properties-2" title="Permalink"></a></h3><pre><code class="language-julia hljs">length(v)      # Global length
size(v)        # Returns (length,)
eltype(v)      # Element type</code></pre><h2 id="Indexing"><a class="docs-heading-anchor" href="#Indexing">Indexing</a><a id="Indexing-1"></a><a class="docs-heading-anchor-permalink" href="#Indexing" title="Permalink"></a></h2><p>All distributed types support element access and assignment. These are collective operations - all MPI ranks must call them with the same arguments.</p><h3 id="VectorMPI-Indexing"><a class="docs-heading-anchor" href="#VectorMPI-Indexing">VectorMPI Indexing</a><a id="VectorMPI-Indexing-1"></a><a class="docs-heading-anchor-permalink" href="#VectorMPI-Indexing" title="Permalink"></a></h3><pre><code class="language-julia hljs">v[i]           # Get element (collective)
v[i] = x       # Set element (collective)
v[1:10]        # Range indexing (returns VectorMPI)
v[1:10] = x    # Range assignment (scalar or vector)
v[idx]         # VectorMPI{Int} indexing (returns VectorMPI)
v[idx] = src   # VectorMPI{Int} assignment (src::VectorMPI)</code></pre><h3 id="MatrixMPI-Indexing"><a class="docs-heading-anchor" href="#MatrixMPI-Indexing">MatrixMPI Indexing</a><a id="MatrixMPI-Indexing-1"></a><a class="docs-heading-anchor-permalink" href="#MatrixMPI-Indexing" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Single element
A[i, j]        # Get element
A[i, j] = x    # Set element

# Range indexing (returns MatrixMPI)
A[1:3, 2:5]    # Submatrix by ranges
A[1:3, :]      # Row range, all columns
A[:, 2:5]      # All rows, column range

# VectorMPI indices (returns MatrixMPI)
A[row_idx, col_idx]  # Both indices are VectorMPI{Int}

# Mixed indexing (returns MatrixMPI or VectorMPI)
A[row_idx, 1:5]      # VectorMPI rows, range columns
A[row_idx, :]        # VectorMPI rows, all columns
A[1:5, col_idx]      # Range rows, VectorMPI columns
A[:, col_idx]        # All rows, VectorMPI columns
A[row_idx, j]        # VectorMPI rows, single column (returns VectorMPI)
A[i, col_idx]        # Single row, VectorMPI columns (returns VectorMPI)</code></pre><h3 id="SparseMatrixMPI-Indexing"><a class="docs-heading-anchor" href="#SparseMatrixMPI-Indexing">SparseMatrixMPI Indexing</a><a id="SparseMatrixMPI-Indexing-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixMPI-Indexing" title="Permalink"></a></h3><pre><code class="language-julia hljs"># Single element
A[i, j]        # Get element (returns 0 for structural zeros)
A[i, j] = x    # Set element (modifies structure if needed)

# Range indexing (returns SparseMatrixMPI)
A[1:3, 2:5]    # Submatrix by ranges
A[1:3, :]      # Row range, all columns
A[:, 2:5]      # All rows, column range

# VectorMPI indices (returns SparseMatrixMPI)
A[row_idx, col_idx]  # Both indices are VectorMPI{Int}

# Mixed indexing (returns SparseMatrixMPI or VectorMPI)
A[row_idx, 1:5]      # VectorMPI rows, range columns
A[row_idx, :]        # VectorMPI rows, all columns
A[1:5, col_idx]      # Range rows, VectorMPI columns
A[:, col_idx]        # All rows, VectorMPI columns
A[row_idx, j]        # VectorMPI rows, single column (returns VectorMPI)
A[i, col_idx]        # Single row, VectorMPI columns (returns VectorMPI)</code></pre><h3 id="setindex!-Source-Types"><a class="docs-heading-anchor" href="#setindex!-Source-Types">setindex! Source Types</a><a id="setindex!-Source-Types-1"></a><a class="docs-heading-anchor-permalink" href="#setindex!-Source-Types" title="Permalink"></a></h3><p>For <code>setindex!</code> operations, the source type depends on the indexing pattern:</p><table><tr><th style="text-align: right">Index Pattern</th><th style="text-align: right">Source Type</th></tr><tr><td style="text-align: right"><code>A[i, j] = x</code></td><td style="text-align: right">Scalar</td></tr><tr><td style="text-align: right"><code>A[range, range] = x</code></td><td style="text-align: right">Scalar, Matrix, or distributed matrix</td></tr><tr><td style="text-align: right"><code>A[VectorMPI, VectorMPI] = src</code></td><td style="text-align: right">MatrixMPI (matching partitions)</td></tr><tr><td style="text-align: right"><code>A[VectorMPI, range] = src</code></td><td style="text-align: right">MatrixMPI</td></tr><tr><td style="text-align: right"><code>A[range, VectorMPI] = src</code></td><td style="text-align: right">MatrixMPI</td></tr><tr><td style="text-align: right"><code>A[VectorMPI, j] = src</code></td><td style="text-align: right">VectorMPI</td></tr><tr><td style="text-align: right"><code>A[i, VectorMPI] = src</code></td><td style="text-align: right">VectorMPI</td></tr></table><h2 id="Utility-Functions"><a class="docs-heading-anchor" href="#Utility-Functions">Utility Functions</a><a id="Utility-Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-Functions" title="Permalink"></a></h2><h3 id="Partition-Computation"><a class="docs-heading-anchor" href="#Partition-Computation">Partition Computation</a><a id="Partition-Computation-1"></a><a class="docs-heading-anchor-permalink" href="#Partition-Computation" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.uniform_partition"><a class="docstring-binding" href="#LinearAlgebraMPI.uniform_partition"><code>LinearAlgebraMPI.uniform_partition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">uniform_partition(n::Int, nranks::Int) -&gt; Vector{Int}</code></pre><p>Compute a balanced partition of <code>n</code> elements across <code>nranks</code> ranks. Returns a vector of length <code>nranks + 1</code> with 1-indexed partition boundaries.</p><p>The first <code>mod(n, nranks)</code> ranks get <code>div(n, nranks) + 1</code> elements, the remaining ranks get <code>div(n, nranks)</code> elements.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">partition = uniform_partition(10, 4)  # [1, 4, 7, 9, 11]
# Rank 0: 1:3 (3 elements)
# Rank 1: 4:6 (3 elements)
# Rank 2: 7:8 (2 elements)
# Rank 3: 9:10 (2 elements)</code></pre></div></section></details></article><h3 id="Repartitioning"><a class="docs-heading-anchor" href="#Repartitioning">Repartitioning</a><a id="Repartitioning-1"></a><a class="docs-heading-anchor-permalink" href="#Repartitioning" title="Permalink"></a></h3><p>Redistribute distributed data to a new partition. Provides plan caching for repeated operations and a fast path when partitions already match (no communication needed).</p><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.repartition"><a class="docstring-binding" href="#LinearAlgebraMPI.repartition"><code>LinearAlgebraMPI.repartition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">repartition(x::VectorMPI{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a VectorMPI to a new partition <code>p</code>.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == length(x) + 1</code>.</p><p>Returns a new VectorMPI with the same data but <code>partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">v = VectorMPI([1.0, 2.0, 3.0, 4.0])  # uniform partition
new_partition = [1, 2, 5]  # rank 0 gets 1 element, rank 1 gets 3
v_repart = repartition(v, new_partition)</code></pre></div></section><section><div><pre><code class="language-julia hljs">repartition(A::MatrixMPI{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a MatrixMPI to a new row partition <code>p</code>. The col_partition remains unchanged.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == size(A, 1) + 1</code>.</p><p>Returns a new MatrixMPI with the same data but <code>row_partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = MatrixMPI(randn(6, 4))  # uniform partition
new_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank
A_repart = repartition(A, new_partition)</code></pre></div></section><section><div><pre><code class="language-julia hljs">repartition(A::SparseMatrixMPI{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a SparseMatrixMPI to a new row partition <code>p</code>. The col_partition remains unchanged.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == size(A, 1) + 1</code>.</p><p>Returns a new SparseMatrixMPI with the same data but <code>row_partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = SparseMatrixMPI{Float64}(sprand(6, 4, 0.5))  # uniform partition
new_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank
A_repart = repartition(A, new_partition)</code></pre></div></section></details></article><h3 id="Rank-Selective-Output"><a class="docs-heading-anchor" href="#Rank-Selective-Output">Rank-Selective Output</a><a id="Rank-Selective-Output-1"></a><a class="docs-heading-anchor-permalink" href="#Rank-Selective-Output" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.io0"><a class="docstring-binding" href="#LinearAlgebraMPI.io0"><code>LinearAlgebraMPI.io0</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)</code></pre><p>Return <code>io</code> if the current MPI rank is in set <code>r</code>, otherwise return <code>dn</code> (default: <code>devnull</code>).</p><p>This is useful for printing only from specific ranks:</p><pre><code class="language-julia hljs">println(io0(), &quot;Hello from rank 0!&quot;)
println(io0(r=Set([0,1])), &quot;Hello from ranks 0 and 1!&quot;)</code></pre><p>With string interpolation:</p><pre><code class="language-julia hljs">println(io0(), &quot;Matrix A = $A&quot;)</code></pre></div></section></details></article><h3 id="Gathering-Distributed-Data"><a class="docs-heading-anchor" href="#Gathering-Distributed-Data">Gathering Distributed Data</a><a id="Gathering-Distributed-Data-1"></a><a class="docs-heading-anchor-permalink" href="#Gathering-Distributed-Data" title="Permalink"></a></h3><p>Convert distributed MPI types to standard Julia types (gathers data to all ranks):</p><pre><code class="language-julia hljs">Vector(v::VectorMPI)              # Gather to Vector
Matrix(A::MatrixMPI)              # Gather to Matrix
SparseMatrixCSC(A::SparseMatrixMPI) # Gather to SparseMatrixCSC</code></pre><p>These conversions enable <code>show</code> and string interpolation:</p><pre><code class="language-julia hljs">println(io0(), &quot;Result: $v&quot;)      # Works with VectorMPI
println(io0(), &quot;Matrix: $A&quot;)      # Works with MatrixMPI/SparseMatrixMPI</code></pre><h3 id="Local-Constructors"><a class="docs-heading-anchor" href="#Local-Constructors">Local Constructors</a><a id="Local-Constructors-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Constructors" title="Permalink"></a></h3><p>Create distributed types from local data (each rank provides only its portion):</p><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.VectorMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.VectorMPI_local"><code>LinearAlgebraMPI.VectorMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">VectorMPI_local(v_local::Vector{T}, comm::MPI.Comm=MPI.COMM_WORLD) where T</code></pre><p>Create a VectorMPI from a local vector on each rank.</p><p>Unlike <code>VectorMPI(v_global)</code> which takes a global vector and partitions it, this constructor takes only the local portion of the vector that each rank owns. The partition is computed by gathering the local sizes from all ranks.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Rank 0 has [1.0, 2.0], Rank 1 has [3.0, 4.0, 5.0]
v = VectorMPI_local([1.0, 2.0])  # on rank 0
v = VectorMPI_local([3.0, 4.0, 5.0])  # on rank 1
# Result: distributed vector [1.0, 2.0, 3.0, 4.0, 5.0] with partition [1, 3, 6]</code></pre></div></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.MatrixMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.MatrixMPI_local"><code>LinearAlgebraMPI.MatrixMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">MatrixMPI_local(A_local::Matrix{T}; comm=MPI.COMM_WORLD, col_partition=...) where T</code></pre><p>Create a MatrixMPI from a local matrix on each rank.</p><p>Unlike <code>MatrixMPI(M_global)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>All ranks must have local matrices with the same number of columns. A collective error is raised if the column counts don&#39;t match.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>comm::MPI.Comm</code>: MPI communicator (default: <code>MPI.COMM_WORLD</code>)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(size(A_local,2), nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Rank 0 has 2×3 matrix, Rank 1 has 3×3 matrix
A = MatrixMPI_local(randn(2, 3))  # on rank 0
A = MatrixMPI_local(randn(3, 3))  # on rank 1
# Result: 5×3 distributed matrix with row_partition [1, 3, 6]</code></pre></div></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixMPI_local"><code>LinearAlgebraMPI.SparseMatrixMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixMPI_local(A_local::SparseMatrixCSR{T,Int}; comm=MPI.COMM_WORLD, col_partition=...) where T
SparseMatrixMPI_local(A_local::Adjoint{T,SparseMatrixCSC{T,Int}}; comm=MPI.COMM_WORLD, col_partition=...) where T</code></pre><p>Create a SparseMatrixMPI from a local sparse matrix on each rank.</p><p>Unlike <code>SparseMatrixMPI{T}(A_global)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>The input <code>A_local</code> must be a <code>SparseMatrixCSR{T,Int}</code> (or <code>Adjoint</code> of <code>SparseMatrixCSC{T,Int}</code>) where:</p><ul><li><code>A_local.parent.n</code> = number of local rows on this rank</li><li><code>A_local.parent.m</code> = global number of columns (must match on all ranks)</li><li><code>A_local.parent.rowval</code> = global column indices</li></ul><p>All ranks must have local matrices with the same number of columns (block widths must match). A collective error is raised if the column counts don&#39;t match.</p><p>Note: For <code>Adjoint</code> inputs, the values are conjugated to match the adjoint semantics.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>comm::MPI.Comm</code>: MPI communicator (default: <code>MPI.COMM_WORLD</code>)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(A_local.parent.m, nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create local rows in CSR format
# Rank 0 owns rows 1-2 of a 5×3 matrix, Rank 1 owns rows 3-5
local_csc = sparse([1, 1, 2], [1, 2, 3], [1.0, 2.0, 3.0], 2, 3)  # 2 local rows, 3 cols
A = SparseMatrixMPI_local(SparseMatrixCSR(local_csc))</code></pre></div></section></details></article><h2 id="Factorization"><a class="docs-heading-anchor" href="#Factorization">Factorization</a><a id="Factorization-1"></a><a class="docs-heading-anchor-permalink" href="#Factorization" title="Permalink"></a></h2><p>LinearAlgebraMPI provides distributed sparse direct solvers using MUMPS (MUltifrontal Massively Parallel Solver).</p><h3 id="LU-Factorization"><a class="docs-heading-anchor" href="#LU-Factorization">LU Factorization</a><a id="LU-Factorization-1"></a><a class="docs-heading-anchor-permalink" href="#LU-Factorization" title="Permalink"></a></h3><pre><code class="language-julia hljs">F = lu(A::SparseMatrixMPI{T})</code></pre><p>Compute LU factorization of a distributed sparse matrix using MUMPS. Suitable for general (non-symmetric) matrices.</p><h3 id="LDLT-Factorization"><a class="docs-heading-anchor" href="#LDLT-Factorization">LDLT Factorization</a><a id="LDLT-Factorization-1"></a><a class="docs-heading-anchor-permalink" href="#LDLT-Factorization" title="Permalink"></a></h3><pre><code class="language-julia hljs">F = ldlt(A::SparseMatrixMPI{T})</code></pre><p>Compute LDLT factorization of a distributed symmetric sparse matrix using MUMPS. More efficient than LU for symmetric matrices.</p><p>Note: Uses transpose (<code>L^T</code>), not adjoint (<code>L*</code>). Correct for real symmetric and complex symmetric matrices, but NOT for complex Hermitian matrices.</p><h3 id="Solving-Linear-Systems"><a class="docs-heading-anchor" href="#Solving-Linear-Systems">Solving Linear Systems</a><a id="Solving-Linear-Systems-1"></a><a class="docs-heading-anchor-permalink" href="#Solving-Linear-Systems" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.solve"><a class="docstring-binding" href="#LinearAlgebraMPI.solve"><code>LinearAlgebraMPI.solve</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve(F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T</code></pre><p>Solve the linear system A*x = b using the precomputed MUMPS factorization.</p></div></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.solve!"><a class="docstring-binding" href="#LinearAlgebraMPI.solve!"><code>LinearAlgebraMPI.solve!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve!(x::VectorMPI{T}, F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T</code></pre><p>Solve A*x = b in-place using MUMPS factorization.</p></div></section></details></article><h3 id="Resource-Management"><a class="docs-heading-anchor" href="#Resource-Management">Resource Management</a><a id="Resource-Management-1"></a><a class="docs-heading-anchor-permalink" href="#Resource-Management" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.finalize!"><a class="docstring-binding" href="#LinearAlgebraMPI.finalize!"><code>LinearAlgebraMPI.finalize!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">finalize!(F::MUMPSFactorizationMPI)</code></pre><p>Release MUMPS resources. Must be called on all ranks together.</p></div></section></details></article><h3 id="Usage-Examples"><a class="docs-heading-anchor" href="#Usage-Examples">Usage Examples</a><a id="Usage-Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Usage-Examples" title="Permalink"></a></h3><pre><code class="language-julia hljs">using LinearAlgebraMPI
using LinearAlgebra
using SparseArrays

# Create a distributed sparse matrix
A_local = sprand(1000, 1000, 0.01) + 10I
A_local = A_local + A_local&#39;  # Make symmetric
A = SparseMatrixMPI{Float64}(A_local)

# LDLT factorization (for symmetric matrices)
F = ldlt(A)

# Solve Ax = b
b = VectorMPI(ones(1000))
x = solve(F, b)

# Or use backslash
x = F \ b

# For non-symmetric matrices, use LU
A_nonsym = SparseMatrixMPI{Float64}(sprand(1000, 1000, 0.01) + 10I)
F_lu = lu(A_nonsym)
x = F_lu \ b</code></pre><h3 id="Direct-Solve-Syntax"><a class="docs-heading-anchor" href="#Direct-Solve-Syntax">Direct Solve Syntax</a><a id="Direct-Solve-Syntax-1"></a><a class="docs-heading-anchor-permalink" href="#Direct-Solve-Syntax" title="Permalink"></a></h3><p>Both left division (<code>\</code>) and right division (<code>/</code>) are supported:</p><pre><code class="language-julia hljs"># Left division: solve A*x = b
x = A \ b
x = transpose(A) \ b    # solve transpose(A)*x = b
x = A&#39; \ b              # solve A&#39;*x = b

# Right division: solve x*A = b (for row vectors)
x = transpose(b) / A           # solve x*A = transpose(b)
x = transpose(b) / transpose(A)  # solve x*transpose(A) = transpose(b)</code></pre><h2 id="Cache-Management"><a class="docs-heading-anchor" href="#Cache-Management">Cache Management</a><a id="Cache-Management-1"></a><a class="docs-heading-anchor-permalink" href="#Cache-Management" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.clear_plan_cache!"><a class="docstring-binding" href="#LinearAlgebraMPI.clear_plan_cache!"><code>LinearAlgebraMPI.clear_plan_cache!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">clear_plan_cache!()</code></pre><p>Clear all memoized plan caches.</p></div></section></details></article><h2 id="Full-API-Index"><a class="docs-heading-anchor" href="#Full-API-Index">Full API Index</a><a id="Full-API-Index-1"></a><a class="docs-heading-anchor-permalink" href="#Full-API-Index" title="Permalink"></a></h2><ul><li><a href="#LinearAlgebraMPI.MatrixMPI"><code>LinearAlgebraMPI.MatrixMPI</code></a></li><li><a href="#LinearAlgebraMPI.SparseMatrixCSR"><code>LinearAlgebraMPI.SparseMatrixCSR</code></a></li><li><a href="#LinearAlgebraMPI.SparseMatrixMPI"><code>LinearAlgebraMPI.SparseMatrixMPI</code></a></li><li><a href="#LinearAlgebraMPI.VectorMPI"><code>LinearAlgebraMPI.VectorMPI</code></a></li><li><a href="#LinearAlgebraMPI.:⊛"><code>LinearAlgebraMPI.:⊛</code></a></li><li><a href="#LinearAlgebraMPI.MatrixMPI_local"><code>LinearAlgebraMPI.MatrixMPI_local</code></a></li><li><a href="#LinearAlgebraMPI.SparseMatrixMPI_local"><code>LinearAlgebraMPI.SparseMatrixMPI_local</code></a></li><li><a href="#LinearAlgebraMPI.VectorMPI_local"><code>LinearAlgebraMPI.VectorMPI_local</code></a></li><li><a href="#LinearAlgebraMPI.clear_plan_cache!"><code>LinearAlgebraMPI.clear_plan_cache!</code></a></li><li><a href="#LinearAlgebraMPI.finalize!"><code>LinearAlgebraMPI.finalize!</code></a></li><li><a href="#LinearAlgebraMPI.io0"><code>LinearAlgebraMPI.io0</code></a></li><li><a href="#LinearAlgebraMPI.map_rows"><code>LinearAlgebraMPI.map_rows</code></a></li><li><a href="#LinearAlgebraMPI.mean"><code>LinearAlgebraMPI.mean</code></a></li><li><a href="#LinearAlgebraMPI.repartition"><code>LinearAlgebraMPI.repartition</code></a></li><li><a href="#LinearAlgebraMPI.solve"><code>LinearAlgebraMPI.solve</code></a></li><li><a href="#LinearAlgebraMPI.solve!"><code>LinearAlgebraMPI.solve!</code></a></li><li><a href="#LinearAlgebraMPI.uniform_partition"><code>LinearAlgebraMPI.uniform_partition</code></a></li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Thursday 18 December 2025 08:24">Thursday 18 December 2025</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
