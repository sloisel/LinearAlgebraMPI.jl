<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · LinearAlgebraMPI.jl</title><meta name="title" content="API Reference · LinearAlgebraMPI.jl"/><meta property="og:title" content="API Reference · LinearAlgebraMPI.jl"/><meta property="twitter:title" content="API Reference · LinearAlgebraMPI.jl"/><meta name="description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="og:description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="twitter:description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="og:url" content="https://sloisel.github.io/LinearAlgebraMPI.jl/api/"/><meta property="twitter:url" content="https://sloisel.github.io/LinearAlgebraMPI.jl/api/"/><link rel="canonical" href="https://sloisel.github.io/LinearAlgebraMPI.jl/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">LinearAlgebraMPI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../guide/">User Guide</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Distributed-Types"><span>Distributed Types</span></a></li><li><a class="tocitem" href="#Local-Constructors"><span>Local Constructors</span></a></li><li><a class="tocitem" href="#Row-wise-Operations"><span>Row-wise Operations</span></a></li><li><a class="tocitem" href="#Linear-System-Solvers"><span>Linear System Solvers</span></a></li><li><a class="tocitem" href="#Partition-Utilities"><span>Partition Utilities</span></a></li><li><a class="tocitem" href="#Cache-Management"><span>Cache Management</span></a></li><li><a class="tocitem" href="#IO-Utilities"><span>IO Utilities</span></a></li><li><a class="tocitem" href="#Type-Mappings"><span>Type Mappings</span></a></li><li><a class="tocitem" href="#Supported-Operations"><span>Supported Operations</span></a></li><li><a class="tocitem" href="#Factorization-Types"><span>Factorization Types</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sloisel/LinearAlgebraMPI.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/main/docs/src/api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><p>This page provides detailed documentation for all exported types and functions in LinearAlgebraMPI.jl.</p><div class="admonition is-info" id="MPI-Collective-Operations-a1387683e48aabeb"><header class="admonition-header">MPI Collective Operations<a class="admonition-anchor" href="#MPI-Collective-Operations-a1387683e48aabeb" title="Permalink"></a></header><div class="admonition-body"><p>Unless otherwise noted, all functions are MPI collective operations. Every MPI rank must call these functions together.</p></div></div><h2 id="Distributed-Types"><a class="docs-heading-anchor" href="#Distributed-Types">Distributed Types</a><a id="Distributed-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Types" title="Permalink"></a></h2><h3 id="VectorMPI"><a class="docs-heading-anchor" href="#VectorMPI">VectorMPI</a><a id="VectorMPI-1"></a><a class="docs-heading-anchor-permalink" href="#VectorMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.VectorMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.VectorMPI"><code>LinearAlgebraMPI.VectorMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">VectorMPI{T,AV}</code></pre><p>A distributed dense vector partitioned across MPI ranks.</p><p><strong>Type Parameters</strong></p><ul><li><code>T</code>: Element type (e.g., <code>Float64</code>, <code>ComplexF64</code>)</li><li><code>AV&lt;:AbstractVector{T}</code>: Storage type for local vector (e.g., <code>Vector{T}</code> or <code>MtlVector{T}</code>)</li></ul><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the partition</li><li><code>partition::Vector{Int}</code>: Partition boundaries, length = nranks + 1 (always on CPU)</li><li><code>v::AV</code>: Local vector elements owned by this rank</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/vectors.jl#L6-L19">source</a></section></details></article><h3 id="MatrixMPI"><a class="docs-heading-anchor" href="#MatrixMPI">MatrixMPI</a><a id="MatrixMPI-1"></a><a class="docs-heading-anchor-permalink" href="#MatrixMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.MatrixMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.MatrixMPI"><code>LinearAlgebraMPI.MatrixMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MatrixMPI{T,AM}</code></pre><p>A distributed dense matrix partitioned by rows across MPI ranks.</p><p><strong>Type Parameters</strong></p><ul><li><code>T</code>: Element type (e.g., <code>Float64</code>, <code>ComplexF64</code>)</li><li><code>AM&lt;:AbstractMatrix{T}</code>: Storage type for local matrix (e.g., <code>Matrix{T}</code> or <code>MtlMatrix{T}</code>)</li></ul><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1 (always on CPU)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (always on CPU)</li><li><code>A::AM</code>: Local rows (NOT transposed), size = (local_nrows, ncols)</li></ul><p><strong>Invariants</strong></p><ul><li><code>row_partition</code> and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows + 1</li><li><code>col_partition[nranks+1]</code> = total number of columns + 1</li><li><code>size(A, 1) == row_partition[rank+2] - row_partition[rank+1]</code></li><li><code>size(A, 2) == col_partition[end] - 1</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/dense.jl#L36-L57">source</a></section></details></article><h3 id="SparseMatrixMPI"><a class="docs-heading-anchor" href="#SparseMatrixMPI">SparseMatrixMPI</a><a id="SparseMatrixMPI-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixMPI"><code>LinearAlgebraMPI.SparseMatrixMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixMPI{T,Ti,AV}</code></pre><p>A distributed sparse matrix partitioned by rows across MPI ranks.</p><p><strong>Type Parameters</strong></p><ul><li><code>T</code>: Element type (e.g., <code>Float64</code>, <code>ComplexF64</code>)</li><li><code>Ti</code>: Index type (e.g., <code>Int</code>, <code>Int32</code>), defaults to <code>Int</code></li><li><code>AV&lt;:AbstractVector{T}</code>: Storage type for nonzero values (<code>Vector{T}</code> for CPU, <code>MtlVector{T}</code> for GPU)</li></ul><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (placeholder for transpose)</li><li><code>col_indices::Vector{Int}</code>: Global column indices that appear in the local part (local→global mapping)</li><li><code>rowptr::Vector{Ti}</code>: Row pointers for CSR format (always CPU)</li><li><code>colval::Vector{Ti}</code>: LOCAL column indices 1:length(col_indices) for each nonzero (always CPU)</li><li><code>nzval::AV</code>: Nonzero values (CPU or GPU)</li><li><code>nrows_local::Int</code>: Number of local rows</li><li><code>ncols_compressed::Int</code>: Number of unique columns = length(col_indices)</li><li><code>cached_transpose</code>: Cached materialized transpose (bidirectionally linked)</li></ul><p><strong>Invariants</strong></p><ul><li><code>col_indices</code>, <code>row_partition</code>, and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows</li><li><code>col_partition[nranks+1]</code> = total number of columns</li><li><code>nrows_local == row_partition[rank+1] - row_partition[rank]</code> (number of local rows)</li><li><code>ncols_compressed == length(col_indices)</code> (compressed column dimension)</li><li><code>colval</code> contains local indices in <code>1:ncols_compressed</code></li><li><code>rowptr</code> has length <code>nrows_local + 1</code></li></ul><p><strong>Storage Details</strong></p><p>The local rows are stored in CSR format (Compressed Sparse Row), which enables efficient row-wise iteration - essential for a row-partitioned distributed matrix.</p><p>The CSR storage consists of:</p><ul><li><code>rowptr</code>: Row pointers where row i has nonzeros at positions rowptr[i]:(rowptr[i+1]-1)</li><li><code>colval</code>: LOCAL column indices (1:ncols_compressed), not global indices</li><li><code>nzval</code>: Nonzero values</li><li><code>col_indices[local_idx]</code> maps local→global column indices</li></ul><p>This compression avoids &quot;hypersparse&quot; storage where the column dimension would be the global number of columns even if only a few columns have nonzeros locally.</p><p><strong>GPU Support</strong></p><p>Structure arrays (<code>rowptr</code>, <code>colval</code>) always stay on CPU for MPI communication and indexing. Only <code>nzval</code> can live on GPU, with type determined by <code>AV</code>:</p><ul><li><code>Vector{T}</code>: CPU storage</li><li><code>MtlVector{T}</code>: Metal GPU storage (macOS)</li></ul><p>Use <code>mtl(A)</code> to convert to GPU, <code>cpu(A)</code> to convert back.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/sparse.jl#L266-L317">source</a></section></details></article><h3 id="SparseMatrixCSR"><a class="docs-heading-anchor" href="#SparseMatrixCSR">SparseMatrixCSR</a><a id="SparseMatrixCSR-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixCSR" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixCSR"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixCSR"><code>LinearAlgebraMPI.SparseMatrixCSR</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixCSR{Tv,Ti} = Transpose{Tv, SparseMatrixCSC{Tv,Ti}}</code></pre><p>Type alias for CSR (Compressed Sparse Row) storage format.</p><p><strong>The Dual Life of Transpose{SparseMatrixCSC}</strong></p><p>In Julia, the type <code>Transpose{Tv, SparseMatrixCSC{Tv,Ti}}</code> has two interpretations:</p><ol><li><p><strong>Semantic interpretation</strong>: A lazy transpose wrapper around a CSC matrix. When you call <code>transpose(A)</code> on a SparseMatrixCSC, you get this wrapper that represents A^T without copying data.</p></li><li><p><strong>Storage interpretation</strong>: CSR (row-major) access to sparse data. The underlying CSC stores columns contiguously, but through the transpose wrapper, we can iterate efficiently over rows instead of columns.</p></li></ol><p>This alias clarifies intent: use <code>SparseMatrixCSR</code> when you want row-major storage semantics, and <code>transpose(A)</code> when you want the mathematical transpose.</p><p><strong>CSR vs CSC Storage</strong></p><ul><li><strong>CSC (Compressed Sparse Column)</strong>: Julia&#39;s native sparse format. Efficient for column-wise operations, matrix-vector products with column access.</li><li><strong>CSR (Compressed Sparse Row)</strong>: Efficient for row-wise operations, matrix-vector products with row access, and row-partitioned distributed matrices.</li></ul><p>For <code>SparseMatrixCSR</code>, the underlying <code>parent::SparseMatrixCSC</code> stores the <em>transposed</em> matrix. If <code>B = SparseMatrixCSR(A)</code> represents matrix M, then <code>B.parent</code> is a CSC storing M^T. This means:</p><ul><li><code>B.parent.colptr</code> acts as row pointers for M</li><li><code>B.parent.rowval</code> contains column indices for M</li><li><code>B.parent.nzval</code> contains values in row-major order</li></ul><p><strong>Usage Note</strong></p><p>Julia will still display this type as <code>Transpose{Float64, SparseMatrixCSC{...}}</code>, not as <code>SparseMatrixCSR</code>. The alias improves code clarity but doesn&#39;t affect type printing.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/LinearAlgebraMPI.jl#L35-L74">source</a></section></details></article><h2 id="Local-Constructors"><a class="docs-heading-anchor" href="#Local-Constructors">Local Constructors</a><a id="Local-Constructors-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Constructors" title="Permalink"></a></h2><p>These constructors create distributed types from local data without global communication.</p><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.VectorMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.VectorMPI_local"><code>LinearAlgebraMPI.VectorMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">VectorMPI_local(v_local::AbstractVector{T}, comm::MPI.Comm=MPI.COMM_WORLD) where T</code></pre><p>Create a VectorMPI from a local vector on each rank.</p><p>Unlike <code>VectorMPI(v_global)</code> which takes a global vector and partitions it, this constructor takes only the local portion of the vector that each rank owns. The partition is computed by gathering the local sizes from all ranks.</p><p>The type of <code>v_local</code> determines the storage backend (CPU Vector, GPU MtlVector, etc.).</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Rank 0 has [1.0, 2.0], Rank 1 has [3.0, 4.0, 5.0]
v = VectorMPI_local([1.0, 2.0])  # on rank 0
v = VectorMPI_local([3.0, 4.0, 5.0])  # on rank 1
# Result: distributed vector [1.0, 2.0, 3.0, 4.0, 5.0] with partition [1, 3, 6]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/vectors.jl#L48-L66">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.MatrixMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.MatrixMPI_local"><code>LinearAlgebraMPI.MatrixMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">MatrixMPI_local(A_local::AbstractMatrix{T}; comm=MPI.COMM_WORLD, col_partition=...) where T</code></pre><p>Create a MatrixMPI from a local matrix on each rank.</p><p>Unlike <code>MatrixMPI(M_global)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>The type of <code>A_local</code> determines the storage backend (CPU Matrix, GPU MtlMatrix, etc.).</p><p>All ranks must have local matrices with the same number of columns. A collective error is raised if the column counts don&#39;t match.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>comm::MPI.Comm</code>: MPI communicator (default: <code>MPI.COMM_WORLD</code>)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(size(A_local,2), nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Rank 0 has 2×3 matrix, Rank 1 has 3×3 matrix
A = MatrixMPI_local(randn(2, 3))  # on rank 0
A = MatrixMPI_local(randn(3, 3))  # on rank 1
# Result: 5×3 distributed matrix with row_partition [1, 3, 6]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/dense.jl#L88-L113">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixMPI_local"><code>LinearAlgebraMPI.SparseMatrixMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixMPI_local(A_local::SparseMatrixCSR{T,Ti}; comm=MPI.COMM_WORLD, col_partition=...) where {T,Ti}
SparseMatrixMPI_local(A_local::Adjoint{T,SparseMatrixCSC{T,Ti}}; comm=MPI.COMM_WORLD, col_partition=...) where {T,Ti}</code></pre><p>Create a SparseMatrixMPI from a local sparse matrix on each rank.</p><p>Unlike <code>SparseMatrixMPI{T}(A_global)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>The input <code>A_local</code> must be a <code>SparseMatrixCSR{T,Ti}</code> (or <code>Adjoint</code> of <code>SparseMatrixCSC{T,Ti}</code>) where:</p><ul><li><code>A_local.parent.n</code> = number of local rows on this rank</li><li><code>A_local.parent.m</code> = global number of columns (must match on all ranks)</li><li><code>A_local.parent.rowval</code> = global column indices</li></ul><p>All ranks must have local matrices with the same number of columns (block widths must match). A collective error is raised if the column counts don&#39;t match.</p><p>Note: For <code>Adjoint</code> inputs, the values are conjugated to match the adjoint semantics.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>comm::MPI.Comm</code>: MPI communicator (default: <code>MPI.COMM_WORLD</code>)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(A_local.parent.m, nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create local rows in CSR format
# Rank 0 owns rows 1-2 of a 5×3 matrix, Rank 1 owns rows 3-5
local_csc = sparse([1, 1, 2], [1, 2, 3], [1.0, 2.0, 3.0], 2, 3)  # 2 local rows, 3 cols
A = SparseMatrixMPI_local(SparseMatrixCSR(local_csc))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/sparse.jl#L403-L434">source</a></section></details></article><h2 id="Row-wise-Operations"><a class="docs-heading-anchor" href="#Row-wise-Operations">Row-wise Operations</a><a id="Row-wise-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Row-wise-Operations" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.map_rows"><a class="docstring-binding" href="#LinearAlgebraMPI.map_rows"><code>LinearAlgebraMPI.map_rows</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">map_rows(f, A...)</code></pre><p>Apply function <code>f</code> to corresponding rows of distributed arrays, with CPU fallback.</p><p>This is the safe version that handles functions with arbitrary closures by converting GPU arrays to CPU, applying the function, and converting back. Use <code>map_rows_gpu</code> for performance-critical inner loops where <code>f</code> is isbits-compatible.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Function to apply to each row (can capture non-isbits data)</li><li><code>A...</code>: One or more distributed arrays (VectorMPI or MatrixMPI)</li></ul><p><strong>Returns</strong></p><ul><li>VectorMPI or MatrixMPI depending on the return type of <code>f</code></li></ul><p>See also: <a href="#LinearAlgebraMPI.map_rows_gpu"><code>map_rows_gpu</code></a> for GPU-native version (requires isbits closures)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/LinearAlgebraMPI.jl#L1042-L1059">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.map_rows_gpu"><a class="docstring-binding" href="#LinearAlgebraMPI.map_rows_gpu"><code>LinearAlgebraMPI.map_rows_gpu</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">map_rows_gpu(f, A...)</code></pre><p>Apply function <code>f</code> to corresponding rows of distributed vectors/matrices (GPU-native).</p><p>Each argument in <code>A...</code> must be either a <code>VectorMPI</code> or <code>MatrixMPI</code>. All inputs are repartitioned to match the partition of the first argument before applying <code>f</code>.</p><p>This implementation uses GPU-friendly broadcasting: matrices are converted to Vector{SVector} via transpose+reinterpret, then f is broadcast over all arguments. This avoids GPU-&gt;CPU-&gt;GPU round-trips when the underlying arrays are on GPU.</p><p><strong>Important</strong>: The function <code>f</code> must be isbits-compatible (no captured non-isbits data) for GPU execution. Use <a href="#LinearAlgebraMPI.map_rows"><code>map_rows</code></a> for functions with arbitrary closures.</p><p>For each row index i, <code>f</code> is called with:</p><ul><li>For <code>VectorMPI</code>: the scalar element at index i</li><li>For <code>MatrixMPI</code>: an SVector containing the i-th row</li></ul><p><strong>Result Type</strong></p><p>The result type depends on what <code>f</code> returns:</p><table><tr><th style="text-align: right"><code>f</code> returns</th><th style="text-align: right">Result</th></tr><tr><td style="text-align: right">scalar (<code>Number</code>)</td><td style="text-align: right"><code>VectorMPI</code> (one element per input row)</td></tr><tr><td style="text-align: right"><code>SVector{K,T}</code></td><td style="text-align: right"><code>MatrixMPI</code> (K columns, one row per input row)</td></tr></table><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Element-wise product of two vectors
u = VectorMPI([1.0, 2.0, 3.0])
v = VectorMPI([4.0, 5.0, 6.0])
w = map_rows_gpu((a, b) -&gt; a * b, u, v)  # VectorMPI([4.0, 10.0, 18.0])

# Row norms of a matrix
A = MatrixMPI(randn(5, 3))
norms = map_rows_gpu(r -&gt; norm(r), A)  # VectorMPI of row norms

# Return SVector to build a matrix
A = MatrixMPI(randn(3, 2))
result = map_rows_gpu(r -&gt; SVector(sum(r), prod(r)), A)  # 3×2 MatrixMPI

# Mixed inputs: matrix rows combined with vector elements
A = MatrixMPI(randn(4, 3))
w = VectorMPI([1.0, 2.0, 3.0, 4.0])
result = map_rows_gpu((row, wi) -&gt; sum(row) * wi, A, w)  # VectorMPI</code></pre><p>See also: <a href="#LinearAlgebraMPI.map_rows"><code>map_rows</code></a> for CPU fallback version (handles arbitrary closures)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/LinearAlgebraMPI.jl#L928-L979">source</a></section></details></article><h2 id="Linear-System-Solvers"><a class="docs-heading-anchor" href="#Linear-System-Solvers">Linear System Solvers</a><a id="Linear-System-Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-System-Solvers" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.solve"><a class="docstring-binding" href="#LinearAlgebraMPI.solve"><code>LinearAlgebraMPI.solve</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve(F::MUMPSFactorizationMPI{Tin, AVin, Tinternal}, b::VectorMPI) where {Tin, AVin, Tinternal}</code></pre><p>Solve the linear system A*x = b using the precomputed MUMPS factorization.</p><p>The input vector b can have any compatible element type and backend. The result is returned with the same element type and backend as the original matrix used for factorization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/mumps_factorization.jl#L506-L514">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.solve!"><a class="docstring-binding" href="#LinearAlgebraMPI.solve!"><code>LinearAlgebraMPI.solve!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve!(x::VectorMPI, F::MUMPSFactorizationMPI{Tin, AVin, Tinternal}, b::VectorMPI) where {Tin, AVin, Tinternal}</code></pre><p>Solve A*x = b in-place using MUMPS factorization.</p><p>Automatically converts inputs to CPU Float64/ComplexF64 for MUMPS, then converts results back to the element type and backend of x.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/mumps_factorization.jl#L522-L529">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.finalize!"><a class="docstring-binding" href="#LinearAlgebraMPI.finalize!"><code>LinearAlgebraMPI.finalize!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">finalize!(F::MUMPSFactorizationMPI)</code></pre><p>Release MUMPS resources. Must be called on all ranks together.</p><p>Note: If the MUMPS object is shared with the analysis cache (owns<em>mumps=false), this only removes the factorization from the registry. The MUMPS object itself is finalized when `clear</em>mumps<em>analysis</em>cache!()` is called.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/mumps_factorization.jl#L614-L622">source</a></section></details></article><h2 id="Partition-Utilities"><a class="docs-heading-anchor" href="#Partition-Utilities">Partition Utilities</a><a id="Partition-Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Partition-Utilities" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.uniform_partition"><a class="docstring-binding" href="#LinearAlgebraMPI.uniform_partition"><code>LinearAlgebraMPI.uniform_partition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">uniform_partition(n::Int, nranks::Int) -&gt; Vector{Int}</code></pre><p>Compute a balanced partition of <code>n</code> elements across <code>nranks</code> ranks. Returns a vector of length <code>nranks + 1</code> with 1-indexed partition boundaries.</p><p>The first <code>mod(n, nranks)</code> ranks get <code>div(n, nranks) + 1</code> elements, the remaining ranks get <code>div(n, nranks)</code> elements.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">partition = uniform_partition(10, 4)  # [1, 4, 7, 9, 11]
# Rank 0: 1:3 (3 elements)
# Rank 1: 4:6 (3 elements)
# Rank 2: 7:8 (2 elements)
# Rank 3: 9:10 (2 elements)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/LinearAlgebraMPI.jl#L225-L242">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.repartition"><a class="docstring-binding" href="#LinearAlgebraMPI.repartition"><code>LinearAlgebraMPI.repartition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">repartition(x::VectorMPI{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a VectorMPI to a new partition <code>p</code>.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == length(x) + 1</code>.</p><p>Returns a new VectorMPI with the same data but <code>partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">v = VectorMPI([1.0, 2.0, 3.0, 4.0])  # uniform partition
new_partition = [1, 2, 5]  # rank 0 gets 1 element, rank 1 gets 3
v_repart = repartition(v, new_partition)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/vectors.jl#L670-L686">source</a></section><section><div><pre><code class="language-julia hljs">repartition(A::MatrixMPI{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a MatrixMPI to a new row partition <code>p</code>. The col_partition remains unchanged.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == size(A, 1) + 1</code>.</p><p>Returns a new MatrixMPI with the same data but <code>row_partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = MatrixMPI(randn(6, 4))  # uniform partition
new_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank
A_repart = repartition(A, new_partition)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/dense.jl#L1777-L1794">source</a></section><section><div><pre><code class="language-julia hljs">repartition(A::SparseMatrixMPI{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a SparseMatrixMPI to a new row partition <code>p</code>. The col_partition remains unchanged.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == size(A, 1) + 1</code>.</p><p>Returns a new SparseMatrixMPI with the same data but <code>row_partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = SparseMatrixMPI{Float64}(sprand(6, 4, 0.5))  # uniform partition
new_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank
A_repart = repartition(A, new_partition)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/sparse.jl#L4776-L4793">source</a></section></details></article><h2 id="Cache-Management"><a class="docs-heading-anchor" href="#Cache-Management">Cache Management</a><a id="Cache-Management-1"></a><a class="docs-heading-anchor-permalink" href="#Cache-Management" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.clear_plan_cache!"><a class="docstring-binding" href="#LinearAlgebraMPI.clear_plan_cache!"><code>LinearAlgebraMPI.clear_plan_cache!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">clear_plan_cache!()</code></pre><p>Clear all memoized plan caches, including the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/LinearAlgebraMPI.jl#L173-L178">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.clear_mumps_analysis_cache!"><a class="docstring-binding" href="#LinearAlgebraMPI.clear_mumps_analysis_cache!"><code>LinearAlgebraMPI.clear_mumps_analysis_cache!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">clear_mumps_analysis_cache!()</code></pre><p>Clear the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/mumps_factorization.jl#L84-L89">source</a></section></details></article><h2 id="IO-Utilities"><a class="docs-heading-anchor" href="#IO-Utilities">IO Utilities</a><a id="IO-Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#IO-Utilities" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.io0"><a class="docstring-binding" href="#LinearAlgebraMPI.io0"><code>LinearAlgebraMPI.io0</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)</code></pre><p>Return <code>io</code> if the current MPI rank is in set <code>r</code>, otherwise return <code>dn</code> (default: <code>devnull</code>).</p><p>This is useful for printing only from specific ranks:</p><pre><code class="language-julia hljs">println(io0(), &quot;Hello from rank 0!&quot;)
println(io0(r=Set([0,1])), &quot;Hello from ranks 0 and 1!&quot;)</code></pre><p>With string interpolation:</p><pre><code class="language-julia hljs">println(io0(), &quot;Matrix A = $A&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/b6e179084b0dff9679686f28b3b846d8fa4f2f01/src/LinearAlgebraMPI.jl#L629-L644">source</a></section></details></article><h2 id="Type-Mappings"><a class="docs-heading-anchor" href="#Type-Mappings">Type Mappings</a><a id="Type-Mappings-1"></a><a class="docs-heading-anchor-permalink" href="#Type-Mappings" title="Permalink"></a></h2><h3 id="Native-to-MPI-Conversions"><a class="docs-heading-anchor" href="#Native-to-MPI-Conversions">Native to MPI Conversions</a><a id="Native-to-MPI-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#Native-to-MPI-Conversions" title="Permalink"></a></h3><table><tr><th style="text-align: right">Native Type</th><th style="text-align: right">MPI Type</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right"><code>VectorMPI{T,AV}</code></td><td style="text-align: right">Distributed vector</td></tr><tr><td style="text-align: right"><code>Matrix{T}</code></td><td style="text-align: right"><code>MatrixMPI{T,AM}</code></td><td style="text-align: right">Distributed dense matrix</td></tr><tr><td style="text-align: right"><code>SparseMatrixCSC{T,Ti}</code></td><td style="text-align: right"><code>SparseMatrixMPI{T,Ti,AV}</code></td><td style="text-align: right">Distributed sparse matrix</td></tr></table><p>The <code>AV</code> and <code>AM</code> type parameters specify the underlying storage (<code>Vector{T}</code>/<code>Matrix{T}</code> for CPU, <code>MtlVector{T}</code>/<code>MtlMatrix{T}</code> for Metal GPU).</p><h3 id="MPI-to-Native-Conversions"><a class="docs-heading-anchor" href="#MPI-to-Native-Conversions">MPI to Native Conversions</a><a id="MPI-to-Native-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-to-Native-Conversions" title="Permalink"></a></h3><table><tr><th style="text-align: right">MPI Type</th><th style="text-align: right">Native Type</th><th style="text-align: right">Function</th></tr><tr><td style="text-align: right"><code>VectorMPI{T,AV}</code></td><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right"><code>Vector(v)</code></td></tr><tr><td style="text-align: right"><code>MatrixMPI{T,AM}</code></td><td style="text-align: right"><code>Matrix{T}</code></td><td style="text-align: right"><code>Matrix(A)</code></td></tr><tr><td style="text-align: right"><code>SparseMatrixMPI{T,Ti,AV}</code></td><td style="text-align: right"><code>SparseMatrixCSC{T,Ti}</code></td><td style="text-align: right"><code>SparseMatrixCSC(A)</code></td></tr></table><h2 id="Supported-Operations"><a class="docs-heading-anchor" href="#Supported-Operations">Supported Operations</a><a id="Supported-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Supported-Operations" title="Permalink"></a></h2><h3 id="VectorMPI-Operations"><a class="docs-heading-anchor" href="#VectorMPI-Operations">VectorMPI Operations</a><a id="VectorMPI-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#VectorMPI-Operations" title="Permalink"></a></h3><ul><li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code> (scalar)</li><li>Linear algebra: <code>norm</code>, <code>dot</code>, <code>conj</code></li><li>Indexing: <code>v[i]</code> (global index)</li><li>Conversion: <code>Vector(v)</code></li></ul><h3 id="MatrixMPI-Operations"><a class="docs-heading-anchor" href="#MatrixMPI-Operations">MatrixMPI Operations</a><a id="MatrixMPI-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#MatrixMPI-Operations" title="Permalink"></a></h3><ul><li>Arithmetic: <code>*</code> (scalar), matrix-vector product</li><li>Transpose: <code>transpose(A)</code></li><li>Indexing: <code>A[i, j]</code> (global indices)</li><li>Conversion: <code>Matrix(A)</code></li></ul><h3 id="SparseMatrixMPI-Operations"><a class="docs-heading-anchor" href="#SparseMatrixMPI-Operations">SparseMatrixMPI Operations</a><a id="SparseMatrixMPI-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixMPI-Operations" title="Permalink"></a></h3><ul><li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code> (scalar, matrix-vector, matrix-matrix)</li><li>Transpose: <code>transpose(A)</code></li><li>Linear solve: <code>A \ b</code>, <code>Symmetric(A) \ b</code></li><li>Utilities: <code>nnz</code>, <code>norm</code>, <code>issymmetric</code></li><li>Conversion: <code>SparseMatrixCSC(A)</code></li></ul><h2 id="Factorization-Types"><a class="docs-heading-anchor" href="#Factorization-Types">Factorization Types</a><a id="Factorization-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Factorization-Types" title="Permalink"></a></h2><p>LinearAlgebraMPI uses MUMPS for sparse direct solves:</p><ul><li><code>lu(A)</code>: LU factorization (general matrices)</li><li><code>ldlt(A)</code>: LDLT factorization (symmetric matrices, faster)</li></ul><p>Both return factorization objects that support:</p><ul><li><code>F \ b</code>: Solve with factorization</li><li><code>finalize!(F)</code>: Release MUMPS resources</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Tuesday 6 January 2026 23:46">Tuesday 6 January 2026</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
