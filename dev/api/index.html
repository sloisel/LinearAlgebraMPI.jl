<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · HPCLinearAlgebra.jl</title><meta name="title" content="API Reference · HPCLinearAlgebra.jl"/><meta property="og:title" content="API Reference · HPCLinearAlgebra.jl"/><meta property="twitter:title" content="API Reference · HPCLinearAlgebra.jl"/><meta name="description" content="Documentation for HPCLinearAlgebra.jl."/><meta property="og:description" content="Documentation for HPCLinearAlgebra.jl."/><meta property="twitter:description" content="Documentation for HPCLinearAlgebra.jl."/><meta property="og:url" content="https://sloisel.github.io/HPCLinearAlgebra.jl/api/"/><meta property="twitter:url" content="https://sloisel.github.io/HPCLinearAlgebra.jl/api/"/><link rel="canonical" href="https://sloisel.github.io/HPCLinearAlgebra.jl/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">HPCLinearAlgebra.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../guide/">User Guide</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Distributed-Types"><span>Distributed Types</span></a></li><li><a class="tocitem" href="#Local-Constructors"><span>Local Constructors</span></a></li><li><a class="tocitem" href="#Row-wise-Operations"><span>Row-wise Operations</span></a></li><li><a class="tocitem" href="#Linear-System-Solvers"><span>Linear System Solvers</span></a></li><li><a class="tocitem" href="#Partition-Utilities"><span>Partition Utilities</span></a></li><li><a class="tocitem" href="#Cache-Management"><span>Cache Management</span></a></li><li><a class="tocitem" href="#IO-Utilities"><span>IO Utilities</span></a></li><li><a class="tocitem" href="#Backend-Types"><span>Backend Types</span></a></li><li><a class="tocitem" href="#Type-Mappings"><span>Type Mappings</span></a></li><li><a class="tocitem" href="#Supported-Operations"><span>Supported Operations</span></a></li><li><a class="tocitem" href="#Factorization-Types"><span>Factorization Types</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sloisel/HPCLinearAlgebra.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/main/docs/src/api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><p>This page provides detailed documentation for all exported types and functions in HPCLinearAlgebra.jl.</p><div class="admonition is-info" id="MPI-Collective-Operations-a1387683e48aabeb"><header class="admonition-header">MPI Collective Operations<a class="admonition-anchor" href="#MPI-Collective-Operations-a1387683e48aabeb" title="Permalink"></a></header><div class="admonition-body"><p>Unless otherwise noted, all functions are MPI collective operations. Every MPI rank must call these functions together.</p></div></div><h2 id="Distributed-Types"><a class="docs-heading-anchor" href="#Distributed-Types">Distributed Types</a><a id="Distributed-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Types" title="Permalink"></a></h2><h3 id="HPCVector"><a class="docs-heading-anchor" href="#HPCVector">HPCVector</a><a id="HPCVector-1"></a><a class="docs-heading-anchor-permalink" href="#HPCVector" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.HPCVector"><a class="docstring-binding" href="#HPCLinearAlgebra.HPCVector"><code>HPCLinearAlgebra.HPCVector</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">HPCVector{T, B&lt;:HPCBackend}</code></pre><p>A distributed dense vector partitioned across MPI ranks.</p><p><strong>Type Parameters</strong></p><ul><li><code>T</code>: Element type (e.g., <code>Float64</code>, <code>ComplexF64</code>)</li><li><code>B&lt;:HPCBackend</code>: Backend configuration (device, communication, solver)</li></ul><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the partition</li><li><code>partition::Vector{Int}</code>: Partition boundaries, length = nranks + 1 (always on CPU)</li><li><code>v::AbstractVector{T}</code>: Local vector elements owned by this rank</li><li><code>backend::B</code>: The HPC backend configuration</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/vectors.jl#L6-L20">source</a></section></details></article><h3 id="HPCMatrix"><a class="docs-heading-anchor" href="#HPCMatrix">HPCMatrix</a><a id="HPCMatrix-1"></a><a class="docs-heading-anchor-permalink" href="#HPCMatrix" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.HPCMatrix"><a class="docstring-binding" href="#HPCLinearAlgebra.HPCMatrix"><code>HPCLinearAlgebra.HPCMatrix</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">HPCMatrix{T, B&lt;:HPCBackend}</code></pre><p>A distributed dense matrix partitioned by rows across MPI ranks.</p><p><strong>Type Parameters</strong></p><ul><li><code>T</code>: Element type (e.g., <code>Float64</code>, <code>ComplexF64</code>)</li><li><code>B&lt;:HPCBackend</code>: Backend configuration (device, communication, solver)</li></ul><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1 (always on CPU)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (always on CPU)</li><li><code>A::AbstractMatrix{T}</code>: Local rows (NOT transposed), size = (local_nrows, ncols)</li><li><code>backend::B</code>: The HPC backend configuration</li></ul><p><strong>Invariants</strong></p><ul><li><code>row_partition</code> and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows + 1</li><li><code>col_partition[nranks+1]</code> = total number of columns + 1</li><li><code>size(A, 1) == row_partition[rank+2] - row_partition[rank+1]</code></li><li><code>size(A, 2) == col_partition[end] - 1</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/dense.jl#L36-L58">source</a></section></details></article><h3 id="HPCSparseMatrix"><a class="docs-heading-anchor" href="#HPCSparseMatrix">HPCSparseMatrix</a><a id="HPCSparseMatrix-1"></a><a class="docs-heading-anchor-permalink" href="#HPCSparseMatrix" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.HPCSparseMatrix"><a class="docstring-binding" href="#HPCLinearAlgebra.HPCSparseMatrix"><code>HPCLinearAlgebra.HPCSparseMatrix</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">HPCSparseMatrix{T,Ti,AV}</code></pre><p>A distributed sparse matrix partitioned by rows across MPI ranks.</p><p><strong>Type Parameters</strong></p><ul><li><code>T</code>: Element type (e.g., <code>Float64</code>, <code>ComplexF64</code>)</li><li><code>Ti</code>: Index type (e.g., <code>Int</code>, <code>Int32</code>), defaults to <code>Int</code></li><li><code>AV&lt;:AbstractVector{T}</code>: Storage type for nonzero values (<code>Vector{T}</code> for CPU, <code>MtlVector{T}</code> for GPU)</li></ul><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (placeholder for transpose)</li><li><code>col_indices::Vector{Int}</code>: Global column indices that appear in the local part (local→global mapping)</li><li><code>rowptr::Vector{Ti}</code>: Row pointers for CSR format (always CPU)</li><li><code>colval::Vector{Ti}</code>: LOCAL column indices 1:length(col_indices) for each nonzero (always CPU)</li><li><code>nzval::AV</code>: Nonzero values (CPU or GPU)</li><li><code>nrows_local::Int</code>: Number of local rows</li><li><code>ncols_compressed::Int</code>: Number of unique columns = length(col_indices)</li><li><code>cached_transpose</code>: Cached materialized transpose (bidirectionally linked)</li></ul><p><strong>Invariants</strong></p><ul><li><code>col_indices</code>, <code>row_partition</code>, and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows</li><li><code>col_partition[nranks+1]</code> = total number of columns</li><li><code>nrows_local == row_partition[rank+1] - row_partition[rank]</code> (number of local rows)</li><li><code>ncols_compressed == length(col_indices)</code> (compressed column dimension)</li><li><code>colval</code> contains local indices in <code>1:ncols_compressed</code></li><li><code>rowptr</code> has length <code>nrows_local + 1</code></li></ul><p><strong>Storage Details</strong></p><p>The local rows are stored in CSR format (Compressed Sparse Row), which enables efficient row-wise iteration - essential for a row-partitioned distributed matrix.</p><p>The CSR storage consists of:</p><ul><li><code>rowptr</code>: Row pointers where row i has nonzeros at positions rowptr[i]:(rowptr[i+1]-1)</li><li><code>colval</code>: LOCAL column indices (1:ncols_compressed), not global indices</li><li><code>nzval</code>: Nonzero values</li><li><code>col_indices[local_idx]</code> maps local→global column indices</li></ul><p>This compression avoids &quot;hypersparse&quot; storage where the column dimension would be the global number of columns even if only a few columns have nonzeros locally.</p><p><strong>GPU Support</strong></p><p>Structure arrays (<code>rowptr</code>, <code>colval</code>) always stay on CPU for MPI communication and indexing. Only <code>nzval</code> can live on GPU, with type determined by the backend device:</p><ul><li><code>DeviceCPU</code>: Vector{T} storage</li><li><code>DeviceMetal</code>: MtlVector{T} storage (macOS)</li><li><code>DeviceCUDA</code>: CuVector{T} storage</li></ul><p>Use <code>to_backend(A, target_backend)</code> to convert between backends.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/sparse.jl#L266-L318">source</a></section></details></article><h3 id="SparseMatrixCSR"><a class="docs-heading-anchor" href="#SparseMatrixCSR">SparseMatrixCSR</a><a id="SparseMatrixCSR-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixCSR" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.SparseMatrixCSR"><a class="docstring-binding" href="#HPCLinearAlgebra.SparseMatrixCSR"><code>HPCLinearAlgebra.SparseMatrixCSR</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixCSR{Tv,Ti} = Transpose{Tv, SparseMatrixCSC{Tv,Ti}}</code></pre><p>Type alias for CSR (Compressed Sparse Row) storage format.</p><p><strong>The Dual Life of Transpose{SparseMatrixCSC}</strong></p><p>In Julia, the type <code>Transpose{Tv, SparseMatrixCSC{Tv,Ti}}</code> has two interpretations:</p><ol><li><p><strong>Semantic interpretation</strong>: A lazy transpose wrapper around a CSC matrix. When you call <code>transpose(A)</code> on a SparseMatrixCSC, you get this wrapper that represents A^T without copying data.</p></li><li><p><strong>Storage interpretation</strong>: CSR (row-major) access to sparse data. The underlying CSC stores columns contiguously, but through the transpose wrapper, we can iterate efficiently over rows instead of columns.</p></li></ol><p>This alias clarifies intent: use <code>SparseMatrixCSR</code> when you want row-major storage semantics, and <code>transpose(A)</code> when you want the mathematical transpose.</p><p><strong>CSR vs CSC Storage</strong></p><ul><li><strong>CSC (Compressed Sparse Column)</strong>: Julia&#39;s native sparse format. Efficient for column-wise operations, matrix-vector products with column access.</li><li><strong>CSR (Compressed Sparse Row)</strong>: Efficient for row-wise operations, matrix-vector products with row access, and row-partitioned distributed matrices.</li></ul><p>For <code>SparseMatrixCSR</code>, the underlying <code>parent::SparseMatrixCSC</code> stores the <em>transposed</em> matrix. If <code>B = SparseMatrixCSR(A)</code> represents matrix M, then <code>B.parent</code> is a CSC storing M^T. This means:</p><ul><li><code>B.parent.colptr</code> acts as row pointers for M</li><li><code>B.parent.rowval</code> contains column indices for M</li><li><code>B.parent.nzval</code> contains values in row-major order</li></ul><p><strong>Usage Note</strong></p><p>Julia will still display this type as <code>Transpose{Float64, SparseMatrixCSC{...}}</code>, not as <code>SparseMatrixCSR</code>. The alias improves code clarity but doesn&#39;t affect type printing.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/HPCLinearAlgebra.jl#L48-L87">source</a></section></details></article><h2 id="Local-Constructors"><a class="docs-heading-anchor" href="#Local-Constructors">Local Constructors</a><a id="Local-Constructors-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Constructors" title="Permalink"></a></h2><p>These constructors create distributed types from local data without global communication.</p><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.HPCVector_local"><a class="docstring-binding" href="#HPCLinearAlgebra.HPCVector_local"><code>HPCLinearAlgebra.HPCVector_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">HPCVector_local(v_local::AbstractVector{T}, backend::HPCBackend) where T</code></pre><p>Create a HPCVector from a local vector on each rank.</p><p>Unlike <code>HPCVector(v_global, backend)</code> which takes a global vector and partitions it, this constructor takes only the local portion of the vector that each rank owns. The partition is computed by gathering the local sizes from all ranks.</p><p><strong>Arguments</strong></p><ul><li><code>v_local</code>: Local vector portion owned by this rank</li><li><code>backend</code>: The HPCBackend configuration (determines communication)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">backend = backend_cpu_mpi(MPI.COMM_WORLD)
# Rank 0 has [1.0, 2.0], Rank 1 has [3.0, 4.0, 5.0]
v = HPCVector_local([1.0, 2.0], backend)  # on rank 0
v = HPCVector_local([3.0, 4.0, 5.0], backend)  # on rank 1
# Result: distributed vector [1.0, 2.0, 3.0, 4.0, 5.0] with partition [1, 3, 6]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/vectors.jl#L54-L75">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.HPCMatrix_local"><a class="docstring-binding" href="#HPCLinearAlgebra.HPCMatrix_local"><code>HPCLinearAlgebra.HPCMatrix_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">HPCMatrix_local(A_local::AbstractMatrix{T}, backend::HPCBackend; col_partition=...) where T</code></pre><p>Create a HPCMatrix from a local matrix on each rank.</p><p>Unlike <code>HPCMatrix(M_global, backend)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>The type of <code>A_local</code> determines the storage backend (CPU Matrix, GPU MtlMatrix, etc.).</p><p>All ranks must have local matrices with the same number of columns. A collective error is raised if the column counts don&#39;t match.</p><p><strong>Arguments</strong></p><ul><li><code>A_local::AbstractMatrix{T}</code>: Local rows owned by this rank</li><li><code>backend::HPCBackend</code>: The HPC backend configuration</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(size(A_local,2), nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">backend = backend_cpu_mpi(MPI.COMM_WORLD)
# Rank 0 has 2×3 matrix, Rank 1 has 3×3 matrix
A = HPCMatrix_local(randn(2, 3), backend)  # on rank 0
A = HPCMatrix_local(randn(3, 3), backend)  # on rank 1
# Result: 5×3 distributed matrix with row_partition [1, 3, 6]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/dense.jl#L95-L124">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.HPCSparseMatrix_local"><a class="docstring-binding" href="#HPCLinearAlgebra.HPCSparseMatrix_local"><code>HPCLinearAlgebra.HPCSparseMatrix_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">HPCSparseMatrix_local(A_local::SparseMatrixCSR{T,Ti}, backend::HPCBackend; col_partition=...) where {T,Ti}
HPCSparseMatrix_local(A_local::Adjoint{T,SparseMatrixCSC{T,Ti}}, backend::HPCBackend; col_partition=...) where {T,Ti}</code></pre><p>Create a HPCSparseMatrix from a local sparse matrix on each rank.</p><p>Unlike <code>HPCSparseMatrix{T}(A_global, backend)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>The input <code>A_local</code> must be a <code>SparseMatrixCSR{T,Ti}</code> (or <code>Adjoint</code> of <code>SparseMatrixCSC{T,Ti}</code>) where:</p><ul><li><code>A_local.parent.n</code> = number of local rows on this rank</li><li><code>A_local.parent.m</code> = global number of columns (must match on all ranks)</li><li><code>A_local.parent.rowval</code> = global column indices</li></ul><p>All ranks must have local matrices with the same number of columns (block widths must match). A collective error is raised if the column counts don&#39;t match.</p><p>Note: For <code>Adjoint</code> inputs, the values are conjugated to match the adjoint semantics.</p><p><strong>Arguments</strong></p><ul><li><code>A_local</code>: Local sparse matrix in CSR format</li><li><code>backend::HPCBackend</code>: Backend configuration (device, comm, solver)</li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(A_local.parent.m, nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs">backend = backend_cpu_mpi(MPI.COMM_WORLD)
# Create local rows in CSR format
# Rank 0 owns rows 1-2 of a 5×3 matrix, Rank 1 owns rows 3-5
local_csc = sparse([1, 1, 2], [1, 2, 3], [1.0, 2.0, 3.0], 2, 3)  # 2 local rows, 3 cols
A = HPCSparseMatrix_local(SparseMatrixCSR(local_csc), backend)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/sparse.jl#L418-L453">source</a></section></details></article><h2 id="Row-wise-Operations"><a class="docs-heading-anchor" href="#Row-wise-Operations">Row-wise Operations</a><a id="Row-wise-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Row-wise-Operations" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.map_rows"><a class="docstring-binding" href="#HPCLinearAlgebra.map_rows"><code>HPCLinearAlgebra.map_rows</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">map_rows(f, A...)</code></pre><p>Apply function <code>f</code> to corresponding rows of distributed arrays, with CPU fallback.</p><p>This is the safe version that handles functions with arbitrary closures by converting GPU arrays to CPU, applying the function, and converting back. Use <code>map_rows_gpu</code> for performance-critical inner loops where <code>f</code> is isbits-compatible.</p><p><strong>Arguments</strong></p><ul><li><code>f</code>: Function to apply to each row (can capture non-isbits data)</li><li><code>A...</code>: One or more distributed arrays (HPCVector or HPCMatrix)</li></ul><p><strong>Returns</strong></p><ul><li>HPCVector or HPCMatrix depending on the return type of <code>f</code></li></ul><p>See also: <a href="#HPCLinearAlgebra.map_rows_gpu"><code>map_rows_gpu</code></a> for GPU-native version (requires isbits closures)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/HPCLinearAlgebra.jl#L1119-L1136">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.map_rows_gpu"><a class="docstring-binding" href="#HPCLinearAlgebra.map_rows_gpu"><code>HPCLinearAlgebra.map_rows_gpu</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">map_rows_gpu(f, A...)</code></pre><p>Apply function <code>f</code> to corresponding rows of distributed vectors/matrices (GPU-native).</p><p>Each argument in <code>A...</code> must be either a <code>HPCVector</code> or <code>HPCMatrix</code>. All inputs are repartitioned to match the partition of the first argument before applying <code>f</code>.</p><p>This implementation uses GPU-friendly broadcasting: matrices are converted to Vector{SVector} via transpose+reinterpret, then f is broadcast over all arguments. This avoids GPU-&gt;CPU-&gt;GPU round-trips when the underlying arrays are on GPU.</p><p><strong>Important</strong>: The function <code>f</code> must be isbits-compatible (no captured non-isbits data) for GPU execution. Use <a href="#HPCLinearAlgebra.map_rows"><code>map_rows</code></a> for functions with arbitrary closures.</p><p>For each row index i, <code>f</code> is called with:</p><ul><li>For <code>HPCVector</code>: the scalar element at index i</li><li>For <code>HPCMatrix</code>: an SVector containing the i-th row</li></ul><p><strong>Result Type</strong></p><p>The result type depends on what <code>f</code> returns:</p><table><tr><th style="text-align: right"><code>f</code> returns</th><th style="text-align: right">Result</th></tr><tr><td style="text-align: right">scalar (<code>Number</code>)</td><td style="text-align: right"><code>HPCVector</code> (one element per input row)</td></tr><tr><td style="text-align: right"><code>SVector{K,T}</code></td><td style="text-align: right"><code>HPCMatrix</code> (K columns, one row per input row)</td></tr></table><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Element-wise product of two vectors
u = HPCVector([1.0, 2.0, 3.0])
v = HPCVector([4.0, 5.0, 6.0])
w = map_rows_gpu((a, b) -&gt; a * b, u, v)  # HPCVector([4.0, 10.0, 18.0])

# Row norms of a matrix
A = HPCMatrix(randn(5, 3))
norms = map_rows_gpu(r -&gt; norm(r), A)  # HPCVector of row norms

# Return SVector to build a matrix
A = HPCMatrix(randn(3, 2))
result = map_rows_gpu(r -&gt; SVector(sum(r), prod(r)), A)  # 3×2 HPCMatrix

# Mixed inputs: matrix rows combined with vector elements
A = HPCMatrix(randn(4, 3))
w = HPCVector([1.0, 2.0, 3.0, 4.0])
result = map_rows_gpu((row, wi) -&gt; sum(row) * wi, A, w)  # HPCVector</code></pre><p>See also: <a href="#HPCLinearAlgebra.map_rows"><code>map_rows</code></a> for CPU fallback version (handles arbitrary closures)</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/HPCLinearAlgebra.jl#L1000-L1051">source</a></section></details></article><h2 id="Linear-System-Solvers"><a class="docs-heading-anchor" href="#Linear-System-Solvers">Linear System Solvers</a><a id="Linear-System-Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-System-Solvers" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.solve"><a class="docstring-binding" href="#HPCLinearAlgebra.solve"><code>HPCLinearAlgebra.solve</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve(F::MUMPSFactorization{Tin, Bin, Tinternal}, b::HPCVector) where {Tin, Bin, Tinternal}</code></pre><p>Solve the linear system A*x = b using the precomputed MUMPS factorization.</p><p>The input vector b can have any compatible element type and backend. The result is returned with the same element type and backend as the original matrix used for factorization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/mumps_factorization.jl#L511-L519">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.solve!"><a class="docstring-binding" href="#HPCLinearAlgebra.solve!"><code>HPCLinearAlgebra.solve!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve!(x::HPCVector, F::MUMPSFactorization{Tin, Bin, Tinternal}, b::HPCVector) where {Tin, Bin, Tinternal}</code></pre><p>Solve A*x = b in-place using MUMPS factorization.</p><p>Automatically converts inputs to CPU Float64/ComplexF64 for MUMPS, then converts results back to the element type and backend of x.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/mumps_factorization.jl#L527-L534">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.finalize!"><a class="docstring-binding" href="#HPCLinearAlgebra.finalize!"><code>HPCLinearAlgebra.finalize!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">finalize!(F::MUMPSFactorization)</code></pre><p>Release MUMPS resources. Must be called on all ranks together.</p><p>Note: If the MUMPS object is shared with the analysis cache (owns<em>mumps=false), this only removes the factorization from the registry. The MUMPS object itself is finalized when `clear</em>mumps<em>analysis</em>cache!()` is called.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/mumps_factorization.jl#L609-L617">source</a></section></details></article><h2 id="Partition-Utilities"><a class="docs-heading-anchor" href="#Partition-Utilities">Partition Utilities</a><a id="Partition-Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Partition-Utilities" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.uniform_partition"><a class="docstring-binding" href="#HPCLinearAlgebra.uniform_partition"><code>HPCLinearAlgebra.uniform_partition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">uniform_partition(n::Int, nranks::Int) -&gt; Vector{Int}</code></pre><p>Compute a balanced partition of <code>n</code> elements across <code>nranks</code> ranks. Returns a vector of length <code>nranks + 1</code> with 1-indexed partition boundaries.</p><p>The first <code>mod(n, nranks)</code> ranks get <code>div(n, nranks) + 1</code> elements, the remaining ranks get <code>div(n, nranks)</code> elements.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">partition = uniform_partition(10, 4)  # [1, 4, 7, 9, 11]
# Rank 0: 1:3 (3 elements)
# Rank 1: 4:6 (3 elements)
# Rank 2: 7:8 (2 elements)
# Rank 3: 9:10 (2 elements)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/HPCLinearAlgebra.jl#L227-L244">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.repartition"><a class="docstring-binding" href="#HPCLinearAlgebra.repartition"><code>HPCLinearAlgebra.repartition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">repartition(x::HPCVector{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a HPCVector to a new partition <code>p</code>.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == length(x) + 1</code>.</p><p>Returns a new HPCVector with the same data but <code>partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">v = HPCVector([1.0, 2.0, 3.0, 4.0])  # uniform partition
new_partition = [1, 2, 5]  # rank 0 gets 1 element, rank 1 gets 3
v_repart = repartition(v, new_partition)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/vectors.jl#L693-L709">source</a></section><section><div><pre><code class="language-julia hljs">repartition(A::HPCMatrix{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a HPCMatrix to a new row partition <code>p</code>. The col_partition remains unchanged.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == size(A, 1) + 1</code>.</p><p>Returns a new HPCMatrix with the same data but <code>row_partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = HPCMatrix(randn(6, 4))  # uniform partition
new_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank
A_repart = repartition(A, new_partition)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/dense.jl#L1780-L1797">source</a></section><section><div><pre><code class="language-julia hljs">repartition(A::HPCSparseMatrix{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a HPCSparseMatrix to a new row partition <code>p</code>. The col_partition remains unchanged.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == size(A, 1) + 1</code>.</p><p>Returns a new HPCSparseMatrix with the same data but <code>row_partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = HPCSparseMatrix{Float64}(sprand(6, 4, 0.5))  # uniform partition
new_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank
A_repart = repartition(A, new_partition)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/sparse.jl#L4815-L4832">source</a></section></details></article><h2 id="Cache-Management"><a class="docs-heading-anchor" href="#Cache-Management">Cache Management</a><a id="Cache-Management-1"></a><a class="docs-heading-anchor-permalink" href="#Cache-Management" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.clear_plan_cache!"><a class="docstring-binding" href="#HPCLinearAlgebra.clear_plan_cache!"><code>HPCLinearAlgebra.clear_plan_cache!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">clear_plan_cache!()</code></pre><p>Clear all memoized plan caches, including the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/HPCLinearAlgebra.jl#L175-L180">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.clear_mumps_analysis_cache!"><a class="docstring-binding" href="#HPCLinearAlgebra.clear_mumps_analysis_cache!"><code>HPCLinearAlgebra.clear_mumps_analysis_cache!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">clear_mumps_analysis_cache!()</code></pre><p>Clear the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/mumps_factorization.jl#L84-L89">source</a></section></details></article><h2 id="IO-Utilities"><a class="docs-heading-anchor" href="#IO-Utilities">IO Utilities</a><a id="IO-Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#IO-Utilities" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.io0"><a class="docstring-binding" href="#HPCLinearAlgebra.io0"><code>HPCLinearAlgebra.io0</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)</code></pre><p>Return <code>io</code> if the current MPI rank is in set <code>r</code>, otherwise return <code>dn</code> (default: <code>devnull</code>).</p><p>This is useful for printing only from specific ranks:</p><pre><code class="language-julia hljs">println(io0(), &quot;Hello from rank 0!&quot;)
println(io0(r=Set([0,1])), &quot;Hello from ranks 0 and 1!&quot;)</code></pre><p>With string interpolation:</p><pre><code class="language-julia hljs">println(io0(), &quot;Matrix A = $A&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/HPCLinearAlgebra.jl#L701-L716">source</a></section></details></article><h2 id="Backend-Types"><a class="docs-heading-anchor" href="#Backend-Types">Backend Types</a><a id="Backend-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Backend-Types" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.HPCBackend"><a class="docstring-binding" href="#HPCLinearAlgebra.HPCBackend"><code>HPCLinearAlgebra.HPCBackend</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">HPCBackend{D&lt;:AbstractDevice, C&lt;:AbstractComm, S&lt;:AbstractSolver}</code></pre><p>Unified backend type that encapsulates device, communication, and solver configuration.</p><p><strong>Type Parameters</strong></p><ul><li><code>D</code>: Device type (DeviceCPU, DeviceMetal, DeviceCUDA)</li><li><code>C</code>: Communication type (CommSerial, CommMPI)</li><li><code>S</code>: Solver type (SolverMUMPS, or cuDSS variants)</li></ul><p><strong>Fields</strong></p><ul><li><code>device::D</code>: The compute device</li><li><code>comm::C</code>: The communication backend</li><li><code>solver::S</code>: The linear solver</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># CPU with MPI and MUMPS
backend = HPCBackend(DeviceCPU(), CommMPI(MPI.COMM_WORLD), SolverMUMPS())

# CPU serial (single process)
backend = HPCBackend(DeviceCPU(), CommSerial(), SolverMUMPS())</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L110-L133">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.DeviceCPU"><a class="docstring-binding" href="#HPCLinearAlgebra.DeviceCPU"><code>HPCLinearAlgebra.DeviceCPU</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DeviceCPU &lt;: AbstractDevice</code></pre><p>CPU device - uses standard Julia arrays (Vector, Matrix).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L24-L28">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.DeviceMetal"><a class="docstring-binding" href="#HPCLinearAlgebra.DeviceMetal"><code>HPCLinearAlgebra.DeviceMetal</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DeviceMetal &lt;: AbstractDevice</code></pre><p>Metal GPU device (macOS) - uses Metal.jl arrays (MtlVector, MtlMatrix). Defined here as a placeholder; actual Metal support is in the Metal extension.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L31-L36">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.DeviceCUDA"><a class="docstring-binding" href="#HPCLinearAlgebra.DeviceCUDA"><code>HPCLinearAlgebra.DeviceCUDA</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">DeviceCUDA &lt;: AbstractDevice</code></pre><p>CUDA GPU device - uses CUDA.jl arrays (CuVector, CuMatrix). Defined here as a placeholder; actual CUDA support is in the CUDA extension.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L39-L44">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.CommSerial"><a class="docstring-binding" href="#HPCLinearAlgebra.CommSerial"><code>HPCLinearAlgebra.CommSerial</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">CommSerial &lt;: AbstractComm</code></pre><p>Serial (single-process) communication - all collective operations become no-ops.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L58-L62">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.CommMPI"><a class="docstring-binding" href="#HPCLinearAlgebra.CommMPI"><code>HPCLinearAlgebra.CommMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">CommMPI &lt;: AbstractComm</code></pre><p>MPI-based distributed communication.</p><p><strong>Fields</strong></p><ul><li><code>comm::MPI.Comm</code>: The MPI communicator to use for all operations.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L65-L72">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.SolverMUMPS"><a class="docstring-binding" href="#HPCLinearAlgebra.SolverMUMPS"><code>HPCLinearAlgebra.SolverMUMPS</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SolverMUMPS &lt;: AbstractSolver</code></pre><p>MUMPS sparse direct solver. Works on CPU with MPI or serial communication.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L88-L92">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.BACKEND_CPU_MPI"><a class="docstring-binding" href="#HPCLinearAlgebra.BACKEND_CPU_MPI"><code>HPCLinearAlgebra.BACKEND_CPU_MPI</code></a> — <span class="docstring-category">Constant</span></summary><section><div><pre><code class="language-julia hljs">BACKEND_CPU_MPI</code></pre><p>Pre-constructed CPU backend with MPI communication (using COMM_WORLD) and MUMPS solver. This is the default backend for distributed CPU computations.</p><p>Note: While this constant is created at module load time, actual MPI operations will only work after MPI.Init() has been called.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L349-L357">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.BACKEND_CPU_SERIAL"><a class="docstring-binding" href="#HPCLinearAlgebra.BACKEND_CPU_SERIAL"><code>HPCLinearAlgebra.BACKEND_CPU_SERIAL</code></a> — <span class="docstring-category">Constant</span></summary><section><div><pre><code class="language-julia hljs">BACKEND_CPU_SERIAL</code></pre><p>Pre-constructed CPU backend with serial (single-process) communication and MUMPS solver. Use this for non-MPI code or single-process testing.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L341-L346">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.backend_metal_mpi"><a class="docstring-binding" href="#HPCLinearAlgebra.backend_metal_mpi"><code>HPCLinearAlgebra.backend_metal_mpi</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">backend_metal_mpi(comm::MPI.Comm) -&gt; HPCBackend</code></pre><p>Create a Metal GPU backend with MPI communication. Requires the Metal extension to be loaded.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L363-L368">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.backend_cuda_mpi"><a class="docstring-binding" href="#HPCLinearAlgebra.backend_cuda_mpi"><code>HPCLinearAlgebra.backend_cuda_mpi</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">backend_cuda_mpi(comm::MPI.Comm) -&gt; HPCBackend</code></pre><p>Create a CUDA GPU backend with MPI communication and cuDSS solver. Requires the CUDA extension to be loaded.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/backends.jl#L379-L384">source</a></section></details></article><article><details class="docstring" open="true"><summary id="HPCLinearAlgebra.to_backend"><a class="docstring-binding" href="#HPCLinearAlgebra.to_backend"><code>HPCLinearAlgebra.to_backend</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">to_backend(v::HPCVector, backend::HPCBackend) -&gt; HPCVector</code></pre><p>Convert a HPCVector to use a different backend.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/HPCLinearAlgebra.jl#L298-L302">source</a></section><section><div><pre><code class="language-julia hljs">to_backend(A::HPCMatrix, backend::HPCBackend) -&gt; HPCMatrix</code></pre><p>Convert a HPCMatrix to use a different backend.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/HPCLinearAlgebra.jl#L308-L312">source</a></section><section><div><pre><code class="language-julia hljs">to_backend(A::HPCSparseMatrix, backend::HPCBackend) -&gt; HPCSparseMatrix</code></pre><p>Convert a HPCSparseMatrix to use a different backend. The nzval and target structure arrays are converted; CPU structure arrays remain on CPU.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/HPCLinearAlgebra.jl/blob/4ca3c32515e6fbfa53a4c23fec635702c49f7a57/src/HPCLinearAlgebra.jl#L318-L323">source</a></section></details></article><h2 id="Type-Mappings"><a class="docs-heading-anchor" href="#Type-Mappings">Type Mappings</a><a id="Type-Mappings-1"></a><a class="docs-heading-anchor-permalink" href="#Type-Mappings" title="Permalink"></a></h2><h3 id="Native-to-Distributed-Conversions"><a class="docs-heading-anchor" href="#Native-to-Distributed-Conversions">Native to Distributed Conversions</a><a id="Native-to-Distributed-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#Native-to-Distributed-Conversions" title="Permalink"></a></h3><table><tr><th style="text-align: right">Native Type</th><th style="text-align: right">Distributed Type</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right"><code>HPCVector{T,B}</code></td><td style="text-align: right">Distributed vector</td></tr><tr><td style="text-align: right"><code>Matrix{T}</code></td><td style="text-align: right"><code>HPCMatrix{T,B}</code></td><td style="text-align: right">Distributed dense matrix</td></tr><tr><td style="text-align: right"><code>SparseMatrixCSC{T,Ti}</code></td><td style="text-align: right"><code>HPCSparseMatrix{T,Ti,B}</code></td><td style="text-align: right">Distributed sparse matrix</td></tr></table><p>The <code>B&lt;:HPCBackend</code> type parameter specifies the backend configuration (device, communication, solver). Use pre-constructed backends like <code>BACKEND_CPU_MPI</code> or factory functions like <code>backend_cuda_mpi(comm)</code>.</p><h3 id="Distributed-to-Native-Conversions"><a class="docs-heading-anchor" href="#Distributed-to-Native-Conversions">Distributed to Native Conversions</a><a id="Distributed-to-Native-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-to-Native-Conversions" title="Permalink"></a></h3><table><tr><th style="text-align: right">Distributed Type</th><th style="text-align: right">Native Type</th><th style="text-align: right">Function</th></tr><tr><td style="text-align: right"><code>HPCVector{T,B}</code></td><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right"><code>Vector(v)</code></td></tr><tr><td style="text-align: right"><code>HPCMatrix{T,B}</code></td><td style="text-align: right"><code>Matrix{T}</code></td><td style="text-align: right"><code>Matrix(A)</code></td></tr><tr><td style="text-align: right"><code>HPCSparseMatrix{T,Ti,B}</code></td><td style="text-align: right"><code>SparseMatrixCSC{T,Ti}</code></td><td style="text-align: right"><code>SparseMatrixCSC(A)</code></td></tr></table><h2 id="Supported-Operations"><a class="docs-heading-anchor" href="#Supported-Operations">Supported Operations</a><a id="Supported-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Supported-Operations" title="Permalink"></a></h2><h3 id="HPCVector-Operations"><a class="docs-heading-anchor" href="#HPCVector-Operations">HPCVector Operations</a><a id="HPCVector-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#HPCVector-Operations" title="Permalink"></a></h3><ul><li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code> (scalar)</li><li>Linear algebra: <code>norm</code>, <code>dot</code>, <code>conj</code></li><li>Indexing: <code>v[i]</code> (global index)</li><li>Conversion: <code>Vector(v)</code></li></ul><h3 id="HPCMatrix-Operations"><a class="docs-heading-anchor" href="#HPCMatrix-Operations">HPCMatrix Operations</a><a id="HPCMatrix-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#HPCMatrix-Operations" title="Permalink"></a></h3><ul><li>Arithmetic: <code>*</code> (scalar), matrix-vector product</li><li>Transpose: <code>transpose(A)</code></li><li>Indexing: <code>A[i, j]</code> (global indices)</li><li>Conversion: <code>Matrix(A)</code></li></ul><h3 id="HPCSparseMatrix-Operations"><a class="docs-heading-anchor" href="#HPCSparseMatrix-Operations">HPCSparseMatrix Operations</a><a id="HPCSparseMatrix-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#HPCSparseMatrix-Operations" title="Permalink"></a></h3><ul><li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code> (scalar, matrix-vector, matrix-matrix)</li><li>Transpose: <code>transpose(A)</code></li><li>Linear solve: <code>A \ b</code>, <code>Symmetric(A) \ b</code></li><li>Utilities: <code>nnz</code>, <code>norm</code>, <code>issymmetric</code></li><li>Conversion: <code>SparseMatrixCSC(A)</code></li></ul><h2 id="Factorization-Types"><a class="docs-heading-anchor" href="#Factorization-Types">Factorization Types</a><a id="Factorization-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Factorization-Types" title="Permalink"></a></h2><p>HPCLinearAlgebra uses MUMPS for sparse direct solves:</p><ul><li><code>lu(A)</code>: LU factorization (general matrices)</li><li><code>ldlt(A)</code>: LDLT factorization (symmetric matrices, faster)</li></ul><p>Both return factorization objects that support:</p><ul><li><code>F \ b</code>: Solve with factorization</li><li><code>finalize!(F)</code>: Release MUMPS resources</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Saturday 10 January 2026 20:37">Saturday 10 January 2026</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
