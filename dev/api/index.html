<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>API Reference · LinearAlgebraMPI.jl</title><meta name="title" content="API Reference · LinearAlgebraMPI.jl"/><meta property="og:title" content="API Reference · LinearAlgebraMPI.jl"/><meta property="twitter:title" content="API Reference · LinearAlgebraMPI.jl"/><meta name="description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="og:description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="twitter:description" content="Documentation for LinearAlgebraMPI.jl."/><meta property="og:url" content="https://sloisel.github.io/LinearAlgebraMPI.jl/api/"/><meta property="twitter:url" content="https://sloisel.github.io/LinearAlgebraMPI.jl/api/"/><link rel="canonical" href="https://sloisel.github.io/LinearAlgebraMPI.jl/api/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">LinearAlgebraMPI.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../installation/">Installation</a></li><li><a class="tocitem" href="../guide/">User Guide</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>API Reference</a><ul class="internal"><li><a class="tocitem" href="#Distributed-Types"><span>Distributed Types</span></a></li><li><a class="tocitem" href="#Local-Constructors"><span>Local Constructors</span></a></li><li><a class="tocitem" href="#Row-wise-Operations"><span>Row-wise Operations</span></a></li><li><a class="tocitem" href="#Linear-System-Solvers"><span>Linear System Solvers</span></a></li><li><a class="tocitem" href="#Partition-Utilities"><span>Partition Utilities</span></a></li><li><a class="tocitem" href="#Cache-Management"><span>Cache Management</span></a></li><li><a class="tocitem" href="#IO-Utilities"><span>IO Utilities</span></a></li><li><a class="tocitem" href="#Type-Mappings"><span>Type Mappings</span></a></li><li><a class="tocitem" href="#Supported-Operations"><span>Supported Operations</span></a></li><li><a class="tocitem" href="#Factorization-Types"><span>Factorization Types</span></a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>API Reference</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>API Reference</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sloisel/LinearAlgebraMPI.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/main/docs/src/api.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="API-Reference"><a class="docs-heading-anchor" href="#API-Reference">API Reference</a><a id="API-Reference-1"></a><a class="docs-heading-anchor-permalink" href="#API-Reference" title="Permalink"></a></h1><p>This page provides detailed documentation for all exported types and functions in LinearAlgebraMPI.jl.</p><div class="admonition is-info" id="MPI-Collective-Operations-a1387683e48aabeb"><header class="admonition-header">MPI Collective Operations<a class="admonition-anchor" href="#MPI-Collective-Operations-a1387683e48aabeb" title="Permalink"></a></header><div class="admonition-body"><p>Unless otherwise noted, all functions are MPI collective operations. Every MPI rank must call these functions together.</p></div></div><h2 id="Distributed-Types"><a class="docs-heading-anchor" href="#Distributed-Types">Distributed Types</a><a id="Distributed-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Distributed-Types" title="Permalink"></a></h2><h3 id="VectorMPI"><a class="docs-heading-anchor" href="#VectorMPI">VectorMPI</a><a id="VectorMPI-1"></a><a class="docs-heading-anchor-permalink" href="#VectorMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.VectorMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.VectorMPI"><code>LinearAlgebraMPI.VectorMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">VectorMPI{T}</code></pre><p>A distributed dense vector partitioned across MPI ranks.</p><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the partition</li><li><code>partition::Vector{Int}</code>: Partition boundaries, length = nranks + 1</li><li><code>v::Vector{T}</code>: Local vector elements owned by this rank</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/vectors.jl#L3-L12">source</a></section></details></article><h3 id="MatrixMPI"><a class="docs-heading-anchor" href="#MatrixMPI">MatrixMPI</a><a id="MatrixMPI-1"></a><a class="docs-heading-anchor-permalink" href="#MatrixMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.MatrixMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.MatrixMPI"><code>LinearAlgebraMPI.MatrixMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">MatrixMPI{T}</code></pre><p>A distributed dense matrix partitioned by rows across MPI ranks.</p><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (for transpose)</li><li><code>A::Matrix{T}</code>: Local rows (NOT transposed), size = (local_nrows, ncols)</li></ul><p><strong>Invariants</strong></p><ul><li><code>row_partition</code> and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows + 1</li><li><code>col_partition[nranks+1]</code> = total number of columns + 1</li><li><code>size(A, 1) == row_partition[rank+2] - row_partition[rank+1]</code></li><li><code>size(A, 2) == col_partition[end] - 1</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/dense.jl#L36-L53">source</a></section></details></article><h3 id="SparseMatrixMPI"><a class="docs-heading-anchor" href="#SparseMatrixMPI">SparseMatrixMPI</a><a id="SparseMatrixMPI-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixMPI" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixMPI"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixMPI"><code>LinearAlgebraMPI.SparseMatrixMPI</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixMPI{T,Ti}</code></pre><p>A distributed sparse matrix partitioned by rows across MPI ranks.</p><p><strong>Type Parameters</strong></p><ul><li><code>T</code>: Element type (e.g., <code>Float64</code>, <code>ComplexF64</code>)</li><li><code>Ti</code>: Index type (e.g., <code>Int</code>, <code>Int32</code>), defaults to <code>Int</code></li></ul><p><strong>Fields</strong></p><ul><li><code>structural_hash::Blake3Hash</code>: 256-bit Blake3 hash of the structural pattern</li><li><code>row_partition::Vector{Int}</code>: Row partition boundaries, length = nranks + 1</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries, length = nranks + 1 (placeholder for transpose)</li><li><code>col_indices::Vector{Int}</code>: Global column indices that appear in the local part (local→global mapping)</li><li><code>A::SparseMatrixCSR{T,Ti}</code>: Local rows in CSR format for efficient row-wise iteration</li><li><code>cached_transpose</code>: Cached materialized transpose (bidirectionally linked)</li></ul><p><strong>Invariants</strong></p><ul><li><code>col_indices</code>, <code>row_partition</code>, and <code>col_partition</code> are sorted</li><li><code>row_partition[nranks+1]</code> = total number of rows</li><li><code>col_partition[nranks+1]</code> = total number of columns</li><li><code>size(A, 1) == row_partition[rank+1] - row_partition[rank]</code> (number of local rows)</li><li><code>size(A.parent, 1) == length(col_indices)</code> (compressed column dimension)</li><li><code>A.parent.rowval</code> contains local indices in <code>1:length(col_indices)</code></li></ul><p><strong>Storage Details</strong></p><p>The local rows are stored in CSR format (Compressed Sparse Row), which enables efficient row-wise iteration - essential for a row-partitioned distributed matrix.</p><p>In Julia, <code>SparseMatrixCSR{T,Ti}</code> is a type alias for <code>Transpose{T, SparseMatrixCSC{T,Ti}}</code>. This type has a dual interpretation:</p><ul><li><strong>Semantic view</strong>: A lazy transpose of a CSC matrix</li><li><strong>Storage view</strong>: Row-major (CSR) access to the data</li></ul><p>The underlying <code>A.parent::SparseMatrixCSC</code> stores the transposed data with:</p><ul><li><code>A.parent.m = length(col_indices)</code> (compressed, not global ncols)</li><li><code>A.parent.n</code> = number of local rows (columns in the parent = rows in CSR)</li><li><code>A.parent.colptr</code> = row pointers for the CSR format</li><li><code>A.parent.rowval</code> = LOCAL column indices (1:length(col_indices))</li><li><code>col_indices[local_idx]</code> maps local→global column indices</li></ul><p>This compression avoids &quot;hypersparse&quot; storage where the column dimension would be the global number of columns even if only a few columns have nonzeros locally.</p><p>Access the underlying CSC storage via <code>A.parent</code> when needed for low-level operations.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/sparse.jl#L260-L305">source</a></section></details></article><h3 id="SparseMatrixCSR"><a class="docs-heading-anchor" href="#SparseMatrixCSR">SparseMatrixCSR</a><a id="SparseMatrixCSR-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixCSR" title="Permalink"></a></h3><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixCSR"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixCSR"><code>LinearAlgebraMPI.SparseMatrixCSR</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixCSR{Tv,Ti} = Transpose{Tv, SparseMatrixCSC{Tv,Ti}}</code></pre><p>Type alias for CSR (Compressed Sparse Row) storage format.</p><p><strong>The Dual Life of Transpose{SparseMatrixCSC}</strong></p><p>In Julia, the type <code>Transpose{Tv, SparseMatrixCSC{Tv,Ti}}</code> has two interpretations:</p><ol><li><p><strong>Semantic interpretation</strong>: A lazy transpose wrapper around a CSC matrix. When you call <code>transpose(A)</code> on a SparseMatrixCSC, you get this wrapper that represents A^T without copying data.</p></li><li><p><strong>Storage interpretation</strong>: CSR (row-major) access to sparse data. The underlying CSC stores columns contiguously, but through the transpose wrapper, we can iterate efficiently over rows instead of columns.</p></li></ol><p>This alias clarifies intent: use <code>SparseMatrixCSR</code> when you want row-major storage semantics, and <code>transpose(A)</code> when you want the mathematical transpose.</p><p><strong>CSR vs CSC Storage</strong></p><ul><li><strong>CSC (Compressed Sparse Column)</strong>: Julia&#39;s native sparse format. Efficient for column-wise operations, matrix-vector products with column access.</li><li><strong>CSR (Compressed Sparse Row)</strong>: Efficient for row-wise operations, matrix-vector products with row access, and row-partitioned distributed matrices.</li></ul><p>For <code>SparseMatrixCSR</code>, the underlying <code>parent::SparseMatrixCSC</code> stores the <em>transposed</em> matrix. If <code>B = SparseMatrixCSR(A)</code> represents matrix M, then <code>B.parent</code> is a CSC storing M^T. This means:</p><ul><li><code>B.parent.colptr</code> acts as row pointers for M</li><li><code>B.parent.rowval</code> contains column indices for M</li><li><code>B.parent.nzval</code> contains values in row-major order</li></ul><p><strong>Usage Note</strong></p><p>Julia will still display this type as <code>Transpose{Float64, SparseMatrixCSC{...}}</code>, not as <code>SparseMatrixCSR</code>. The alias improves code clarity but doesn&#39;t affect type printing.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/LinearAlgebraMPI.jl#L29-L68">source</a></section></details></article><h2 id="Local-Constructors"><a class="docs-heading-anchor" href="#Local-Constructors">Local Constructors</a><a id="Local-Constructors-1"></a><a class="docs-heading-anchor-permalink" href="#Local-Constructors" title="Permalink"></a></h2><p>These constructors create distributed types from local data without global communication.</p><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.VectorMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.VectorMPI_local"><code>LinearAlgebraMPI.VectorMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">VectorMPI_local(v_local::Vector{T}, comm::MPI.Comm=MPI.COMM_WORLD) where T</code></pre><p>Create a VectorMPI from a local vector on each rank.</p><p>Unlike <code>VectorMPI(v_global)</code> which takes a global vector and partitions it, this constructor takes only the local portion of the vector that each rank owns. The partition is computed by gathering the local sizes from all ranks.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Rank 0 has [1.0, 2.0], Rank 1 has [3.0, 4.0, 5.0]
v = VectorMPI_local([1.0, 2.0])  # on rank 0
v = VectorMPI_local([3.0, 4.0, 5.0])  # on rank 1
# Result: distributed vector [1.0, 2.0, 3.0, 4.0, 5.0] with partition [1, 3, 6]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/vectors.jl#L19-L35">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.MatrixMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.MatrixMPI_local"><code>LinearAlgebraMPI.MatrixMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">MatrixMPI_local(A_local::Matrix{T}; comm=MPI.COMM_WORLD, col_partition=...) where T</code></pre><p>Create a MatrixMPI from a local matrix on each rank.</p><p>Unlike <code>MatrixMPI(M_global)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>All ranks must have local matrices with the same number of columns. A collective error is raised if the column counts don&#39;t match.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>comm::MPI.Comm</code>: MPI communicator (default: <code>MPI.COMM_WORLD</code>)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(size(A_local,2), nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Rank 0 has 2×3 matrix, Rank 1 has 3×3 matrix
A = MatrixMPI_local(randn(2, 3))  # on rank 0
A = MatrixMPI_local(randn(3, 3))  # on rank 1
# Result: 5×3 distributed matrix with row_partition [1, 3, 6]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/dense.jl#L61-L84">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.SparseMatrixMPI_local"><a class="docstring-binding" href="#LinearAlgebraMPI.SparseMatrixMPI_local"><code>LinearAlgebraMPI.SparseMatrixMPI_local</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">SparseMatrixMPI_local(A_local::SparseMatrixCSR{T,Ti}; comm=MPI.COMM_WORLD, col_partition=...) where {T,Ti}
SparseMatrixMPI_local(A_local::Adjoint{T,SparseMatrixCSC{T,Ti}}; comm=MPI.COMM_WORLD, col_partition=...) where {T,Ti}</code></pre><p>Create a SparseMatrixMPI from a local sparse matrix on each rank.</p><p>Unlike <code>SparseMatrixMPI{T}(A_global)</code> which takes a global matrix and partitions it, this constructor takes only the local rows of the matrix that each rank owns. The row partition is computed by gathering the local row counts from all ranks.</p><p>The input <code>A_local</code> must be a <code>SparseMatrixCSR{T,Ti}</code> (or <code>Adjoint</code> of <code>SparseMatrixCSC{T,Ti}</code>) where:</p><ul><li><code>A_local.parent.n</code> = number of local rows on this rank</li><li><code>A_local.parent.m</code> = global number of columns (must match on all ranks)</li><li><code>A_local.parent.rowval</code> = global column indices</li></ul><p>All ranks must have local matrices with the same number of columns (block widths must match). A collective error is raised if the column counts don&#39;t match.</p><p>Note: For <code>Adjoint</code> inputs, the values are conjugated to match the adjoint semantics.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>comm::MPI.Comm</code>: MPI communicator (default: <code>MPI.COMM_WORLD</code>)</li><li><code>col_partition::Vector{Int}</code>: Column partition boundaries (default: <code>uniform_partition(A_local.parent.m, nranks)</code>)</li></ul><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Create local rows in CSR format
# Rank 0 owns rows 1-2 of a 5×3 matrix, Rank 1 owns rows 3-5
local_csc = sparse([1, 1, 2], [1, 2, 3], [1.0, 2.0, 3.0], 2, 3)  # 2 local rows, 3 cols
A = SparseMatrixMPI_local(SparseMatrixCSR(local_csc))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/sparse.jl#L351-L382">source</a></section></details></article><h2 id="Row-wise-Operations"><a class="docs-heading-anchor" href="#Row-wise-Operations">Row-wise Operations</a><a id="Row-wise-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Row-wise-Operations" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.map_rows"><a class="docstring-binding" href="#LinearAlgebraMPI.map_rows"><code>LinearAlgebraMPI.map_rows</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">map_rows(f, A...)</code></pre><p>Apply function <code>f</code> to corresponding rows of distributed vectors/matrices.</p><p>Each argument in <code>A...</code> must be either a <code>VectorMPI</code> or <code>MatrixMPI</code>. All inputs are repartitioned to match the partition of the first argument before applying <code>f</code>.</p><p>For each row index i, <code>f</code> is called with the i-th row from each input:</p><ul><li>For <code>VectorMPI</code>, the i-th &quot;row&quot; is a length-1 view of element i</li><li>For <code>MatrixMPI</code>, the i-th row is a row vector (a view into the local matrix)</li></ul><p><strong>Result Type (vcat semantics)</strong></p><p>The result type depends on what <code>f</code> returns, matching the behavior of <code>vcat</code>:</p><table><tr><th style="text-align: right"><code>f</code> returns</th><th style="text-align: right">Julia type</th><th style="text-align: right">Result</th></tr><tr><td style="text-align: right">scalar</td><td style="text-align: right"><code>Number</code></td><td style="text-align: right"><code>VectorMPI</code> (one element per input row)</td></tr><tr><td style="text-align: right">column vector</td><td style="text-align: right"><code>AbstractVector</code></td><td style="text-align: right"><code>VectorMPI</code> (vcat concatenates all vectors)</td></tr><tr><td style="text-align: right">row vector</td><td style="text-align: right"><code>Transpose</code>, <code>Adjoint</code></td><td style="text-align: right"><code>MatrixMPI</code> (vcat stacks as rows)</td></tr><tr><td style="text-align: right">matrix</td><td style="text-align: right"><code>AbstractMatrix</code></td><td style="text-align: right"><code>MatrixMPI</code> (vcat stacks rows)</td></tr></table><p><strong>Lazy Wrappers</strong></p><p>Julia&#39;s <code>transpose(v)</code> and <code>v&#39;</code> (adjoint) return lazy wrappers that are subtypes of <code>AbstractMatrix</code>, so they produce <code>MatrixMPI</code> results:</p><pre><code class="language-julia hljs">map_rows(r -&gt; [1,2,3], A)           # Vector → VectorMPI (length 3n)
map_rows(r -&gt; [1,2,3]&#39;, A)          # Adjoint → MatrixMPI (n×3)
map_rows(r -&gt; transpose([1,2,3]), A) # Transpose → MatrixMPI (n×3)
map_rows(r -&gt; conj([1,2,3]), A)     # Vector → VectorMPI (length 3n)
map_rows(r -&gt; [1 2 3], A)           # Matrix literal → MatrixMPI (n×3)</code></pre><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Element-wise product of two vectors
u = VectorMPI([1.0, 2.0, 3.0])
v = VectorMPI([4.0, 5.0, 6.0])
w = map_rows((a, b) -&gt; a[1] * b[1], u, v)  # VectorMPI([4.0, 10.0, 18.0])

# Row norms of a matrix
A = MatrixMPI(randn(5, 3))
norms = map_rows(r -&gt; norm(r), A)  # VectorMPI of row norms

# Expand each row to multiple elements (vcat behavior)
A = MatrixMPI(randn(3, 2))
result = map_rows(r -&gt; [1, 2, 3], A)  # VectorMPI of length 9

# Return row vectors to build a matrix
A = MatrixMPI(randn(3, 2))
result = map_rows(r -&gt; [1, 2, 3]&#39;, A)  # 3×3 MatrixMPI

# Variable-length output per row
v = VectorMPI([1.0, 2.0, 3.0])
result = map_rows(r -&gt; ones(Int(r[1])), v)  # VectorMPI of length 6 (1+2+3)

# Mixed inputs: matrix rows weighted by vector elements
A = MatrixMPI(randn(4, 3))
w = VectorMPI([1.0, 2.0, 3.0, 4.0])
result = map_rows((row, wi) -&gt; sum(row) * wi[1], A, w)  # VectorMPI</code></pre><p>This is the MPI-distributed version of:</p><pre><code class="language-julia hljs">map_rows(f, A...) = vcat((f.((eachrow.(A))...))...)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/LinearAlgebraMPI.jl#L818-L888">source</a></section></details></article><h2 id="Linear-System-Solvers"><a class="docs-heading-anchor" href="#Linear-System-Solvers">Linear System Solvers</a><a id="Linear-System-Solvers-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-System-Solvers" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.solve"><a class="docstring-binding" href="#LinearAlgebraMPI.solve"><code>LinearAlgebraMPI.solve</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve(F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T</code></pre><p>Solve the linear system A*x = b using the precomputed MUMPS factorization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/mumps_factorization.jl#L439-L443">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.solve!"><a class="docstring-binding" href="#LinearAlgebraMPI.solve!"><code>LinearAlgebraMPI.solve!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">solve!(x::VectorMPI{T}, F::MUMPSFactorizationMPI{T}, b::VectorMPI{T}) where T</code></pre><p>Solve A*x = b in-place using MUMPS factorization.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/mumps_factorization.jl#L450-L454">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.finalize!"><a class="docstring-binding" href="#LinearAlgebraMPI.finalize!"><code>LinearAlgebraMPI.finalize!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">finalize!(F::MUMPSFactorizationMPI)</code></pre><p>Release MUMPS resources. Must be called on all ranks together.</p><p>Note: If the MUMPS object is shared with the analysis cache (owns<em>mumps=false), this only removes the factorization from the registry. The MUMPS object itself is finalized when `clear</em>mumps<em>analysis</em>cache!()` is called.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/mumps_factorization.jl#L503-L511">source</a></section></details></article><h2 id="Partition-Utilities"><a class="docs-heading-anchor" href="#Partition-Utilities">Partition Utilities</a><a id="Partition-Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#Partition-Utilities" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.uniform_partition"><a class="docstring-binding" href="#LinearAlgebraMPI.uniform_partition"><code>LinearAlgebraMPI.uniform_partition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">uniform_partition(n::Int, nranks::Int) -&gt; Vector{Int}</code></pre><p>Compute a balanced partition of <code>n</code> elements across <code>nranks</code> ranks. Returns a vector of length <code>nranks + 1</code> with 1-indexed partition boundaries.</p><p>The first <code>mod(n, nranks)</code> ranks get <code>div(n, nranks) + 1</code> elements, the remaining ranks get <code>div(n, nranks)</code> elements.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">partition = uniform_partition(10, 4)  # [1, 4, 7, 9, 11]
# Rank 0: 1:3 (3 elements)
# Rank 1: 4:6 (3 elements)
# Rank 2: 7:8 (2 elements)
# Rank 3: 9:10 (2 elements)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/LinearAlgebraMPI.jl#L173-L190">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.repartition"><a class="docstring-binding" href="#LinearAlgebraMPI.repartition"><code>LinearAlgebraMPI.repartition</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">repartition(x::VectorMPI{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a VectorMPI to a new partition <code>p</code>.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == length(x) + 1</code>.</p><p>Returns a new VectorMPI with the same data but <code>partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">v = VectorMPI([1.0, 2.0, 3.0, 4.0])  # uniform partition
new_partition = [1, 2, 5]  # rank 0 gets 1 element, rank 1 gets 3
v_repart = repartition(v, new_partition)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/vectors.jl#L496-L512">source</a></section><section><div><pre><code class="language-julia hljs">repartition(A::MatrixMPI{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a MatrixMPI to a new row partition <code>p</code>. The col_partition remains unchanged.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == size(A, 1) + 1</code>.</p><p>Returns a new MatrixMPI with the same data but <code>row_partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = MatrixMPI(randn(6, 4))  # uniform partition
new_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank
A_repart = repartition(A, new_partition)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/dense.jl#L1602-L1619">source</a></section><section><div><pre><code class="language-julia hljs">repartition(A::SparseMatrixMPI{T}, p::Vector{Int}) where T</code></pre><p>Redistribute a SparseMatrixMPI to a new row partition <code>p</code>. The col_partition remains unchanged.</p><p>The partition <code>p</code> must be a valid partition vector of length <code>nranks + 1</code> with <code>p[1] == 1</code> and <code>p[end] == size(A, 1) + 1</code>.</p><p>Returns a new SparseMatrixMPI with the same data but <code>row_partition == p</code>.</p><p><strong>Example</strong></p><pre><code class="language-julia hljs">A = SparseMatrixMPI{Float64}(sprand(6, 4, 0.5))  # uniform partition
new_partition = [1, 2, 4, 5, 7]  # 1, 2, 1, 2 rows per rank
A_repart = repartition(A, new_partition)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/sparse.jl#L4090-L4107">source</a></section></details></article><h2 id="Cache-Management"><a class="docs-heading-anchor" href="#Cache-Management">Cache Management</a><a id="Cache-Management-1"></a><a class="docs-heading-anchor-permalink" href="#Cache-Management" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.clear_plan_cache!"><a class="docstring-binding" href="#LinearAlgebraMPI.clear_plan_cache!"><code>LinearAlgebraMPI.clear_plan_cache!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">clear_plan_cache!()</code></pre><p>Clear all memoized plan caches, including the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/LinearAlgebraMPI.jl#L137-L142">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.clear_mumps_analysis_cache!"><a class="docstring-binding" href="#LinearAlgebraMPI.clear_mumps_analysis_cache!"><code>LinearAlgebraMPI.clear_mumps_analysis_cache!</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">clear_mumps_analysis_cache!()</code></pre><p>Clear the MUMPS analysis cache. This is a collective operation that must be called on all MPI ranks together.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/mumps_factorization.jl#L83-L88">source</a></section></details></article><h2 id="IO-Utilities"><a class="docs-heading-anchor" href="#IO-Utilities">IO Utilities</a><a id="IO-Utilities-1"></a><a class="docs-heading-anchor-permalink" href="#IO-Utilities" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="LinearAlgebraMPI.io0"><a class="docstring-binding" href="#LinearAlgebraMPI.io0"><code>LinearAlgebraMPI.io0</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">io0(io=stdout; r::Set{Int}=Set{Int}([0]), dn=devnull)</code></pre><p>Return <code>io</code> if the current MPI rank is in set <code>r</code>, otherwise return <code>dn</code> (default: <code>devnull</code>).</p><p>This is useful for printing only from specific ranks:</p><pre><code class="language-julia hljs">println(io0(), &quot;Hello from rank 0!&quot;)
println(io0(r=Set([0,1])), &quot;Hello from ranks 0 and 1!&quot;)</code></pre><p>With string interpolation:</p><pre><code class="language-julia hljs">println(io0(), &quot;Matrix A = $A&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/sloisel/LinearAlgebraMPI.jl/blob/5d932c1c8a73b257e819194d69ad757c4c251e1c/src/LinearAlgebraMPI.jl#L568-L583">source</a></section></details></article><h2 id="Type-Mappings"><a class="docs-heading-anchor" href="#Type-Mappings">Type Mappings</a><a id="Type-Mappings-1"></a><a class="docs-heading-anchor-permalink" href="#Type-Mappings" title="Permalink"></a></h2><h3 id="Native-to-MPI-Conversions"><a class="docs-heading-anchor" href="#Native-to-MPI-Conversions">Native to MPI Conversions</a><a id="Native-to-MPI-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#Native-to-MPI-Conversions" title="Permalink"></a></h3><table><tr><th style="text-align: right">Native Type</th><th style="text-align: right">MPI Type</th><th style="text-align: right">Description</th></tr><tr><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right"><code>VectorMPI{T}</code></td><td style="text-align: right">Distributed vector</td></tr><tr><td style="text-align: right"><code>Matrix{T}</code></td><td style="text-align: right"><code>MatrixMPI{T}</code></td><td style="text-align: right">Distributed dense matrix</td></tr><tr><td style="text-align: right"><code>SparseMatrixCSC{T,Ti}</code></td><td style="text-align: right"><code>SparseMatrixMPI{T,Ti}</code></td><td style="text-align: right">Distributed sparse matrix</td></tr></table><h3 id="MPI-to-Native-Conversions"><a class="docs-heading-anchor" href="#MPI-to-Native-Conversions">MPI to Native Conversions</a><a id="MPI-to-Native-Conversions-1"></a><a class="docs-heading-anchor-permalink" href="#MPI-to-Native-Conversions" title="Permalink"></a></h3><table><tr><th style="text-align: right">MPI Type</th><th style="text-align: right">Native Type</th><th style="text-align: right">Function</th></tr><tr><td style="text-align: right"><code>VectorMPI{T}</code></td><td style="text-align: right"><code>Vector{T}</code></td><td style="text-align: right"><code>Vector(v)</code></td></tr><tr><td style="text-align: right"><code>MatrixMPI{T}</code></td><td style="text-align: right"><code>Matrix{T}</code></td><td style="text-align: right"><code>Matrix(A)</code></td></tr><tr><td style="text-align: right"><code>SparseMatrixMPI{T,Ti}</code></td><td style="text-align: right"><code>SparseMatrixCSC{T,Ti}</code></td><td style="text-align: right"><code>SparseMatrixCSC(A)</code></td></tr></table><h2 id="Supported-Operations"><a class="docs-heading-anchor" href="#Supported-Operations">Supported Operations</a><a id="Supported-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#Supported-Operations" title="Permalink"></a></h2><h3 id="VectorMPI-Operations"><a class="docs-heading-anchor" href="#VectorMPI-Operations">VectorMPI Operations</a><a id="VectorMPI-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#VectorMPI-Operations" title="Permalink"></a></h3><ul><li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code> (scalar)</li><li>Linear algebra: <code>norm</code>, <code>dot</code>, <code>conj</code></li><li>Indexing: <code>v[i]</code> (global index)</li><li>Conversion: <code>Vector(v)</code></li></ul><h3 id="MatrixMPI-Operations"><a class="docs-heading-anchor" href="#MatrixMPI-Operations">MatrixMPI Operations</a><a id="MatrixMPI-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#MatrixMPI-Operations" title="Permalink"></a></h3><ul><li>Arithmetic: <code>*</code> (scalar), matrix-vector product</li><li>Transpose: <code>transpose(A)</code></li><li>Indexing: <code>A[i, j]</code> (global indices)</li><li>Conversion: <code>Matrix(A)</code></li></ul><h3 id="SparseMatrixMPI-Operations"><a class="docs-heading-anchor" href="#SparseMatrixMPI-Operations">SparseMatrixMPI Operations</a><a id="SparseMatrixMPI-Operations-1"></a><a class="docs-heading-anchor-permalink" href="#SparseMatrixMPI-Operations" title="Permalink"></a></h3><ul><li>Arithmetic: <code>+</code>, <code>-</code>, <code>*</code> (scalar, matrix-vector, matrix-matrix)</li><li>Transpose: <code>transpose(A)</code></li><li>Linear solve: <code>A \ b</code>, <code>Symmetric(A) \ b</code></li><li>Utilities: <code>nnz</code>, <code>norm</code>, <code>issymmetric</code></li><li>Conversion: <code>SparseMatrixCSC(A)</code></li></ul><h2 id="Factorization-Types"><a class="docs-heading-anchor" href="#Factorization-Types">Factorization Types</a><a id="Factorization-Types-1"></a><a class="docs-heading-anchor-permalink" href="#Factorization-Types" title="Permalink"></a></h2><p>LinearAlgebraMPI uses MUMPS for sparse direct solves:</p><ul><li><code>lu(A)</code>: LU factorization (general matrices)</li><li><code>ldlt(A)</code>: LDLT factorization (symmetric matrices, faster)</li></ul><p>Both return factorization objects that support:</p><ul><li><code>F \ b</code>: Solve with factorization</li><li><code>finalize!(F)</code>: Release MUMPS resources</li></ul></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 19 December 2025 22:12">Friday 19 December 2025</span>. Using Julia version 1.10.10.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
